MASTER_ADDR=10.0.51.1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
using world size: 32, data-parallel-size: 8, tensor-model-parallel size: 2, pipeline-model-parallel size: 2 
WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:Llama2Tokenizer
accumulate and all-reduce gradients in fp32 for bfloat16 data type.
using torch.bfloat16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. True
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. False
  add_position_embedding .......................... False
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  async_tensor_model_parallel_allreduce ........... False
  attention_dropout ............................... 0.0
  attention_softmax_in_fp32 ....................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ True
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ False
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  check_for_nan_in_loss_and_grad .................. True
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_cache_path ................................. None
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 8
  data_path ....................................... ['10605477142', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/split_0_text_document', '10464907226', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/split_1_text_document', '12465407213', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/split_2_text_document', '16446568076', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/split_3_text_document', '38345096470', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/split_4_text_document', '1672543873', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/ja_wiki_merged_train_text_document', '5000000000', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/lumi_en_arxiv_merged_text_document', '2500000000', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/en_wiki_merged_text_document', '2500000000', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/books_merged_text_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  delay_grad_reduce ............................... True
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 10
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  encoder_num_layers .............................. 32
  encoder_seq_length .............................. 4096
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... False
  eval_interval ................................... 100
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_parallel ................................. False
  ffn_hidden_size ................................. 11008
  finetune ........................................ False
  fp16 ............................................ False
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 1024
  gradient_accumulation_fusion .................... True
  group_query_attention ........................... True
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.0
  hidden_size ..................................... 4096
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  iteration ....................................... 1
  kv_channels ..................................... 128
  lazy_mpu_init ................................... None
  load ............................................ /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 1000
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 4096
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... None
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 3.3e-06
  mmap_warmup ..................................... False
  model_spec ...................................... None
  no_load_optim ................................... True
  no_load_rng ..................................... True
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... RMSNorm
  num_attention_heads ............................. 32
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... None
  num_layers ...................................... 32
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 32
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  override_opt_param_scheduler .................... False
  padded_vocab_size ............................... 43176
  params_dtype .................................... torch.bfloat16
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 2
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... rope
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... selective
  recompute_method ................................ None
  recompute_num_layers ............................ None
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_return_doc_ids ............................ False
  retro_workdir ................................... None
  rotary_percent .................................. 1.0
  rotary_seq_len_interpolation_factor ............. None
  sample_rate ..................................... 1.0
  save ............................................ /bb/llm/gaf51275/llama/checkpoints/Llama-2-7b-base-extended/okazaki_lab_cc-en-updated/tp2-pp2
  save_interval ................................... 500
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 4096
  sequence_parallel ............................... True
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  split ........................................... 949,50,1
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.1
  swiglu .......................................... True
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 2
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. /bb/llm/gaf51275/jalm/jalm-tokenizer-private/tokenizer/jalm_llama_okazaki_lab_cc_nfkc_16k_aligned_8/merged_tokenizer_sp/jalm_llama.model
  tokenizer_type .................................. Llama2Tokenizer
  train_data_path ................................. None
  train_iters ..................................... 25000
  train_samples ................................... None
  transformer_impl ................................ local
  transformer_pipeline_model_parallel_size ........ 2
  untie_embeddings_and_output_weights ............. True
  use_checkpoint_args ............................. True
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_distributed_optimizer ....................... True
  use_flash_attn .................................. True
  use_mpi ......................................... True
  use_one_sent_docs ............................... False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. True
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... None
  vocab_size ...................................... None
  wandb_entity .................................... prj-jalm
  wandb_id ........................................ None
  wandb_name ...................................... llama-2-7b-base-extended-okazaki-lab-cc-a100-4node-32gpu-4096s-DP=8-TP=2-PP=2-BS=1024-LR=1e-4-MINLR=3.3e-6-WARMUP=1000-WD=0.1-GC=1
  wandb_project ................................... Llama-2-7B
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  world_size ...................................... 32
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 128
> building Llama2Tokenizer tokenizer ...
> initializing torch distributed ...
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
checkpoint path: /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2, iteration: 1
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
wandb: Currently logged in as: okoge (prj-jalm). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /bb/1/llm/gaf51275/llama/Megatron-LM/wandb/run-20231121_173703-cu9l51r1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama-2-7b-base-extended-okazaki-lab-cc-a100-4node-32gpu-4096s-DP=8-TP=2-PP=2-BS=1024-LR=1e-4-MINLR=3.3e-6-WARMUP=1000-WD=0.1-GC=1-2023-11-21-17-37-02
wandb: ⭐️ View project at https://wandb.ai/prj-jalm/Llama-2-7B
wandb: 🚀 View run at https://wandb.ai/prj-jalm/Llama-2-7B/runs/cu9l51r1
> wandb ...
> initialized tensor model parallel with size 2
> initialized pipeline model parallel with size 2
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory '/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/data'
>>> done with dataset index builder. Compilation time: 0.051 seconds
WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
> compiling and loading fused kernels ...
>>> done with compiling and loading fused kernels. Compilation time: 5.593 seconds
time to initialize megatron (seconds): 56.807
[after megatron is initialized] datetime: 2023-11-21 17:37:29 
building GPT model ...
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1707556864
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1707556864
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1707560960
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1707560960
> buckets for gradient all-reduce / reduce-scatter:
    params for bucket 1
      module.language_model.encoder.layers.12.self_attention.query_key_value.weight
      module.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.2.self_attention.dense.weight
      module.language_model.encoder.layers.0.self_attention.dense.weight
      module.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.10.input_norm.weight
      module.language_model.encoder.layers.5.post_attention_norm.weight
      module.language_model.encoder.layers.14.post_attention_norm.weight
      module.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.6.input_norm.weight
      module.language_model.encoder.layers.1.self_attention.query_key_value.weight
      module.language_model.encoder.layers.11.self_attention.dense.weight
      module.language_model.encoder.layers.5.self_attention.query_key_value.weight
      module.language_model.encoder.layers.0.post_attention_norm.weight
      module.language_model.encoder.layers.11.self_attention.query_key_value.weight
      module.language_model.encoder.layers.4.self_attention.query_key_value.weight
      module.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.9.input_norm.weight
      module.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.0.input_norm.weight
      module.language_model.encoder.layers.13.post_attention_norm.weight
      module.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.10.self_attention.dense.weight
      module.language_model.encoder.layers.4.post_attention_norm.weight
      module.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.10.self_attention.query_key_value.weight
      module.language_model.encoder.layers.6.self_attention.query_key_value.weight
      module.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.8.input_norm.weight
      module.language_model.encoder.layers.12.post_attention_norm.weight
      module.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.1.post_attention_norm.weight
      module.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.9.self_attention.dense.weight
      module.language_model.encoder.layers.5.self_attention.dense.weight
      module.language_model.encoder.layers.1.input_norm.weight
      module.language_model.encoder.layers.15.input_norm.weight
      module.language_model.encoder.layers.9.self_attention.query_key_value.weight
      module.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.6.self_attention.dense.weight
      module.language_model.encoder.layers.2.input_norm.weight
      module.language_model.encoder.layers.11.post_attention_norm.weight
      module.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.3.input_norm.weight
      module.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.8.self_attention.dense.weight
      module.language_model.encoder.layers.2.self_attention.query_key_value.weight
      module.language_model.encoder.layers.14.input_norm.weight
      module.language_model.encoder.layers.8.self_attention.query_key_value.weight
      module.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.4.self_attention.dense.weight
      module.language_model.encoder.layers.4.input_norm.weight
      module.language_model.encoder.layers.0.self_attention.query_key_value.weight
      module.language_model.encoder.layers.15.self_attention.dense.weight
      module.language_model.encoder.layers.10.post_attention_norm.weight
      module.language_model.encoder.layers.6.post_attention_norm.weight
      module.language_model.encoder.layers.3.self_attention.query_key_value.weight
      module.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.7.self_attention.dense.weight
      module.language_model.encoder.layers.1.self_attention.dense.weight
      module.language_model.encoder.layers.13.input_norm.weight
      module.language_model.encoder.layers.7.self_attention.query_key_value.weight
      module.language_model.encoder.layers.15.self_attention.query_key_value.weight
      module.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.14.self_attention.dense.weight
      module.language_model.encoder.layers.9.post_attention_norm.weight
      module.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight
      module.language_model.embedding.word_embeddings.weight
      module.language_model.encoder.layers.12.input_norm.weight
      module.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.2.post_attention_norm.weight
      module.language_model.encoder.layers.14.self_attention.query_key_value.weight
      module.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.13.self_attention.dense.weight
      module.language_model.encoder.layers.8.post_attention_norm.weight
      module.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.5.input_norm.weight
      module.language_model.encoder.layers.3.post_attention_norm.weight
      module.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.11.input_norm.weight
      module.language_model.encoder.layers.7.input_norm.weight
      module.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.13.self_attention.query_key_value.weight
      module.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.12.self_attention.dense.weight
      module.language_model.encoder.layers.7.post_attention_norm.weight
      module.language_model.encoder.layers.3.self_attention.dense.weight
      module.language_model.encoder.layers.15.post_attention_norm.weight
      module.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight
     total number of elements: 1707556864
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
> learning rate decay style: cosine
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
 loading checkpoint from /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2 at iteration 1
 checkpoint version 3.0
  successfully loaded checkpoint from /bb/llm/gaf51275/llama/llama-megatron-convert-checkpoint-hf/Llama-2-7b-extended/okazaki_lab_cc/tp2-pp2 at iteration 1
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
(min, max) time across ranks (ms):
    load-checkpoint ................................: (4930.16, 4930.18)
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-11-21 17:37:34 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      25600000
    validation: 2570240
    test:       10240
> building train, validation, and test datasets for GPT ...
Single data path provided for train, valid & test
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.009291 seconds
    number of documents: 15155280
 > dataset split:
    train:
     document indices in [0, 14382361) total of 14382361 documents
    validation:
     document indices in [14382361, 15140125) total of 757764 documents
    test:
     document indices in [15140125, 15155280) total of 15155 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/37ae7c2eb33659b0a2b1f80f7cd27e56_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/37ae7c2eb33659b0a2b1f80f7cd27e56_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/37ae7c2eb33659b0a2b1f80f7cd27e56_shuffle_idx.npy
    loaded indexed file in 0.017 seconds
    total number of samples: 4095813
    total number of epochs: 1
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/d0e3132c328b3684201b94acc3128317_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/d0e3132c328b3684201b94acc3128317_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/d0e3132c328b3684201b94acc3128317_shuffle_idx.npy
    loaded indexed file in 0.014 seconds
    total number of samples: 432667
    total number of epochs: 2
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/5d799cbb7d19da4de571e34af703b6d2_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/5d799cbb7d19da4de571e34af703b6d2_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/5d799cbb7d19da4de571e34af703b6d2_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of samples: 4237
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.005593 seconds
    number of documents: 14645865
 > dataset split:
    train:
     document indices in [0, 13898926) total of 13898926 documents
    validation:
     document indices in [13898926, 14631219) total of 732293 documents
    test:
     document indices in [14631219, 14645865) total of 14646 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/767218c81a7b9071108a99ed453003d3_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/767218c81a7b9071108a99ed453003d3_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/767218c81a7b9071108a99ed453003d3_shuffle_idx.npy
    loaded indexed file in 0.017 seconds
    total number of samples: 4030778
    total number of epochs: 1
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/ef68dfcfdd12144299751b2fa73026da_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/ef68dfcfdd12144299751b2fa73026da_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/ef68dfcfdd12144299751b2fa73026da_shuffle_idx.npy
    loaded indexed file in 0.013 seconds
    total number of samples: 447050
    total number of epochs: 2
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/b46a915d741b4174373a3b74798ab07a_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/b46a915d741b4174373a3b74798ab07a_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/b46a915d741b4174373a3b74798ab07a_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of samples: 4870
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.007118 seconds
    number of documents: 17791633
 > dataset split:
    train:
     document indices in [0, 16884259) total of 16884259 documents
    validation:
     document indices in [16884259, 17773841) total of 889582 documents
    test:
     document indices in [17773841, 17791633) total of 17792 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/9e64cf6a9182f6bd6dcf72dcd7b6703c_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/9e64cf6a9182f6bd6dcf72dcd7b6703c_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/9e64cf6a9182f6bd6dcf72dcd7b6703c_shuffle_idx.npy
    loaded indexed file in 0.017 seconds
    total number of samples: 4814195
    total number of epochs: 1
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/1a140a8993c7e1cb82532311b6d16a43_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/1a140a8993c7e1cb82532311b6d16a43_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/1a140a8993c7e1cb82532311b6d16a43_shuffle_idx.npy
    loaded indexed file in 0.015 seconds
    total number of samples: 508490
    total number of epochs: 2
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/ddb3dedf32955416af35b0ca23900596_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/ddb3dedf32955416af35b0ca23900596_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/ddb3dedf32955416af35b0ca23900596_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of samples: 4927
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.005310 seconds
    number of documents: 23317920
 > dataset split:
    train:
     document indices in [0, 22128706) total of 22128706 documents
    validation:
     document indices in [22128706, 23294602) total of 1165896 documents
    test:
     document indices in [23294602, 23317920) total of 23318 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e2e2e5a1d4e3eb44f7950cd58e36f6b7_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e2e2e5a1d4e3eb44f7950cd58e36f6b7_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e2e2e5a1d4e3eb44f7950cd58e36f6b7_shuffle_idx.npy
    loaded indexed file in 0.033 seconds
    total number of samples: 6349008
    total number of epochs: 1
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/fe8f046587a67ae3cdeeeb8466a1026e_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/fe8f046587a67ae3cdeeeb8466a1026e_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/fe8f046587a67ae3cdeeeb8466a1026e_shuffle_idx.npy
    loaded indexed file in 0.014 seconds
    total number of samples: 676206
    total number of epochs: 2
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/6e07b93a890151565e4cb2765081a395_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/6e07b93a890151565e4cb2765081a395_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/6e07b93a890151565e4cb2765081a395_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of samples: 6571
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.006792 seconds
    number of documents: 53682242
 > dataset split:
    train:
     document indices in [0, 50944448) total of 50944448 documents
    validation:
     document indices in [50944448, 53628560) total of 2684112 documents
    test:
     document indices in [53628560, 53682242) total of 53682 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/dd9b6e87f6faf9ff0e1b4bcccbed739d_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/dd9b6e87f6faf9ff0e1b4bcccbed739d_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/dd9b6e87f6faf9ff0e1b4bcccbed739d_shuffle_idx.npy
    loaded indexed file in 0.017 seconds
    total number of samples: 14804533
    total number of epochs: 1
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/2ec3323890da53347e204fc32f2889fc_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/2ec3323890da53347e204fc32f2889fc_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/2ec3323890da53347e204fc32f2889fc_shuffle_idx.npy
    loaded indexed file in 0.017 seconds
    total number of samples: 1571739
    total number of epochs: 2
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/3c901fa2e36e2144b1f0acc74e5d2884_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/3c901fa2e36e2144b1f0acc74e5d2884_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/3c901fa2e36e2144b1f0acc74e5d2884_shuffle_idx.npy
    loaded indexed file in 0.003 seconds
    total number of samples: 15883
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.006326 seconds
    number of documents: 1363395
 > dataset split:
    train:
     document indices in [0, 1293862) total of 1293862 documents
    validation:
     document indices in [1293862, 1362032) total of 68170 documents
    test:
     document indices in [1362032, 1363395) total of 1363 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/098084d9bd45664596c98c0605a7a2f4_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/098084d9bd45664596c98c0605a7a2f4_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/098084d9bd45664596c98c0605a7a2f4_shuffle_idx.npy
    loaded indexed file in 0.014 seconds
    total number of samples: 772112
    total number of epochs: 2
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/b04bb9b7f753046066f3e7be0b1cbec3_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/b04bb9b7f753046066f3e7be0b1cbec3_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/b04bb9b7f753046066f3e7be0b1cbec3_shuffle_idx.npy
    loaded indexed file in 0.004 seconds
    total number of samples: 43865
    total number of epochs: 2
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/56666b06ed69879a51e4902670ccc060_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/56666b06ed69879a51e4902670ccc060_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/56666b06ed69879a51e4902670ccc060_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of samples: 348
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.005450 seconds
    number of documents: 947630
 > dataset split:
    train:
     document indices in [0, 899300) total of 899300 documents
    validation:
     document indices in [899300, 946682) total of 47382 documents
    test:
     document indices in [946682, 947630) total of 948 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/8f12a01afda75ffc40441604a48ef42a_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/8f12a01afda75ffc40441604a48ef42a_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/8f12a01afda75ffc40441604a48ef42a_shuffle_idx.npy
    loaded indexed file in 0.016 seconds
    total number of samples: 3315568
    total number of epochs: 1
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/1dbd05c3240aac2e68dd571ea86e90fb_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/1dbd05c3240aac2e68dd571ea86e90fb_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/1dbd05c3240aac2e68dd571ea86e90fb_shuffle_idx.npy
    loaded indexed file in 0.005 seconds
    total number of samples: 175764
    total number of epochs: 1
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/b9b3b95888edc3ec4aad4193b41ec9e3_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/b9b3b95888edc3ec4aad4193b41ec9e3_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/b9b3b95888edc3ec4aad4193b41ec9e3_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of samples: 3599
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.007477 seconds
    number of documents: 6681920
 > dataset split:
    train:
     document indices in [0, 6341142) total of 6341142 documents
    validation:
     document indices in [6341142, 6675238) total of 334096 documents
    test:
     document indices in [6675238, 6681920) total of 6682 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/f938354f66c6ea9a17f91b60f422cdfc_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/f938354f66c6ea9a17f91b60f422cdfc_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/f938354f66c6ea9a17f91b60f422cdfc_shuffle_idx.npy
    loaded indexed file in 0.018 seconds
    total number of samples: 1239024
    total number of epochs: 1
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/f8d206e94ee6e5724e4fcd5bed12e714_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/f8d206e94ee6e5724e4fcd5bed12e714_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/f8d206e94ee6e5724e4fcd5bed12e714_shuffle_idx.npy
    loaded indexed file in 0.007 seconds
    total number of samples: 107358
    total number of epochs: 2
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/91366b002de3248404f9e001fb6e7985_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/91366b002de3248404f9e001fb6e7985_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/91366b002de3248404f9e001fb6e7985_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of samples: 1022
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.005516 seconds
    number of documents: 8645722
 > dataset split:
    train:
     document indices in [0, 8204790) total of 8204790 documents
    validation:
     document indices in [8204790, 8637076) total of 432286 documents
    test:
     document indices in [8637076, 8645722) total of 8646 documents
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/5adeb930d82924f0aacec9f03bad5f6b_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/5adeb930d82924f0aacec9f03bad5f6b_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/5adeb930d82924f0aacec9f03bad5f6b_shuffle_idx.npy
    loaded indexed file in 0.017 seconds
    total number of samples: 5071223
    total number of epochs: 1
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/f9a8cf4b0a03672e22fcf568697b4bd8_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/f9a8cf4b0a03672e22fcf568697b4bd8_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/f9a8cf4b0a03672e22fcf568697b4bd8_shuffle_idx.npy
    loaded indexed file in 0.008 seconds
    total number of samples: 267197
    total number of epochs: 1
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/8d048abcb0ef820c253ce1e940ded5b6_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/8d048abcb0ef820c253ce1e940ded5b6_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_1500_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/8d048abcb0ef820c253ce1e940ded5b6_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of samples: 5370
    total number of epochs: 1
> building indices for blendable datasets ...
 > sample ratios:
   dataset 0, input: 0.106055, achieved: 0.106055
   dataset 1, input: 0.104649, achieved: 0.104649
   dataset 2, input: 0.124654, achieved: 0.124654
   dataset 3, input: 0.164466, achieved: 0.164466
   dataset 4, input: 0.383451, achieved: 0.383451
   dataset 5, input: 0.0167254, achieved: 0.0167254
   dataset 6, input: 0.05, achieved: 0.05
   dataset 7, input: 0.025, achieved: 0.025
   dataset 8, input: 0.025, achieved: 0.025
> elapsed time for building blendable dataset indices: 0.46 (sec)
> size of blendable dataset: 25728004 samples
> building indices for blendable datasets ...
 > sample ratios:
   dataset 0, input: 0.106055, achieved: 0.106055
   dataset 1, input: 0.104649, achieved: 0.104649
   dataset 2, input: 0.124654, achieved: 0.124654
   dataset 3, input: 0.164466, achieved: 0.164465
   dataset 4, input: 0.383451, achieved: 0.383451
   dataset 5, input: 0.0167254, achieved: 0.0167257
   dataset 6, input: 0.05, achieved: 0.0500001
   dataset 7, input: 0.025, achieved: 0.0250002
   dataset 8, input: 0.025, achieved: 0.0249998
> elapsed time for building blendable dataset indices: 0.05 (sec)
> size of blendable dataset: 2583096 samples
> building indices for blendable datasets ...
 > sample ratios:
   dataset 0, input: 0.106055, achieved: 0.106061
   dataset 1, input: 0.104649, achieved: 0.104604
   dataset 2, input: 0.124654, achieved: 0.124611
   dataset 3, input: 0.164466, achieved: 0.164433
   dataset 4, input: 0.383451, achieved: 0.38345
   dataset 5, input: 0.0167254, achieved: 0.0167055
   dataset 6, input: 0.05, achieved: 0.0500194
   dataset 7, input: 0.025, achieved: 0.0250583
   dataset 8, input: 0.025, achieved: 0.0250583
> elapsed time for building blendable dataset indices: 0.00 (sec)
> size of blendable dataset: 10296 samples
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-11-21 17:37:39 
done with setup ...
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (5215.11, 5227.47)
    train/valid/test-data-iterators-setup ..........: (4792.07, 5091.87)
training ...
[before the start of training step] datetime: 2023-11-21 17:37:39 
 iteration        2/   25000 | consumed samples:         2048 | elapsed time per iteration (ms): 44622.3 | learning rate: 1.000E-07 | global batch size:  1024 | lm loss: 7.063287E+00 | loss scale: 1.0 | grad norm: 76.918 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 0] (after 2 iterations) memory (MB) | allocated: 12277.83837890625 | max allocated: 25187.09765625 | reserved: 25724.0 | max reserved: 25724.0
[Rank 1] (after 2 iterations) memory (MB) | allocated: 12277.83837890625 | max allocated: 25191.09765625 | reserved: 25724.0 | max reserved: 25724.0
[Rank 16] (after 2 iterations) memory (MB) | allocated: 12310.54052734375 | max allocated: 18829.9384765625 | reserved: 19262.0 | max reserved: 19262.0
[Rank 17] (after 2 iterations) memory (MB) | allocated: 12310.54052734375 | max allocated: 18829.564453125 | reserved: 19234.0 | max reserved: 19234.0
 iteration        3/   25000 | consumed samples:         3072 | elapsed time per iteration (ms): 39093.9 | learning rate: 2.000E-07 | global batch size:  1024 | lm loss: 7.085420E+00 | loss scale: 1.0 | grad norm: 78.834 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        4/   25000 | consumed samples:         4096 | elapsed time per iteration (ms): 39113.1 | learning rate: 3.000E-07 | global batch size:  1024 | lm loss: 7.099434E+00 | loss scale: 1.0 | grad norm: 76.885 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        5/   25000 | consumed samples:         5120 | elapsed time per iteration (ms): 39273.6 | learning rate: 4.000E-07 | global batch size:  1024 | lm loss: 7.073908E+00 | loss scale: 1.0 | grad norm: 81.893 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        6/   25000 | consumed samples:         6144 | elapsed time per iteration (ms): 39123.0 | learning rate: 5.000E-07 | global batch size:  1024 | lm loss: 7.067625E+00 | loss scale: 1.0 | grad norm: 78.523 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        7/   25000 | consumed samples:         7168 | elapsed time per iteration (ms): 39135.1 | learning rate: 6.000E-07 | global batch size:  1024 | lm loss: 7.085030E+00 | loss scale: 1.0 | grad norm: 78.127 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        8/   25000 | consumed samples:         8192 | elapsed time per iteration (ms): 39541.6 | learning rate: 7.000E-07 | global batch size:  1024 | lm loss: 7.057525E+00 | loss scale: 1.0 | grad norm: 78.068 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        9/   25000 | consumed samples:         9216 | elapsed time per iteration (ms): 39133.1 | learning rate: 8.000E-07 | global batch size:  1024 | lm loss: 6.995553E+00 | loss scale: 1.0 | grad norm: 74.148 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       10/   25000 | consumed samples:        10240 | elapsed time per iteration (ms): 39221.3 | learning rate: 9.000E-07 | global batch size:  1024 | lm loss: 7.008186E+00 | loss scale: 1.0 | grad norm: 72.338 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/   25000 | consumed samples:        11264 | elapsed time per iteration (ms): 39237.8 | learning rate: 1.000E-06 | global batch size:  1024 | lm loss: 6.955640E+00 | loss scale: 1.0 | grad norm: 71.613 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/   25000 | consumed samples:        12288 | elapsed time per iteration (ms): 39156.4 | learning rate: 1.100E-06 | global batch size:  1024 | lm loss: 6.772930E+00 | loss scale: 1.0 | grad norm: 59.081 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/   25000 | consumed samples:        13312 | elapsed time per iteration (ms): 39150.3 | learning rate: 1.200E-06 | global batch size:  1024 | lm loss: 6.757976E+00 | loss scale: 1.0 | grad norm: 60.523 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/   25000 | consumed samples:        14336 | elapsed time per iteration (ms): 39137.6 | learning rate: 1.300E-06 | global batch size:  1024 | lm loss: 6.731629E+00 | loss scale: 1.0 | grad norm: 60.688 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/   25000 | consumed samples:        15360 | elapsed time per iteration (ms): 39214.0 | learning rate: 1.400E-06 | global batch size:  1024 | lm loss: 6.284318E+00 | loss scale: 1.0 | grad norm: 40.434 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/   25000 | consumed samples:        16384 | elapsed time per iteration (ms): 39525.8 | learning rate: 1.500E-06 | global batch size:  1024 | lm loss: 6.221579E+00 | loss scale: 1.0 | grad norm: 38.548 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/   25000 | consumed samples:        17408 | elapsed time per iteration (ms): 39229.2 | learning rate: 1.600E-06 | global batch size:  1024 | lm loss: 6.195732E+00 | loss scale: 1.0 | grad norm: 35.986 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/   25000 | consumed samples:        18432 | elapsed time per iteration (ms): 39202.1 | learning rate: 1.700E-06 | global batch size:  1024 | lm loss: 6.125274E+00 | loss scale: 1.0 | grad norm: 34.253 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/   25000 | consumed samples:        19456 | elapsed time per iteration (ms): 39139.7 | learning rate: 1.800E-06 | global batch size:  1024 | lm loss: 6.097548E+00 | loss scale: 1.0 | grad norm: 32.260 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/   25000 | consumed samples:        20480 | elapsed time per iteration (ms): 39220.1 | learning rate: 1.900E-06 | global batch size:  1024 | lm loss: 5.889977E+00 | loss scale: 1.0 | grad norm: 30.432 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       21/   25000 | consumed samples:        21504 | elapsed time per iteration (ms): 39231.3 | learning rate: 2.000E-06 | global batch size:  1024 | lm loss: 5.874559E+00 | loss scale: 1.0 | grad norm: 31.724 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       22/   25000 | consumed samples:        22528 | elapsed time per iteration (ms): 39386.5 | learning rate: 2.100E-06 | global batch size:  1024 | lm loss: 5.825082E+00 | loss scale: 1.0 | grad norm: 27.763 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       23/   25000 | consumed samples:        23552 | elapsed time per iteration (ms): 39149.8 | learning rate: 2.200E-06 | global batch size:  1024 | lm loss: 5.745293E+00 | loss scale: 1.0 | grad norm: 25.018 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       24/   25000 | consumed samples:        24576 | elapsed time per iteration (ms): 39469.5 | learning rate: 2.300E-06 | global batch size:  1024 | lm loss: 5.689956E+00 | loss scale: 1.0 | grad norm: 22.997 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       25/   25000 | consumed samples:        25600 | elapsed time per iteration (ms): 39219.4 | learning rate: 2.400E-06 | global batch size:  1024 | lm loss: 5.639511E+00 | loss scale: 1.0 | grad norm: 19.709 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       26/   25000 | consumed samples:        26624 | elapsed time per iteration (ms): 39239.9 | learning rate: 2.500E-06 | global batch size:  1024 | lm loss: 5.606865E+00 | loss scale: 1.0 | grad norm: 18.404 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       27/   25000 | consumed samples:        27648 | elapsed time per iteration (ms): 39282.1 | learning rate: 2.600E-06 | global batch size:  1024 | lm loss: 5.575204E+00 | loss scale: 1.0 | grad norm: 17.983 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       28/   25000 | consumed samples:        28672 | elapsed time per iteration (ms): 39284.4 | learning rate: 2.700E-06 | global batch size:  1024 | lm loss: 5.401739E+00 | loss scale: 1.0 | grad norm: 14.468 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       29/   25000 | consumed samples:        29696 | elapsed time per iteration (ms): 39167.8 | learning rate: 2.800E-06 | global batch size:  1024 | lm loss: 5.353386E+00 | loss scale: 1.0 | grad norm: 13.688 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       30/   25000 | consumed samples:        30720 | elapsed time per iteration (ms): 39243.4 | learning rate: 2.900E-06 | global batch size:  1024 | lm loss: 5.286372E+00 | loss scale: 1.0 | grad norm: 12.671 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       31/   25000 | consumed samples:        31744 | elapsed time per iteration (ms): 39182.7 | learning rate: 3.000E-06 | global batch size:  1024 | lm loss: 5.241317E+00 | loss scale: 1.0 | grad norm: 11.909 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       32/   25000 | consumed samples:        32768 | elapsed time per iteration (ms): 39696.5 | learning rate: 3.100E-06 | global batch size:  1024 | lm loss: 5.187243E+00 | loss scale: 1.0 | grad norm: 11.628 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       33/   25000 | consumed samples:        33792 | elapsed time per iteration (ms): 39199.0 | learning rate: 3.200E-06 | global batch size:  1024 | lm loss: 5.161680E+00 | loss scale: 1.0 | grad norm: 10.372 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       34/   25000 | consumed samples:        34816 | elapsed time per iteration (ms): 39174.2 | learning rate: 3.300E-06 | global batch size:  1024 | lm loss: 5.078310E+00 | loss scale: 1.0 | grad norm: 9.918 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       35/   25000 | consumed samples:        35840 | elapsed time per iteration (ms): 39246.3 | learning rate: 3.400E-06 | global batch size:  1024 | lm loss: 5.057772E+00 | loss scale: 1.0 | grad norm: 9.447 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       36/   25000 | consumed samples:        36864 | elapsed time per iteration (ms): 39356.4 | learning rate: 3.500E-06 | global batch size:  1024 | lm loss: 4.995028E+00 | loss scale: 1.0 | grad norm: 8.701 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       37/   25000 | consumed samples:        37888 | elapsed time per iteration (ms): 39225.9 | learning rate: 3.600E-06 | global batch size:  1024 | lm loss: 4.943658E+00 | loss scale: 1.0 | grad norm: 8.173 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       38/   25000 | consumed samples:        38912 | elapsed time per iteration (ms): 39167.5 | learning rate: 3.700E-06 | global batch size:  1024 | lm loss: 4.910548E+00 | loss scale: 1.0 | grad norm: 7.671 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       39/   25000 | consumed samples:        39936 | elapsed time per iteration (ms): 39239.7 | learning rate: 3.800E-06 | global batch size:  1024 | lm loss: 4.842063E+00 | loss scale: 1.0 | grad norm: 7.928 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       40/   25000 | consumed samples:        40960 | elapsed time per iteration (ms): 39625.7 | learning rate: 3.900E-06 | global batch size:  1024 | lm loss: 4.795257E+00 | loss scale: 1.0 | grad norm: 7.511 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       41/   25000 | consumed samples:        41984 | elapsed time per iteration (ms): 39168.1 | learning rate: 4.000E-06 | global batch size:  1024 | lm loss: 4.742664E+00 | loss scale: 1.0 | grad norm: 6.921 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       42/   25000 | consumed samples:        43008 | elapsed time per iteration (ms): 39177.0 | learning rate: 4.100E-06 | global batch size:  1024 | lm loss: 4.702697E+00 | loss scale: 1.0 | grad norm: 6.531 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       43/   25000 | consumed samples:        44032 | elapsed time per iteration (ms): 39243.0 | learning rate: 4.200E-06 | global batch size:  1024 | lm loss: 4.658751E+00 | loss scale: 1.0 | grad norm: 6.179 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       44/   25000 | consumed samples:        45056 | elapsed time per iteration (ms): 39250.7 | learning rate: 4.300E-06 | global batch size:  1024 | lm loss: 4.615132E+00 | loss scale: 1.0 | grad norm: 5.882 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       45/   25000 | consumed samples:        46080 | elapsed time per iteration (ms): 39327.9 | learning rate: 4.400E-06 | global batch size:  1024 | lm loss: 4.578627E+00 | loss scale: 1.0 | grad norm: 5.459 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       46/   25000 | consumed samples:        47104 | elapsed time per iteration (ms): 39169.1 | learning rate: 4.500E-06 | global batch size:  1024 | lm loss: 4.503383E+00 | loss scale: 1.0 | grad norm: 4.980 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       47/   25000 | consumed samples:        48128 | elapsed time per iteration (ms): 39183.6 | learning rate: 4.600E-06 | global batch size:  1024 | lm loss: 4.467312E+00 | loss scale: 1.0 | grad norm: 4.602 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       48/   25000 | consumed samples:        49152 | elapsed time per iteration (ms): 39710.4 | learning rate: 4.700E-06 | global batch size:  1024 | lm loss: 4.429091E+00 | loss scale: 1.0 | grad norm: 4.203 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       49/   25000 | consumed samples:        50176 | elapsed time per iteration (ms): 39236.3 | learning rate: 4.800E-06 | global batch size:  1024 | lm loss: 4.386766E+00 | loss scale: 1.0 | grad norm: 3.935 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       50/   25000 | consumed samples:        51200 | elapsed time per iteration (ms): 39170.7 | learning rate: 4.900E-06 | global batch size:  1024 | lm loss: 4.345587E+00 | loss scale: 1.0 | grad norm: 3.810 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       51/   25000 | consumed samples:        52224 | elapsed time per iteration (ms): 39175.4 | learning rate: 5.000E-06 | global batch size:  1024 | lm loss: 4.327582E+00 | loss scale: 1.0 | grad norm: 3.624 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       52/   25000 | consumed samples:        53248 | elapsed time per iteration (ms): 39177.1 | learning rate: 5.100E-06 | global batch size:  1024 | lm loss: 4.270515E+00 | loss scale: 1.0 | grad norm: 3.363 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       53/   25000 | consumed samples:        54272 | elapsed time per iteration (ms): 39232.9 | learning rate: 5.200E-06 | global batch size:  1024 | lm loss: 4.253125E+00 | loss scale: 1.0 | grad norm: 3.110 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       54/   25000 | consumed samples:        55296 | elapsed time per iteration (ms): 39406.0 | learning rate: 5.300E-06 | global batch size:  1024 | lm loss: 4.209369E+00 | loss scale: 1.0 | grad norm: 2.983 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       55/   25000 | consumed samples:        56320 | elapsed time per iteration (ms): 39180.7 | learning rate: 5.400E-06 | global batch size:  1024 | lm loss: 4.161178E+00 | loss scale: 1.0 | grad norm: 2.798 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       56/   25000 | consumed samples:        57344 | elapsed time per iteration (ms): 39700.9 | learning rate: 5.500E-06 | global batch size:  1024 | lm loss: 4.149167E+00 | loss scale: 1.0 | grad norm: 2.642 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       57/   25000 | consumed samples:        58368 | elapsed time per iteration (ms): 39175.1 | learning rate: 5.600E-06 | global batch size:  1024 | lm loss: 4.096769E+00 | loss scale: 1.0 | grad norm: 2.465 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       58/   25000 | consumed samples:        59392 | elapsed time per iteration (ms): 39261.5 | learning rate: 5.700E-06 | global batch size:  1024 | lm loss: 4.046746E+00 | loss scale: 1.0 | grad norm: 2.380 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       59/   25000 | consumed samples:        60416 | elapsed time per iteration (ms): 39240.8 | learning rate: 5.800E-06 | global batch size:  1024 | lm loss: 4.016097E+00 | loss scale: 1.0 | grad norm: 2.229 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       60/   25000 | consumed samples:        61440 | elapsed time per iteration (ms): 39180.9 | learning rate: 5.900E-06 | global batch size:  1024 | lm loss: 3.983607E+00 | loss scale: 1.0 | grad norm: 2.097 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       61/   25000 | consumed samples:        62464 | elapsed time per iteration (ms): 39176.7 | learning rate: 6.000E-06 | global batch size:  1024 | lm loss: 3.957295E+00 | loss scale: 1.0 | grad norm: 2.073 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       62/   25000 | consumed samples:        63488 | elapsed time per iteration (ms): 39186.8 | learning rate: 6.100E-06 | global batch size:  1024 | lm loss: 3.929102E+00 | loss scale: 1.0 | grad norm: 2.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       63/   25000 | consumed samples:        64512 | elapsed time per iteration (ms): 39353.4 | learning rate: 6.200E-06 | global batch size:  1024 | lm loss: 3.900204E+00 | loss scale: 1.0 | grad norm: 1.824 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       64/   25000 | consumed samples:        65536 | elapsed time per iteration (ms): 39978.7 | learning rate: 6.300E-06 | global batch size:  1024 | lm loss: 3.864577E+00 | loss scale: 1.0 | grad norm: 1.793 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       65/   25000 | consumed samples:        66560 | elapsed time per iteration (ms): 39166.7 | learning rate: 6.400E-06 | global batch size:  1024 | lm loss: 3.841944E+00 | loss scale: 1.0 | grad norm: 1.866 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       66/   25000 | consumed samples:        67584 | elapsed time per iteration (ms): 39161.1 | learning rate: 6.500E-06 | global batch size:  1024 | lm loss: 3.801600E+00 | loss scale: 1.0 | grad norm: 1.785 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       67/   25000 | consumed samples:        68608 | elapsed time per iteration (ms): 39148.5 | learning rate: 6.600E-06 | global batch size:  1024 | lm loss: 3.785682E+00 | loss scale: 1.0 | grad norm: 1.765 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       68/   25000 | consumed samples:        69632 | elapsed time per iteration (ms): 39166.1 | learning rate: 6.700E-06 | global batch size:  1024 | lm loss: 3.761956E+00 | loss scale: 1.0 | grad norm: 1.884 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       69/   25000 | consumed samples:        70656 | elapsed time per iteration (ms): 39300.3 | learning rate: 6.800E-06 | global batch size:  1024 | lm loss: 3.728032E+00 | loss scale: 1.0 | grad norm: 1.940 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       70/   25000 | consumed samples:        71680 | elapsed time per iteration (ms): 39166.1 | learning rate: 6.900E-06 | global batch size:  1024 | lm loss: 3.703054E+00 | loss scale: 1.0 | grad norm: 1.754 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       71/   25000 | consumed samples:        72704 | elapsed time per iteration (ms): 39159.0 | learning rate: 7.000E-06 | global batch size:  1024 | lm loss: 3.666823E+00 | loss scale: 1.0 | grad norm: 1.382 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       72/   25000 | consumed samples:        73728 | elapsed time per iteration (ms): 39863.4 | learning rate: 7.100E-06 | global batch size:  1024 | lm loss: 3.646672E+00 | loss scale: 1.0 | grad norm: 1.370 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       73/   25000 | consumed samples:        74752 | elapsed time per iteration (ms): 39172.1 | learning rate: 7.200E-06 | global batch size:  1024 | lm loss: 3.627954E+00 | loss scale: 1.0 | grad norm: 1.531 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       74/   25000 | consumed samples:        75776 | elapsed time per iteration (ms): 39352.0 | learning rate: 7.300E-06 | global batch size:  1024 | lm loss: 3.614097E+00 | loss scale: 1.0 | grad norm: 1.485 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       75/   25000 | consumed samples:        76800 | elapsed time per iteration (ms): 39172.3 | learning rate: 7.400E-06 | global batch size:  1024 | lm loss: 3.561496E+00 | loss scale: 1.0 | grad norm: 1.368 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       76/   25000 | consumed samples:        77824 | elapsed time per iteration (ms): 39153.6 | learning rate: 7.500E-06 | global batch size:  1024 | lm loss: 3.570945E+00 | loss scale: 1.0 | grad norm: 1.471 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       77/   25000 | consumed samples:        78848 | elapsed time per iteration (ms): 39152.6 | learning rate: 7.600E-06 | global batch size:  1024 | lm loss: 3.514416E+00 | loss scale: 1.0 | grad norm: 1.430 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       78/   25000 | consumed samples:        79872 | elapsed time per iteration (ms): 39159.3 | learning rate: 7.700E-06 | global batch size:  1024 | lm loss: 3.526927E+00 | loss scale: 1.0 | grad norm: 1.279 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       79/   25000 | consumed samples:        80896 | elapsed time per iteration (ms): 39246.2 | learning rate: 7.800E-06 | global batch size:  1024 | lm loss: 3.476391E+00 | loss scale: 1.0 | grad norm: 1.269 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       80/   25000 | consumed samples:        81920 | elapsed time per iteration (ms): 39618.0 | learning rate: 7.900E-06 | global batch size:  1024 | lm loss: 3.460512E+00 | loss scale: 1.0 | grad norm: 1.087 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       81/   25000 | consumed samples:        82944 | elapsed time per iteration (ms): 39387.9 | learning rate: 8.000E-06 | global batch size:  1024 | lm loss: 3.452159E+00 | loss scale: 1.0 | grad norm: 1.037 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       82/   25000 | consumed samples:        83968 | elapsed time per iteration (ms): 39161.8 | learning rate: 8.100E-06 | global batch size:  1024 | lm loss: 3.441804E+00 | loss scale: 1.0 | grad norm: 1.359 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       83/   25000 | consumed samples:        84992 | elapsed time per iteration (ms): 39152.0 | learning rate: 8.200E-06 | global batch size:  1024 | lm loss: 3.423817E+00 | loss scale: 1.0 | grad norm: 1.671 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       84/   25000 | consumed samples:        86016 | elapsed time per iteration (ms): 39247.3 | learning rate: 8.300E-06 | global batch size:  1024 | lm loss: 3.404879E+00 | loss scale: 1.0 | grad norm: 1.129 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       85/   25000 | consumed samples:        87040 | elapsed time per iteration (ms): 39264.1 | learning rate: 8.400E-06 | global batch size:  1024 | lm loss: 3.379033E+00 | loss scale: 1.0 | grad norm: 1.424 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       86/   25000 | consumed samples:        88064 | elapsed time per iteration (ms): 39187.1 | learning rate: 8.500E-06 | global batch size:  1024 | lm loss: 3.364731E+00 | loss scale: 1.0 | grad norm: 1.321 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       87/   25000 | consumed samples:        89088 | elapsed time per iteration (ms): 39197.2 | learning rate: 8.600E-06 | global batch size:  1024 | lm loss: 3.350412E+00 | loss scale: 1.0 | grad norm: 1.248 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       88/   25000 | consumed samples:        90112 | elapsed time per iteration (ms): 39483.7 | learning rate: 8.700E-06 | global batch size:  1024 | lm loss: 3.335932E+00 | loss scale: 1.0 | grad norm: 1.191 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       89/   25000 | consumed samples:        91136 | elapsed time per iteration (ms): 39460.4 | learning rate: 8.800E-06 | global batch size:  1024 | lm loss: 3.306145E+00 | loss scale: 1.0 | grad norm: 1.284 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       90/   25000 | consumed samples:        92160 | elapsed time per iteration (ms): 39381.0 | learning rate: 8.900E-06 | global batch size:  1024 | lm loss: 3.295220E+00 | loss scale: 1.0 | grad norm: 1.231 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       91/   25000 | consumed samples:        93184 | elapsed time per iteration (ms): 39271.0 | learning rate: 9.000E-06 | global batch size:  1024 | lm loss: 3.284987E+00 | loss scale: 1.0 | grad norm: 1.150 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       92/   25000 | consumed samples:        94208 | elapsed time per iteration (ms): 39190.0 | learning rate: 9.100E-06 | global batch size:  1024 | lm loss: 3.269663E+00 | loss scale: 1.0 | grad norm: 1.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       93/   25000 | consumed samples:        95232 | elapsed time per iteration (ms): 39272.1 | learning rate: 9.200E-06 | global batch size:  1024 | lm loss: 3.283159E+00 | loss scale: 1.0 | grad norm: 1.028 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       94/   25000 | consumed samples:        96256 | elapsed time per iteration (ms): 39187.2 | learning rate: 9.300E-06 | global batch size:  1024 | lm loss: 3.252340E+00 | loss scale: 1.0 | grad norm: 1.056 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       95/   25000 | consumed samples:        97280 | elapsed time per iteration (ms): 39181.6 | learning rate: 9.400E-06 | global batch size:  1024 | lm loss: 3.229371E+00 | loss scale: 1.0 | grad norm: 0.912 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       96/   25000 | consumed samples:        98304 | elapsed time per iteration (ms): 39455.8 | learning rate: 9.500E-06 | global batch size:  1024 | lm loss: 3.225707E+00 | loss scale: 1.0 | grad norm: 1.176 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       97/   25000 | consumed samples:        99328 | elapsed time per iteration (ms): 39558.7 | learning rate: 9.600E-06 | global batch size:  1024 | lm loss: 3.205904E+00 | loss scale: 1.0 | grad norm: 0.971 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       98/   25000 | consumed samples:       100352 | elapsed time per iteration (ms): 39254.1 | learning rate: 9.700E-06 | global batch size:  1024 | lm loss: 3.203424E+00 | loss scale: 1.0 | grad norm: 0.957 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       99/   25000 | consumed samples:       101376 | elapsed time per iteration (ms): 39284.9 | learning rate: 9.800E-06 | global batch size:  1024 | lm loss: 3.183941E+00 | loss scale: 1.0 | grad norm: 0.852 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      100/   25000 | consumed samples:       102400 | elapsed time per iteration (ms): 39247.1 | learning rate: 9.900E-06 | global batch size:  1024 | lm loss: 3.181735E+00 | loss scale: 1.0 | grad norm: 1.120 | number of skipped iterations:   0 | number of nan iterations:   0 |
-----------------------------------------------------------------------------------------------
 validation loss at iteration 100 | lm loss value: 3.146535E+00 | lm loss PPL: 2.325534E+01 | 
-----------------------------------------------------------------------------------------------
 iteration      101/   25000 | consumed samples:       103424 | elapsed time per iteration (ms): 174131.5 | learning rate: 1.000E-05 | global batch size:  1024 | lm loss: 3.160947E+00 | loss scale: 1.0 | grad norm: 1.493 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      102/   25000 | consumed samples:       104448 | elapsed time per iteration (ms): 39179.9 | learning rate: 1.010E-05 | global batch size:  1024 | lm loss: 3.141949E+00 | loss scale: 1.0 | grad norm: 1.013 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      103/   25000 | consumed samples:       105472 | elapsed time per iteration (ms): 39250.9 | learning rate: 1.020E-05 | global batch size:  1024 | lm loss: 3.139942E+00 | loss scale: 1.0 | grad norm: 1.165 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      104/   25000 | consumed samples:       106496 | elapsed time per iteration (ms): 39422.1 | learning rate: 1.030E-05 | global batch size:  1024 | lm loss: 3.124120E+00 | loss scale: 1.0 | grad norm: 0.905 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      105/   25000 | consumed samples:       107520 | elapsed time per iteration (ms): 39553.1 | learning rate: 1.040E-05 | global batch size:  1024 | lm loss: 3.093058E+00 | loss scale: 1.0 | grad norm: 1.274 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      106/   25000 | consumed samples:       108544 | elapsed time per iteration (ms): 39274.4 | learning rate: 1.050E-05 | global batch size:  1024 | lm loss: 3.109501E+00 | loss scale: 1.0 | grad norm: 0.811 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      107/   25000 | consumed samples:       109568 | elapsed time per iteration (ms): 39214.5 | learning rate: 1.060E-05 | global batch size:  1024 | lm loss: 3.082870E+00 | loss scale: 1.0 | grad norm: 0.998 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      108/   25000 | consumed samples:       110592 | elapsed time per iteration (ms): 39352.5 | learning rate: 1.070E-05 | global batch size:  1024 | lm loss: 3.064459E+00 | loss scale: 1.0 | grad norm: 0.900 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      109/   25000 | consumed samples:       111616 | elapsed time per iteration (ms): 39278.7 | learning rate: 1.080E-05 | global batch size:  1024 | lm loss: 3.090002E+00 | loss scale: 1.0 | grad norm: 1.129 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      110/   25000 | consumed samples:       112640 | elapsed time per iteration (ms): 39180.9 | learning rate: 1.090E-05 | global batch size:  1024 | lm loss: 3.062523E+00 | loss scale: 1.0 | grad norm: 1.264 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      111/   25000 | consumed samples:       113664 | elapsed time per iteration (ms): 39194.4 | learning rate: 1.100E-05 | global batch size:  1024 | lm loss: 3.042269E+00 | loss scale: 1.0 | grad norm: 0.978 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      112/   25000 | consumed samples:       114688 | elapsed time per iteration (ms): 39336.1 | learning rate: 1.110E-05 | global batch size:  1024 | lm loss: 3.044331E+00 | loss scale: 1.0 | grad norm: 1.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      113/   25000 | consumed samples:       115712 | elapsed time per iteration (ms): 39759.9 | learning rate: 1.120E-05 | global batch size:  1024 | lm loss: 3.025613E+00 | loss scale: 1.0 | grad norm: 1.169 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      114/   25000 | consumed samples:       116736 | elapsed time per iteration (ms): 39200.0 | learning rate: 1.130E-05 | global batch size:  1024 | lm loss: 3.022300E+00 | loss scale: 1.0 | grad norm: 1.143 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      115/   25000 | consumed samples:       117760 | elapsed time per iteration (ms): 39197.4 | learning rate: 1.140E-05 | global batch size:  1024 | lm loss: 3.019501E+00 | loss scale: 1.0 | grad norm: 0.873 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      116/   25000 | consumed samples:       118784 | elapsed time per iteration (ms): 39207.9 | learning rate: 1.150E-05 | global batch size:  1024 | lm loss: 3.003156E+00 | loss scale: 1.0 | grad norm: 0.776 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      117/   25000 | consumed samples:       119808 | elapsed time per iteration (ms): 39353.2 | learning rate: 1.160E-05 | global batch size:  1024 | lm loss: 2.991391E+00 | loss scale: 1.0 | grad norm: 0.920 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      118/   25000 | consumed samples:       120832 | elapsed time per iteration (ms): 39332.7 | learning rate: 1.170E-05 | global batch size:  1024 | lm loss: 2.993343E+00 | loss scale: 1.0 | grad norm: 0.919 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      119/   25000 | consumed samples:       121856 | elapsed time per iteration (ms): 39204.3 | learning rate: 1.180E-05 | global batch size:  1024 | lm loss: 2.988041E+00 | loss scale: 1.0 | grad norm: 1.139 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      120/   25000 | consumed samples:       122880 | elapsed time per iteration (ms): 39304.8 | learning rate: 1.190E-05 | global batch size:  1024 | lm loss: 2.988046E+00 | loss scale: 1.0 | grad norm: 1.053 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      121/   25000 | consumed samples:       123904 | elapsed time per iteration (ms): 39707.2 | learning rate: 1.200E-05 | global batch size:  1024 | lm loss: 2.963606E+00 | loss scale: 1.0 | grad norm: 1.359 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      122/   25000 | consumed samples:       124928 | elapsed time per iteration (ms): 39269.8 | learning rate: 1.210E-05 | global batch size:  1024 | lm loss: 2.961107E+00 | loss scale: 1.0 | grad norm: 0.702 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      123/   25000 | consumed samples:       125952 | elapsed time per iteration (ms): 39479.2 | learning rate: 1.220E-05 | global batch size:  1024 | lm loss: 2.944795E+00 | loss scale: 1.0 | grad norm: 1.046 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      124/   25000 | consumed samples:       126976 | elapsed time per iteration (ms): 39197.9 | learning rate: 1.230E-05 | global batch size:  1024 | lm loss: 2.922017E+00 | loss scale: 1.0 | grad norm: 1.470 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      125/   25000 | consumed samples:       128000 | elapsed time per iteration (ms): 39180.6 | learning rate: 1.240E-05 | global batch size:  1024 | lm loss: 2.929122E+00 | loss scale: 1.0 | grad norm: 0.848 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      126/   25000 | consumed samples:       129024 | elapsed time per iteration (ms): 39182.1 | learning rate: 1.250E-05 | global batch size:  1024 | lm loss: 2.928975E+00 | loss scale: 1.0 | grad norm: 1.312 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      127/   25000 | consumed samples:       130048 | elapsed time per iteration (ms): 39441.2 | learning rate: 1.260E-05 | global batch size:  1024 | lm loss: 2.906222E+00 | loss scale: 1.0 | grad norm: 1.025 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      128/   25000 | consumed samples:       131072 | elapsed time per iteration (ms): 39270.1 | learning rate: 1.270E-05 | global batch size:  1024 | lm loss: 2.909553E+00 | loss scale: 1.0 | grad norm: 1.078 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      129/   25000 | consumed samples:       132096 | elapsed time per iteration (ms): 39803.0 | learning rate: 1.280E-05 | global batch size:  1024 | lm loss: 2.893048E+00 | loss scale: 1.0 | grad norm: 0.831 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      130/   25000 | consumed samples:       133120 | elapsed time per iteration (ms): 39186.5 | learning rate: 1.290E-05 | global batch size:  1024 | lm loss: 2.891291E+00 | loss scale: 1.0 | grad norm: 1.032 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      131/   25000 | consumed samples:       134144 | elapsed time per iteration (ms): 39205.4 | learning rate: 1.300E-05 | global batch size:  1024 | lm loss: 2.885561E+00 | loss scale: 1.0 | grad norm: 1.033 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      132/   25000 | consumed samples:       135168 | elapsed time per iteration (ms): 39204.8 | learning rate: 1.310E-05 | global batch size:  1024 | lm loss: 2.871387E+00 | loss scale: 1.0 | grad norm: 0.958 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      133/   25000 | consumed samples:       136192 | elapsed time per iteration (ms): 39344.6 | learning rate: 1.320E-05 | global batch size:  1024 | lm loss: 2.857841E+00 | loss scale: 1.0 | grad norm: 0.939 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      134/   25000 | consumed samples:       137216 | elapsed time per iteration (ms): 39217.8 | learning rate: 1.330E-05 | global batch size:  1024 | lm loss: 2.852279E+00 | loss scale: 1.0 | grad norm: 0.816 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      135/   25000 | consumed samples:       138240 | elapsed time per iteration (ms): 39206.6 | learning rate: 1.340E-05 | global batch size:  1024 | lm loss: 2.849018E+00 | loss scale: 1.0 | grad norm: 0.963 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      136/   25000 | consumed samples:       139264 | elapsed time per iteration (ms): 39361.3 | learning rate: 1.350E-05 | global batch size:  1024 | lm loss: 2.839180E+00 | loss scale: 1.0 | grad norm: 0.956 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      137/   25000 | consumed samples:       140288 | elapsed time per iteration (ms): 39733.1 | learning rate: 1.360E-05 | global batch size:  1024 | lm loss: 2.854729E+00 | loss scale: 1.0 | grad norm: 1.139 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      138/   25000 | consumed samples:       141312 | elapsed time per iteration (ms): 39340.5 | learning rate: 1.370E-05 | global batch size:  1024 | lm loss: 2.832422E+00 | loss scale: 1.0 | grad norm: 0.812 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      139/   25000 | consumed samples:       142336 | elapsed time per iteration (ms): 39219.3 | learning rate: 1.380E-05 | global batch size:  1024 | lm loss: 2.806390E+00 | loss scale: 1.0 | grad norm: 0.892 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      140/   25000 | consumed samples:       143360 | elapsed time per iteration (ms): 39197.6 | learning rate: 1.390E-05 | global batch size:  1024 | lm loss: 2.822145E+00 | loss scale: 1.0 | grad norm: 0.854 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      141/   25000 | consumed samples:       144384 | elapsed time per iteration (ms): 39182.4 | learning rate: 1.400E-05 | global batch size:  1024 | lm loss: 2.825442E+00 | loss scale: 1.0 | grad norm: 0.902 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      142/   25000 | consumed samples:       145408 | elapsed time per iteration (ms): 39247.5 | learning rate: 1.410E-05 | global batch size:  1024 | lm loss: 2.790699E+00 | loss scale: 1.0 | grad norm: 0.828 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      143/   25000 | consumed samples:       146432 | elapsed time per iteration (ms): 39273.2 | learning rate: 1.420E-05 | global batch size:  1024 | lm loss: 2.788152E+00 | loss scale: 1.0 | grad norm: 0.755 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      144/   25000 | consumed samples:       147456 | elapsed time per iteration (ms): 39229.9 | learning rate: 1.430E-05 | global batch size:  1024 | lm loss: 2.798440E+00 | loss scale: 1.0 | grad norm: 0.672 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      145/   25000 | consumed samples:       148480 | elapsed time per iteration (ms): 39980.1 | learning rate: 1.440E-05 | global batch size:  1024 | lm loss: 2.786311E+00 | loss scale: 1.0 | grad norm: 0.750 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      146/   25000 | consumed samples:       149504 | elapsed time per iteration (ms): 39235.1 | learning rate: 1.450E-05 | global batch size:  1024 | lm loss: 2.781289E+00 | loss scale: 1.0 | grad norm: 0.796 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      147/   25000 | consumed samples:       150528 | elapsed time per iteration (ms): 39306.1 | learning rate: 1.460E-05 | global batch size:  1024 | lm loss: 2.789935E+00 | loss scale: 1.0 | grad norm: 0.809 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      148/   25000 | consumed samples:       151552 | elapsed time per iteration (ms): 39240.1 | learning rate: 1.470E-05 | global batch size:  1024 | lm loss: 2.756589E+00 | loss scale: 1.0 | grad norm: 0.665 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      149/   25000 | consumed samples:       152576 | elapsed time per iteration (ms): 39305.4 | learning rate: 1.480E-05 | global batch size:  1024 | lm loss: 2.782149E+00 | loss scale: 1.0 | grad norm: 0.745 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      150/   25000 | consumed samples:       153600 | elapsed time per iteration (ms): 39241.1 | learning rate: 1.490E-05 | global batch size:  1024 | lm loss: 2.765561E+00 | loss scale: 1.0 | grad norm: 0.839 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      151/   25000 | consumed samples:       154624 | elapsed time per iteration (ms): 39257.0 | learning rate: 1.500E-05 | global batch size:  1024 | lm loss: 2.757075E+00 | loss scale: 1.0 | grad norm: 0.898 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      152/   25000 | consumed samples:       155648 | elapsed time per iteration (ms): 39300.4 | learning rate: 1.510E-05 | global batch size:  1024 | lm loss: 2.746478E+00 | loss scale: 1.0 | grad norm: 1.088 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      153/   25000 | consumed samples:       156672 | elapsed time per iteration (ms): 39809.9 | learning rate: 1.520E-05 | global batch size:  1024 | lm loss: 2.735441E+00 | loss scale: 1.0 | grad norm: 1.159 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      154/   25000 | consumed samples:       157696 | elapsed time per iteration (ms): 39464.0 | learning rate: 1.530E-05 | global batch size:  1024 | lm loss: 2.737188E+00 | loss scale: 1.0 | grad norm: 0.950 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      155/   25000 | consumed samples:       158720 | elapsed time per iteration (ms): 39219.3 | learning rate: 1.540E-05 | global batch size:  1024 | lm loss: 2.717506E+00 | loss scale: 1.0 | grad norm: 0.952 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      156/   25000 | consumed samples:       159744 | elapsed time per iteration (ms): 39206.9 | learning rate: 1.550E-05 | global batch size:  1024 | lm loss: 2.730022E+00 | loss scale: 1.0 | grad norm: 0.911 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      157/   25000 | consumed samples:       160768 | elapsed time per iteration (ms): 39305.2 | learning rate: 1.560E-05 | global batch size:  1024 | lm loss: 2.703797E+00 | loss scale: 1.0 | grad norm: 0.742 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      158/   25000 | consumed samples:       161792 | elapsed time per iteration (ms): 39230.2 | learning rate: 1.570E-05 | global batch size:  1024 | lm loss: 2.714159E+00 | loss scale: 1.0 | grad norm: 0.642 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      159/   25000 | consumed samples:       162816 | elapsed time per iteration (ms): 39282.1 | learning rate: 1.580E-05 | global batch size:  1024 | lm loss: 2.725421E+00 | loss scale: 1.0 | grad norm: 0.813 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      160/   25000 | consumed samples:       163840 | elapsed time per iteration (ms): 39215.7 | learning rate: 1.590E-05 | global batch size:  1024 | lm loss: 2.721737E+00 | loss scale: 1.0 | grad norm: 0.825 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      161/   25000 | consumed samples:       164864 | elapsed time per iteration (ms): 39795.0 | learning rate: 1.600E-05 | global batch size:  1024 | lm loss: 2.677075E+00 | loss scale: 1.0 | grad norm: 0.821 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      162/   25000 | consumed samples:       165888 | elapsed time per iteration (ms): 39292.4 | learning rate: 1.610E-05 | global batch size:  1024 | lm loss: 2.707757E+00 | loss scale: 1.0 | grad norm: 0.884 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      163/   25000 | consumed samples:       166912 | elapsed time per iteration (ms): 39389.5 | learning rate: 1.620E-05 | global batch size:  1024 | lm loss: 2.693548E+00 | loss scale: 1.0 | grad norm: 0.995 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      164/   25000 | consumed samples:       167936 | elapsed time per iteration (ms): 39289.9 | learning rate: 1.630E-05 | global batch size:  1024 | lm loss: 2.691104E+00 | loss scale: 1.0 | grad norm: 1.141 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      165/   25000 | consumed samples:       168960 | elapsed time per iteration (ms): 39211.3 | learning rate: 1.640E-05 | global batch size:  1024 | lm loss: 2.684292E+00 | loss scale: 1.0 | grad norm: 0.946 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      166/   25000 | consumed samples:       169984 | elapsed time per iteration (ms): 39268.6 | learning rate: 1.650E-05 | global batch size:  1024 | lm loss: 2.670516E+00 | loss scale: 1.0 | grad norm: 0.924 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      167/   25000 | consumed samples:       171008 | elapsed time per iteration (ms): 39290.1 | learning rate: 1.660E-05 | global batch size:  1024 | lm loss: 2.677655E+00 | loss scale: 1.0 | grad norm: 0.939 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      168/   25000 | consumed samples:       172032 | elapsed time per iteration (ms): 39220.7 | learning rate: 1.670E-05 | global batch size:  1024 | lm loss: 2.657283E+00 | loss scale: 1.0 | grad norm: 1.048 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      169/   25000 | consumed samples:       173056 | elapsed time per iteration (ms): 39729.2 | learning rate: 1.680E-05 | global batch size:  1024 | lm loss: 2.663474E+00 | loss scale: 1.0 | grad norm: 1.100 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      170/   25000 | consumed samples:       174080 | elapsed time per iteration (ms): 39368.5 | learning rate: 1.690E-05 | global batch size:  1024 | lm loss: 2.633128E+00 | loss scale: 1.0 | grad norm: 0.831 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      171/   25000 | consumed samples:       175104 | elapsed time per iteration (ms): 39210.0 | learning rate: 1.700E-05 | global batch size:  1024 | lm loss: 2.655905E+00 | loss scale: 1.0 | grad norm: 0.779 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      172/   25000 | consumed samples:       176128 | elapsed time per iteration (ms): 39388.0 | learning rate: 1.710E-05 | global batch size:  1024 | lm loss: 2.644688E+00 | loss scale: 1.0 | grad norm: 1.074 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      173/   25000 | consumed samples:       177152 | elapsed time per iteration (ms): 39280.7 | learning rate: 1.720E-05 | global batch size:  1024 | lm loss: 2.646265E+00 | loss scale: 1.0 | grad norm: 0.927 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      174/   25000 | consumed samples:       178176 | elapsed time per iteration (ms): 39214.4 | learning rate: 1.730E-05 | global batch size:  1024 | lm loss: 2.643748E+00 | loss scale: 1.0 | grad norm: 0.964 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      175/   25000 | consumed samples:       179200 | elapsed time per iteration (ms): 39278.1 | learning rate: 1.740E-05 | global batch size:  1024 | lm loss: 2.642244E+00 | loss scale: 1.0 | grad norm: 0.997 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      176/   25000 | consumed samples:       180224 | elapsed time per iteration (ms): 39220.1 | learning rate: 1.750E-05 | global batch size:  1024 | lm loss: 2.638056E+00 | loss scale: 1.0 | grad norm: 1.055 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      177/   25000 | consumed samples:       181248 | elapsed time per iteration (ms): 39712.5 | learning rate: 1.760E-05 | global batch size:  1024 | lm loss: 2.644969E+00 | loss scale: 1.0 | grad norm: 0.906 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      178/   25000 | consumed samples:       182272 | elapsed time per iteration (ms): 39277.0 | learning rate: 1.770E-05 | global batch size:  1024 | lm loss: 2.633413E+00 | loss scale: 1.0 | grad norm: 0.797 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      179/   25000 | consumed samples:       183296 | elapsed time per iteration (ms): 39227.2 | learning rate: 1.780E-05 | global batch size:  1024 | lm loss: 2.617664E+00 | loss scale: 1.0 | grad norm: 0.874 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      180/   25000 | consumed samples:       184320 | elapsed time per iteration (ms): 39291.4 | learning rate: 1.790E-05 | global batch size:  1024 | lm loss: 2.617121E+00 | loss scale: 1.0 | grad norm: 0.753 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      181/   25000 | consumed samples:       185344 | elapsed time per iteration (ms): 39289.2 | learning rate: 1.800E-05 | global batch size:  1024 | lm loss: 2.590385E+00 | loss scale: 1.0 | grad norm: 0.850 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      182/   25000 | consumed samples:       186368 | elapsed time per iteration (ms): 39361.7 | learning rate: 1.810E-05 | global batch size:  1024 | lm loss: 2.590152E+00 | loss scale: 1.0 | grad norm: 1.087 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      183/   25000 | consumed samples:       187392 | elapsed time per iteration (ms): 39230.5 | learning rate: 1.820E-05 | global batch size:  1024 | lm loss: 2.601024E+00 | loss scale: 1.0 | grad norm: 1.079 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      184/   25000 | consumed samples:       188416 | elapsed time per iteration (ms): 39235.9 | learning rate: 1.830E-05 | global batch size:  1024 | lm loss: 2.601751E+00 | loss scale: 1.0 | grad norm: 1.252 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      185/   25000 | consumed samples:       189440 | elapsed time per iteration (ms): 39511.7 | learning rate: 1.840E-05 | global batch size:  1024 | lm loss: 2.587920E+00 | loss scale: 1.0 | grad norm: 0.755 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      186/   25000 | consumed samples:       190464 | elapsed time per iteration (ms): 39566.4 | learning rate: 1.850E-05 | global batch size:  1024 | lm loss: 2.593705E+00 | loss scale: 1.0 | grad norm: 1.062 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      187/   25000 | consumed samples:       191488 | elapsed time per iteration (ms): 39284.8 | learning rate: 1.860E-05 | global batch size:  1024 | lm loss: 2.588098E+00 | loss scale: 1.0 | grad norm: 0.912 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      188/   25000 | consumed samples:       192512 | elapsed time per iteration (ms): 39203.3 | learning rate: 1.870E-05 | global batch size:  1024 | lm loss: 2.582868E+00 | loss scale: 1.0 | grad norm: 0.944 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      189/   25000 | consumed samples:       193536 | elapsed time per iteration (ms): 39197.8 | learning rate: 1.880E-05 | global batch size:  1024 | lm loss: 2.569932E+00 | loss scale: 1.0 | grad norm: 0.855 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      190/   25000 | consumed samples:       194560 | elapsed time per iteration (ms): 39298.5 | learning rate: 1.890E-05 | global batch size:  1024 | lm loss: 2.587777E+00 | loss scale: 1.0 | grad norm: 0.962 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      191/   25000 | consumed samples:       195584 | elapsed time per iteration (ms): 39355.7 | learning rate: 1.900E-05 | global batch size:  1024 | lm loss: 2.570746E+00 | loss scale: 1.0 | grad norm: 0.922 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      192/   25000 | consumed samples:       196608 | elapsed time per iteration (ms): 39279.0 | learning rate: 1.910E-05 | global batch size:  1024 | lm loss: 2.584377E+00 | loss scale: 1.0 | grad norm: 1.064 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      193/   25000 | consumed samples:       197632 | elapsed time per iteration (ms): 39483.4 | learning rate: 1.920E-05 | global batch size:  1024 | lm loss: 2.580087E+00 | loss scale: 1.0 | grad norm: 0.884 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      194/   25000 | consumed samples:       198656 | elapsed time per iteration (ms): 39489.2 | learning rate: 1.930E-05 | global batch size:  1024 | lm loss: 2.562219E+00 | loss scale: 1.0 | grad norm: 0.706 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      195/   25000 | consumed samples:       199680 | elapsed time per iteration (ms): 39192.0 | learning rate: 1.940E-05 | global batch size:  1024 | lm loss: 2.562139E+00 | loss scale: 1.0 | grad norm: 0.735 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      196/   25000 | consumed samples:       200704 | elapsed time per iteration (ms): 39344.9 | learning rate: 1.950E-05 | global batch size:  1024 | lm loss: 2.545708E+00 | loss scale: 1.0 | grad norm: 0.829 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      197/   25000 | consumed samples:       201728 | elapsed time per iteration (ms): 39191.6 | learning rate: 1.960E-05 | global batch size:  1024 | lm loss: 2.539500E+00 | loss scale: 1.0 | grad norm: 0.861 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      198/   25000 | consumed samples:       202752 | elapsed time per iteration (ms): 39219.0 | learning rate: 1.970E-05 | global batch size:  1024 | lm loss: 2.545555E+00 | loss scale: 1.0 | grad norm: 0.878 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      199/   25000 | consumed samples:       203776 | elapsed time per iteration (ms): 39328.7 | learning rate: 1.980E-05 | global batch size:  1024 | lm loss: 2.537145E+00 | loss scale: 1.0 | grad norm: 0.958 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      200/   25000 | consumed samples:       204800 | elapsed time per iteration (ms): 39298.5 | learning rate: 1.990E-05 | global batch size:  1024 | lm loss: 2.540467E+00 | loss scale: 1.0 | grad norm: 0.984 | number of skipped iterations:   0 | number of nan iterations:   0 |
-----------------------------------------------------------------------------------------------
 validation loss at iteration 200 | lm loss value: 2.529609E+00 | lm loss PPL: 1.254860E+01 | 
-----------------------------------------------------------------------------------------------
 iteration      201/   25000 | consumed samples:       205824 | elapsed time per iteration (ms): 173950.6 | learning rate: 2.000E-05 | global batch size:  1024 | lm loss: 2.538421E+00 | loss scale: 1.0 | grad norm: 1.045 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      202/   25000 | consumed samples:       206848 | elapsed time per iteration (ms): 39683.4 | learning rate: 2.010E-05 | global batch size:  1024 | lm loss: 2.536777E+00 | loss scale: 1.0 | grad norm: 0.792 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      203/   25000 | consumed samples:       207872 | elapsed time per iteration (ms): 39225.3 | learning rate: 2.020E-05 | global batch size:  1024 | lm loss: 2.535382E+00 | loss scale: 1.0 | grad norm: 0.658 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      204/   25000 | consumed samples:       208896 | elapsed time per iteration (ms): 39214.6 | learning rate: 2.030E-05 | global batch size:  1024 | lm loss: 2.522925E+00 | loss scale: 1.0 | grad norm: 0.896 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      205/   25000 | consumed samples:       209920 | elapsed time per iteration (ms): 39200.2 | learning rate: 2.040E-05 | global batch size:  1024 | lm loss: 2.519657E+00 | loss scale: 1.0 | grad norm: 0.768 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      206/   25000 | consumed samples:       210944 | elapsed time per iteration (ms): 39269.9 | learning rate: 2.050E-05 | global batch size:  1024 | lm loss: 2.512916E+00 | loss scale: 1.0 | grad norm: 0.745 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      207/   25000 | consumed samples:       211968 | elapsed time per iteration (ms): 39270.9 | learning rate: 2.060E-05 | global batch size:  1024 | lm loss: 2.518641E+00 | loss scale: 1.0 | grad norm: 0.775 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      208/   25000 | consumed samples:       212992 | elapsed time per iteration (ms): 39294.4 | learning rate: 2.070E-05 | global batch size:  1024 | lm loss: 2.502554E+00 | loss scale: 1.0 | grad norm: 0.818 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      209/   25000 | consumed samples:       214016 | elapsed time per iteration (ms): 39487.8 | learning rate: 2.080E-05 | global batch size:  1024 | lm loss: 2.520422E+00 | loss scale: 1.0 | grad norm: 0.825 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      210/   25000 | consumed samples:       215040 | elapsed time per iteration (ms): 39726.7 | learning rate: 2.090E-05 | global batch size:  1024 | lm loss: 2.510077E+00 | loss scale: 1.0 | grad norm: 1.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      211/   25000 | consumed samples:       216064 | elapsed time per iteration (ms): 39254.7 | learning rate: 2.100E-05 | global batch size:  1024 | lm loss: 2.515450E+00 | loss scale: 1.0 | grad norm: 1.101 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      212/   25000 | consumed samples:       217088 | elapsed time per iteration (ms): 39272.8 | learning rate: 2.110E-05 | global batch size:  1024 | lm loss: 2.513974E+00 | loss scale: 1.0 | grad norm: 1.067 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      213/   25000 | consumed samples:       218112 | elapsed time per iteration (ms): 39187.5 | learning rate: 2.120E-05 | global batch size:  1024 | lm loss: 2.504406E+00 | loss scale: 1.0 | grad norm: 0.799 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      214/   25000 | consumed samples:       219136 | elapsed time per iteration (ms): 39185.4 | learning rate: 2.130E-05 | global batch size:  1024 | lm loss: 2.499540E+00 | loss scale: 1.0 | grad norm: 0.841 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      215/   25000 | consumed samples:       220160 | elapsed time per iteration (ms): 39177.9 | learning rate: 2.140E-05 | global batch size:  1024 | lm loss: 2.494796E+00 | loss scale: 1.0 | grad norm: 0.962 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      216/   25000 | consumed samples:       221184 | elapsed time per iteration (ms): 39244.3 | learning rate: 2.150E-05 | global batch size:  1024 | lm loss: 2.505530E+00 | loss scale: 1.0 | grad norm: 0.990 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      217/   25000 | consumed samples:       222208 | elapsed time per iteration (ms): 39347.7 | learning rate: 2.160E-05 | global batch size:  1024 | lm loss: 2.489792E+00 | loss scale: 1.0 | grad norm: 0.884 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      218/   25000 | consumed samples:       223232 | elapsed time per iteration (ms): 39853.4 | learning rate: 2.170E-05 | global batch size:  1024 | lm loss: 2.489261E+00 | loss scale: 1.0 | grad norm: 0.712 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      219/   25000 | consumed samples:       224256 | elapsed time per iteration (ms): 39195.1 | learning rate: 2.180E-05 | global batch size:  1024 | lm loss: 2.490226E+00 | loss scale: 1.0 | grad norm: 0.827 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      220/   25000 | consumed samples:       225280 | elapsed time per iteration (ms): 39185.8 | learning rate: 2.190E-05 | global batch size:  1024 | lm loss: 2.486040E+00 | loss scale: 1.0 | grad norm: 0.844 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      221/   25000 | consumed samples:       226304 | elapsed time per iteration (ms): 39262.8 | learning rate: 2.200E-05 | global batch size:  1024 | lm loss: 2.477995E+00 | loss scale: 1.0 | grad norm: 0.801 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      222/   25000 | consumed samples:       227328 | elapsed time per iteration (ms): 39238.6 | learning rate: 2.210E-05 | global batch size:  1024 | lm loss: 2.487920E+00 | loss scale: 1.0 | grad norm: 0.732 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      223/   25000 | consumed samples:       228352 | elapsed time per iteration (ms): 39272.4 | learning rate: 2.220E-05 | global batch size:  1024 | lm loss: 2.471317E+00 | loss scale: 1.0 | grad norm: 0.819 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      224/   25000 | consumed samples:       229376 | elapsed time per iteration (ms): 39201.9 | learning rate: 2.230E-05 | global batch size:  1024 | lm loss: 2.464740E+00 | loss scale: 1.0 | grad norm: 0.859 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      225/   25000 | consumed samples:       230400 | elapsed time per iteration (ms): 39259.7 | learning rate: 2.240E-05 | global batch size:  1024 | lm loss: 2.478991E+00 | loss scale: 1.0 | grad norm: 0.864 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      226/   25000 | consumed samples:       231424 | elapsed time per iteration (ms): 39688.2 | learning rate: 2.250E-05 | global batch size:  1024 | lm loss: 2.470707E+00 | loss scale: 1.0 | grad norm: 0.856 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      227/   25000 | consumed samples:       232448 | elapsed time per iteration (ms): 39343.9 | learning rate: 2.260E-05 | global batch size:  1024 | lm loss: 2.465557E+00 | loss scale: 1.0 | grad norm: 0.875 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      228/   25000 | consumed samples:       233472 | elapsed time per iteration (ms): 39257.1 | learning rate: 2.270E-05 | global batch size:  1024 | lm loss: 2.472053E+00 | loss scale: 1.0 | grad norm: 0.919 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      229/   25000 | consumed samples:       234496 | elapsed time per iteration (ms): 39180.5 | learning rate: 2.280E-05 | global batch size:  1024 | lm loss: 2.469093E+00 | loss scale: 1.0 | grad norm: 0.767 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      230/   25000 | consumed samples:       235520 | elapsed time per iteration (ms): 39204.6 | learning rate: 2.290E-05 | global batch size:  1024 | lm loss: 2.460716E+00 | loss scale: 1.0 | grad norm: 0.980 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      231/   25000 | consumed samples:       236544 | elapsed time per iteration (ms): 39283.3 | learning rate: 2.300E-05 | global batch size:  1024 | lm loss: 2.440804E+00 | loss scale: 1.0 | grad norm: 1.070 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      232/   25000 | consumed samples:       237568 | elapsed time per iteration (ms): 39227.0 | learning rate: 2.310E-05 | global batch size:  1024 | lm loss: 2.445358E+00 | loss scale: 1.0 | grad norm: 0.904 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      233/   25000 | consumed samples:       238592 | elapsed time per iteration (ms): 39292.8 | learning rate: 2.320E-05 | global batch size:  1024 | lm loss: 2.458554E+00 | loss scale: 1.0 | grad norm: 0.929 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      234/   25000 | consumed samples:       239616 | elapsed time per iteration (ms): 39819.1 | learning rate: 2.330E-05 | global batch size:  1024 | lm loss: 2.453139E+00 | loss scale: 1.0 | grad norm: 0.883 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      235/   25000 | consumed samples:       240640 | elapsed time per iteration (ms): 39230.6 | learning rate: 2.340E-05 | global batch size:  1024 | lm loss: 2.447921E+00 | loss scale: 1.0 | grad norm: 0.772 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      236/   25000 | consumed samples:       241664 | elapsed time per iteration (ms): 39476.1 | learning rate: 2.350E-05 | global batch size:  1024 | lm loss: 2.449337E+00 | loss scale: 1.0 | grad norm: 0.845 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      237/   25000 | consumed samples:       242688 | elapsed time per iteration (ms): 39225.3 | learning rate: 2.360E-05 | global batch size:  1024 | lm loss: 2.426696E+00 | loss scale: 1.0 | grad norm: 0.848 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      238/   25000 | consumed samples:       243712 | elapsed time per iteration (ms): 39222.1 | learning rate: 2.370E-05 | global batch size:  1024 | lm loss: 2.432271E+00 | loss scale: 1.0 | grad norm: 0.948 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      239/   25000 | consumed samples:       244736 | elapsed time per iteration (ms): 39288.7 | learning rate: 2.380E-05 | global batch size:  1024 | lm loss: 2.443191E+00 | loss scale: 1.0 | grad norm: 1.016 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      240/   25000 | consumed samples:       245760 | elapsed time per iteration (ms): 39202.1 | learning rate: 2.390E-05 | global batch size:  1024 | lm loss: 2.438699E+00 | loss scale: 1.0 | grad norm: 1.094 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      241/   25000 | consumed samples:       246784 | elapsed time per iteration (ms): 39334.4 | learning rate: 2.400E-05 | global batch size:  1024 | lm loss: 2.434501E+00 | loss scale: 1.0 | grad norm: 0.999 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      242/   25000 | consumed samples:       247808 | elapsed time per iteration (ms): 39707.7 | learning rate: 2.410E-05 | global batch size:  1024 | lm loss: 2.433201E+00 | loss scale: 1.0 | grad norm: 0.812 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      243/   25000 | consumed samples:       248832 | elapsed time per iteration (ms): 39206.6 | learning rate: 2.420E-05 | global batch size:  1024 | lm loss: 2.415711E+00 | loss scale: 1.0 | grad norm: 0.743 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      244/   25000 | consumed samples:       249856 | elapsed time per iteration (ms): 39277.5 | learning rate: 2.430E-05 | global batch size:  1024 | lm loss: 2.422662E+00 | loss scale: 1.0 | grad norm: 0.768 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      245/   25000 | consumed samples:       250880 | elapsed time per iteration (ms): 39289.9 | learning rate: 2.440E-05 | global batch size:  1024 | lm loss: 2.410443E+00 | loss scale: 1.0 | grad norm: 0.804 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      246/   25000 | consumed samples:       251904 | elapsed time per iteration (ms): 39330.4 | learning rate: 2.450E-05 | global batch size:  1024 | lm loss: 2.411587E+00 | loss scale: 1.0 | grad norm: 0.943 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      247/   25000 | consumed samples:       252928 | elapsed time per iteration (ms): 39191.4 | learning rate: 2.460E-05 | global batch size:  1024 | lm loss: 2.414528E+00 | loss scale: 1.0 | grad norm: 1.082 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      248/   25000 | consumed samples:       253952 | elapsed time per iteration (ms): 39205.2 | learning rate: 2.470E-05 | global batch size:  1024 | lm loss: 2.406961E+00 | loss scale: 1.0 | grad norm: 0.939 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      249/   25000 | consumed samples:       254976 | elapsed time per iteration (ms): 39333.0 | learning rate: 2.480E-05 | global batch size:  1024 | lm loss: 2.429425E+00 | loss scale: 1.0 | grad norm: 0.968 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      250/   25000 | consumed samples:       256000 | elapsed time per iteration (ms): 39764.5 | learning rate: 2.490E-05 | global batch size:  1024 | lm loss: 2.415353E+00 | loss scale: 1.0 | grad norm: 0.959 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      251/   25000 | consumed samples:       257024 | elapsed time per iteration (ms): 39162.7 | learning rate: 2.500E-05 | global batch size:  1024 | lm loss: 2.414009E+00 | loss scale: 1.0 | grad norm: 0.818 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      252/   25000 | consumed samples:       258048 | elapsed time per iteration (ms): 39166.0 | learning rate: 2.510E-05 | global batch size:  1024 | lm loss: 2.397561E+00 | loss scale: 1.0 | grad norm: 0.763 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      253/   25000 | consumed samples:       259072 | elapsed time per iteration (ms): 39159.3 | learning rate: 2.520E-05 | global batch size:  1024 | lm loss: 2.418316E+00 | loss scale: 1.0 | grad norm: 0.755 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      254/   25000 | consumed samples:       260096 | elapsed time per iteration (ms): 39256.6 | learning rate: 2.530E-05 | global batch size:  1024 | lm loss: 2.403126E+00 | loss scale: 1.0 | grad norm: 0.743 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      255/   25000 | consumed samples:       261120 | elapsed time per iteration (ms): 39388.4 | learning rate: 2.540E-05 | global batch size:  1024 | lm loss: 2.397789E+00 | loss scale: 1.0 | grad norm: 0.686 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      256/   25000 | consumed samples:       262144 | elapsed time per iteration (ms): 39174.8 | learning rate: 2.550E-05 | global batch size:  1024 | lm loss: 2.391248E+00 | loss scale: 1.0 | grad norm: 0.806 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      257/   25000 | consumed samples:       263168 | elapsed time per iteration (ms): 39237.6 | learning rate: 2.560E-05 | global batch size:  1024 | lm loss: 2.389982E+00 | loss scale: 1.0 | grad norm: 0.865 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      258/   25000 | consumed samples:       264192 | elapsed time per iteration (ms): 39750.8 | learning rate: 2.570E-05 | global batch size:  1024 | lm loss: 2.390703E+00 | loss scale: 1.0 | grad norm: 0.854 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      259/   25000 | consumed samples:       265216 | elapsed time per iteration (ms): 39196.7 | learning rate: 2.580E-05 | global batch size:  1024 | lm loss: 2.392857E+00 | loss scale: 1.0 | grad norm: 0.908 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      260/   25000 | consumed samples:       266240 | elapsed time per iteration (ms): 39341.2 | learning rate: 2.590E-05 | global batch size:  1024 | lm loss: 2.386230E+00 | loss scale: 1.0 | grad norm: 0.985 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      261/   25000 | consumed samples:       267264 | elapsed time per iteration (ms): 39206.6 | learning rate: 2.600E-05 | global batch size:  1024 | lm loss: 2.377812E+00 | loss scale: 1.0 | grad norm: 1.185 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      262/   25000 | consumed samples:       268288 | elapsed time per iteration (ms): 39177.8 | learning rate: 2.610E-05 | global batch size:  1024 | lm loss: 2.382652E+00 | loss scale: 1.0 | grad norm: 0.784 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      263/   25000 | consumed samples:       269312 | elapsed time per iteration (ms): 39256.8 | learning rate: 2.620E-05 | global batch size:  1024 | lm loss: 2.378976E+00 | loss scale: 1.0 | grad norm: 0.894 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      264/   25000 | consumed samples:       270336 | elapsed time per iteration (ms): 39241.0 | learning rate: 2.630E-05 | global batch size:  1024 | lm loss: 2.383760E+00 | loss scale: 1.0 | grad norm: 0.982 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      265/   25000 | consumed samples:       271360 | elapsed time per iteration (ms): 39320.6 | learning rate: 2.640E-05 | global batch size:  1024 | lm loss: 2.371946E+00 | loss scale: 1.0 | grad norm: 0.784 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      266/   25000 | consumed samples:       272384 | elapsed time per iteration (ms): 39837.2 | learning rate: 2.650E-05 | global batch size:  1024 | lm loss: 2.372967E+00 | loss scale: 1.0 | grad norm: 0.894 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      267/   25000 | consumed samples:       273408 | elapsed time per iteration (ms): 39154.5 | learning rate: 2.660E-05 | global batch size:  1024 | lm loss: 2.390092E+00 | loss scale: 1.0 | grad norm: 0.963 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      268/   25000 | consumed samples:       274432 | elapsed time per iteration (ms): 39153.2 | learning rate: 2.670E-05 | global batch size:  1024 | lm loss: 2.353564E+00 | loss scale: 1.0 | grad norm: 0.742 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      269/   25000 | consumed samples:       275456 | elapsed time per iteration (ms): 39134.0 | learning rate: 2.680E-05 | global batch size:  1024 | lm loss: 2.366195E+00 | loss scale: 1.0 | grad norm: 1.034 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      270/   25000 | consumed samples:       276480 | elapsed time per iteration (ms): 39189.2 | learning rate: 2.690E-05 | global batch size:  1024 | lm loss: 2.370196E+00 | loss scale: 1.0 | grad norm: 1.117 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      271/   25000 | consumed samples:       277504 | elapsed time per iteration (ms): 39199.5 | learning rate: 2.700E-05 | global batch size:  1024 | lm loss: 2.361273E+00 | loss scale: 1.0 | grad norm: 0.775 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      272/   25000 | consumed samples:       278528 | elapsed time per iteration (ms): 39231.7 | learning rate: 2.710E-05 | global batch size:  1024 | lm loss: 2.366371E+00 | loss scale: 1.0 | grad norm: 0.936 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      273/   25000 | consumed samples:       279552 | elapsed time per iteration (ms): 39200.7 | learning rate: 2.720E-05 | global batch size:  1024 | lm loss: 2.369704E+00 | loss scale: 1.0 | grad norm: 0.902 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      274/   25000 | consumed samples:       280576 | elapsed time per iteration (ms): 39491.8 | learning rate: 2.730E-05 | global batch size:  1024 | lm loss: 2.363950E+00 | loss scale: 1.0 | grad norm: 0.708 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      275/   25000 | consumed samples:       281600 | elapsed time per iteration (ms): 39436.9 | learning rate: 2.740E-05 | global batch size:  1024 | lm loss: 2.366349E+00 | loss scale: 1.0 | grad norm: 0.741 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      276/   25000 | consumed samples:       282624 | elapsed time per iteration (ms): 39192.8 | learning rate: 2.750E-05 | global batch size:  1024 | lm loss: 2.355345E+00 | loss scale: 1.0 | grad norm: 0.698 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      277/   25000 | consumed samples:       283648 | elapsed time per iteration (ms): 39126.4 | learning rate: 2.760E-05 | global batch size:  1024 | lm loss: 2.354871E+00 | loss scale: 1.0 | grad norm: 0.800 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      278/   25000 | consumed samples:       284672 | elapsed time per iteration (ms): 39127.0 | learning rate: 2.770E-05 | global batch size:  1024 | lm loss: 2.359602E+00 | loss scale: 1.0 | grad norm: 0.864 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      279/   25000 | consumed samples:       285696 | elapsed time per iteration (ms): 39129.7 | learning rate: 2.780E-05 | global batch size:  1024 | lm loss: 2.349811E+00 | loss scale: 1.0 | grad norm: 0.832 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      280/   25000 | consumed samples:       286720 | elapsed time per iteration (ms): 39194.4 | learning rate: 2.790E-05 | global batch size:  1024 | lm loss: 2.365905E+00 | loss scale: 1.0 | grad norm: 0.937 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      281/   25000 | consumed samples:       287744 | elapsed time per iteration (ms): 39294.7 | learning rate: 2.800E-05 | global batch size:  1024 | lm loss: 2.357187E+00 | loss scale: 1.0 | grad norm: 0.899 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      282/   25000 | consumed samples:       288768 | elapsed time per iteration (ms): 39436.3 | learning rate: 2.810E-05 | global batch size:  1024 | lm loss: 2.351297E+00 | loss scale: 1.0 | grad norm: 0.947 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      283/   25000 | consumed samples:       289792 | elapsed time per iteration (ms): 39420.7 | learning rate: 2.820E-05 | global batch size:  1024 | lm loss: 2.356182E+00 | loss scale: 1.0 | grad norm: 1.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      284/   25000 | consumed samples:       290816 | elapsed time per iteration (ms): 39154.7 | learning rate: 2.830E-05 | global batch size:  1024 | lm loss: 2.355090E+00 | loss scale: 1.0 | grad norm: 1.228 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      285/   25000 | consumed samples:       291840 | elapsed time per iteration (ms): 39212.0 | learning rate: 2.840E-05 | global batch size:  1024 | lm loss: 2.337044E+00 | loss scale: 1.0 | grad norm: 0.822 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      286/   25000 | consumed samples:       292864 | elapsed time per iteration (ms): 39157.8 | learning rate: 2.850E-05 | global batch size:  1024 | lm loss: 2.341904E+00 | loss scale: 1.0 | grad norm: 0.814 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      287/   25000 | consumed samples:       293888 | elapsed time per iteration (ms): 39168.5 | learning rate: 2.860E-05 | global batch size:  1024 | lm loss: 2.358099E+00 | loss scale: 1.0 | grad norm: 1.118 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      288/   25000 | consumed samples:       294912 | elapsed time per iteration (ms): 39147.3 | learning rate: 2.870E-05 | global batch size:  1024 | lm loss: 2.335793E+00 | loss scale: 1.0 | grad norm: 0.875 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      289/   25000 | consumed samples:       295936 | elapsed time per iteration (ms): 39148.5 | learning rate: 2.880E-05 | global batch size:  1024 | lm loss: 2.347694E+00 | loss scale: 1.0 | grad norm: 0.770 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      290/   25000 | consumed samples:       296960 | elapsed time per iteration (ms): 39608.3 | learning rate: 2.890E-05 | global batch size:  1024 | lm loss: 2.352688E+00 | loss scale: 1.0 | grad norm: 0.972 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      291/   25000 | consumed samples:       297984 | elapsed time per iteration (ms): 39497.4 | learning rate: 2.900E-05 | global batch size:  1024 | lm loss: 2.336622E+00 | loss scale: 1.0 | grad norm: 0.838 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      292/   25000 | consumed samples:       299008 | elapsed time per iteration (ms): 39281.9 | learning rate: 2.910E-05 | global batch size:  1024 | lm loss: 2.338875E+00 | loss scale: 1.0 | grad norm: 0.828 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      293/   25000 | consumed samples:       300032 | elapsed time per iteration (ms): 39153.8 | learning rate: 2.920E-05 | global batch size:  1024 | lm loss: 2.331981E+00 | loss scale: 1.0 | grad norm: 0.886 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      294/   25000 | consumed samples:       301056 | elapsed time per iteration (ms): 39139.1 | learning rate: 2.930E-05 | global batch size:  1024 | lm loss: 2.327761E+00 | loss scale: 1.0 | grad norm: 0.682 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      295/   25000 | consumed samples:       302080 | elapsed time per iteration (ms): 39210.9 | learning rate: 2.940E-05 | global batch size:  1024 | lm loss: 2.328010E+00 | loss scale: 1.0 | grad norm: 1.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      296/   25000 | consumed samples:       303104 | elapsed time per iteration (ms): 39132.4 | learning rate: 2.950E-05 | global batch size:  1024 | lm loss: 2.326168E+00 | loss scale: 1.0 | grad norm: 0.942 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      297/   25000 | consumed samples:       304128 | elapsed time per iteration (ms): 39200.0 | learning rate: 2.960E-05 | global batch size:  1024 | lm loss: 2.335924E+00 | loss scale: 1.0 | grad norm: 0.811 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      298/   25000 | consumed samples:       305152 | elapsed time per iteration (ms): 39352.3 | learning rate: 2.970E-05 | global batch size:  1024 | lm loss: 2.340332E+00 | loss scale: 1.0 | grad norm: 1.110 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      299/   25000 | consumed samples:       306176 | elapsed time per iteration (ms): 39757.5 | learning rate: 2.980E-05 | global batch size:  1024 | lm loss: 2.318034E+00 | loss scale: 1.0 | grad norm: 0.772 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      300/   25000 | consumed samples:       307200 | elapsed time per iteration (ms): 39179.7 | learning rate: 2.990E-05 | global batch size:  1024 | lm loss: 2.320892E+00 | loss scale: 1.0 | grad norm: 1.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
-----------------------------------------------------------------------------------------------
 validation loss at iteration 300 | lm loss value: 2.304862E+00 | lm loss PPL: 1.002280E+01 | 
-----------------------------------------------------------------------------------------------
 iteration      301/   25000 | consumed samples:       308224 | elapsed time per iteration (ms): 173371.8 | learning rate: 3.000E-05 | global batch size:  1024 | lm loss: 2.317478E+00 | loss scale: 1.0 | grad norm: 0.740 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      302/   25000 | consumed samples:       309248 | elapsed time per iteration (ms): 39194.0 | learning rate: 3.010E-05 | global batch size:  1024 | lm loss: 2.316424E+00 | loss scale: 1.0 | grad norm: 0.859 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      303/   25000 | consumed samples:       310272 | elapsed time per iteration (ms): 39130.9 | learning rate: 3.020E-05 | global batch size:  1024 | lm loss: 2.304013E+00 | loss scale: 1.0 | grad norm: 0.917 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      304/   25000 | consumed samples:       311296 | elapsed time per iteration (ms): 39215.0 | learning rate: 3.030E-05 | global batch size:  1024 | lm loss: 2.307973E+00 | loss scale: 1.0 | grad norm: 0.900 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      305/   25000 | consumed samples:       312320 | elapsed time per iteration (ms): 39121.3 | learning rate: 3.040E-05 | global batch size:  1024 | lm loss: 2.321584E+00 | loss scale: 1.0 | grad norm: 0.821 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      306/   25000 | consumed samples:       313344 | elapsed time per iteration (ms): 39445.4 | learning rate: 3.050E-05 | global batch size:  1024 | lm loss: 2.300610E+00 | loss scale: 1.0 | grad norm: 0.723 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      307/   25000 | consumed samples:       314368 | elapsed time per iteration (ms): 39409.0 | learning rate: 3.060E-05 | global batch size:  1024 | lm loss: 2.308383E+00 | loss scale: 1.0 | grad norm: 0.820 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      308/   25000 | consumed samples:       315392 | elapsed time per iteration (ms): 39230.1 | learning rate: 3.070E-05 | global batch size:  1024 | lm loss: 2.305649E+00 | loss scale: 1.0 | grad norm: 0.752 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      309/   25000 | consumed samples:       316416 | elapsed time per iteration (ms): 39392.5 | learning rate: 3.080E-05 | global batch size:  1024 | lm loss: 2.304993E+00 | loss scale: 1.0 | grad norm: 0.771 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      310/   25000 | consumed samples:       317440 | elapsed time per iteration (ms): 39147.4 | learning rate: 3.090E-05 | global batch size:  1024 | lm loss: 2.303829E+00 | loss scale: 1.0 | grad norm: 0.789 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      311/   25000 | consumed samples:       318464 | elapsed time per iteration (ms): 39143.3 | learning rate: 3.100E-05 | global batch size:  1024 | lm loss: 2.311907E+00 | loss scale: 1.0 | grad norm: 0.977 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      312/   25000 | consumed samples:       319488 | elapsed time per iteration (ms): 39144.8 | learning rate: 3.110E-05 | global batch size:  1024 | lm loss: 2.315240E+00 | loss scale: 1.0 | grad norm: 0.978 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      313/   25000 | consumed samples:       320512 | elapsed time per iteration (ms): 39210.4 | learning rate: 3.120E-05 | global batch size:  1024 | lm loss: 2.293208E+00 | loss scale: 1.0 | grad norm: 0.898 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      314/   25000 | consumed samples:       321536 | elapsed time per iteration (ms): 39434.4 | learning rate: 3.130E-05 | global batch size:  1024 | lm loss: 2.295480E+00 | loss scale: 1.0 | grad norm: 1.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      315/   25000 | consumed samples:       322560 | elapsed time per iteration (ms): 39515.8 | learning rate: 3.140E-05 | global batch size:  1024 | lm loss: 2.307260E+00 | loss scale: 1.0 | grad norm: 1.206 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      316/   25000 | consumed samples:       323584 | elapsed time per iteration (ms): 39161.9 | learning rate: 3.150E-05 | global batch size:  1024 | lm loss: 2.279878E+00 | loss scale: 1.0 | grad norm: 0.794 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      317/   25000 | consumed samples:       324608 | elapsed time per iteration (ms): 39166.1 | learning rate: 3.160E-05 | global batch size:  1024 | lm loss: 2.301230E+00 | loss scale: 1.0 | grad norm: 0.910 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      318/   25000 | consumed samples:       325632 | elapsed time per iteration (ms): 39341.5 | learning rate: 3.170E-05 | global batch size:  1024 | lm loss: 2.292759E+00 | loss scale: 1.0 | grad norm: 1.102 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      319/   25000 | consumed samples:       326656 | elapsed time per iteration (ms): 39323.8 | learning rate: 3.180E-05 | global batch size:  1024 | lm loss: 2.299583E+00 | loss scale: 1.0 | grad norm: 0.856 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      320/   25000 | consumed samples:       327680 | elapsed time per iteration (ms): 39168.1 | learning rate: 3.190E-05 | global batch size:  1024 | lm loss: 2.320975E+00 | loss scale: 1.0 | grad norm: 0.853 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      321/   25000 | consumed samples:       328704 | elapsed time per iteration (ms): 39173.8 | learning rate: 3.200E-05 | global batch size:  1024 | lm loss: 2.275981E+00 | loss scale: 1.0 | grad norm: 0.972 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      322/   25000 | consumed samples:       329728 | elapsed time per iteration (ms): 39324.0 | learning rate: 3.210E-05 | global batch size:  1024 | lm loss: 2.285225E+00 | loss scale: 1.0 | grad norm: 0.957 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      323/   25000 | consumed samples:       330752 | elapsed time per iteration (ms): 39711.7 | learning rate: 3.220E-05 | global batch size:  1024 | lm loss: 2.290578E+00 | loss scale: 1.0 | grad norm: 0.767 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      324/   25000 | consumed samples:       331776 | elapsed time per iteration (ms): 39286.6 | learning rate: 3.230E-05 | global batch size:  1024 | lm loss: 2.290208E+00 | loss scale: 1.0 | grad norm: 0.907 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      325/   25000 | consumed samples:       332800 | elapsed time per iteration (ms): 39199.3 | learning rate: 3.240E-05 | global batch size:  1024 | lm loss: 2.280380E+00 | loss scale: 1.0 | grad norm: 0.636 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      326/   25000 | consumed samples:       333824 | elapsed time per iteration (ms): 39203.0 | learning rate: 3.250E-05 | global batch size:  1024 | lm loss: 2.278413E+00 | loss scale: 1.0 | grad norm: 0.802 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      327/   25000 | consumed samples:       334848 | elapsed time per iteration (ms): 39290.5 | learning rate: 3.260E-05 | global batch size:  1024 | lm loss: 2.293177E+00 | loss scale: 1.0 | grad norm: 0.756 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      328/   25000 | consumed samples:       335872 | elapsed time per iteration (ms): 39285.8 | learning rate: 3.270E-05 | global batch size:  1024 | lm loss: 2.275378E+00 | loss scale: 1.0 | grad norm: 0.700 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      329/   25000 | consumed samples:       336896 | elapsed time per iteration (ms): 39363.8 | learning rate: 3.280E-05 | global batch size:  1024 | lm loss: 2.284586E+00 | loss scale: 1.0 | grad norm: 0.728 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      330/   25000 | consumed samples:       337920 | elapsed time per iteration (ms): 39368.7 | learning rate: 3.290E-05 | global batch size:  1024 | lm loss: 2.264285E+00 | loss scale: 1.0 | grad norm: 0.834 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      331/   25000 | consumed samples:       338944 | elapsed time per iteration (ms): 39656.1 | learning rate: 3.300E-05 | global batch size:  1024 | lm loss: 2.271579E+00 | loss scale: 1.0 | grad norm: 0.924 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      332/   25000 | consumed samples:       339968 | elapsed time per iteration (ms): 39222.2 | learning rate: 3.310E-05 | global batch size:  1024 | lm loss: 2.277453E+00 | loss scale: 1.0 | grad norm: 1.032 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      333/   25000 | consumed samples:       340992 | elapsed time per iteration (ms): 39207.9 | learning rate: 3.320E-05 | global batch size:  1024 | lm loss: 2.274329E+00 | loss scale: 1.0 | grad norm: 0.868 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      334/   25000 | consumed samples:       342016 | elapsed time per iteration (ms): 39355.3 | learning rate: 3.330E-05 | global batch size:  1024 | lm loss: 2.290627E+00 | loss scale: 1.0 | grad norm: 0.839 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      335/   25000 | consumed samples:       343040 | elapsed time per iteration (ms): 39201.2 | learning rate: 3.340E-05 | global batch size:  1024 | lm loss: 2.273570E+00 | loss scale: 1.0 | grad norm: 0.921 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      336/   25000 | consumed samples:       344064 | elapsed time per iteration (ms): 39285.8 | learning rate: 3.350E-05 | global batch size:  1024 | lm loss: 2.267015E+00 | loss scale: 1.0 | grad norm: 0.926 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      337/   25000 | consumed samples:       345088 | elapsed time per iteration (ms): 39262.9 | learning rate: 3.360E-05 | global batch size:  1024 | lm loss: 2.263128E+00 | loss scale: 1.0 | grad norm: 1.048 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      338/   25000 | consumed samples:       346112 | elapsed time per iteration (ms): 39349.9 | learning rate: 3.370E-05 | global batch size:  1024 | lm loss: 2.278339E+00 | loss scale: 1.0 | grad norm: 0.842 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      339/   25000 | consumed samples:       347136 | elapsed time per iteration (ms): 39711.7 | learning rate: 3.380E-05 | global batch size:  1024 | lm loss: 2.277423E+00 | loss scale: 1.0 | grad norm: 0.763 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      340/   25000 | consumed samples:       348160 | elapsed time per iteration (ms): 39171.4 | learning rate: 3.390E-05 | global batch size:  1024 | lm loss: 2.264271E+00 | loss scale: 1.0 | grad norm: 0.971 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      341/   25000 | consumed samples:       349184 | elapsed time per iteration (ms): 39174.9 | learning rate: 3.400E-05 | global batch size:  1024 | lm loss: 2.260683E+00 | loss scale: 1.0 | grad norm: 0.771 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      342/   25000 | consumed samples:       350208 | elapsed time per iteration (ms): 39181.8 | learning rate: 3.410E-05 | global batch size:  1024 | lm loss: 2.254773E+00 | loss scale: 1.0 | grad norm: 0.801 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      343/   25000 | consumed samples:       351232 | elapsed time per iteration (ms): 39190.8 | learning rate: 3.420E-05 | global batch size:  1024 | lm loss: 2.263779E+00 | loss scale: 1.0 | grad norm: 0.922 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      344/   25000 | consumed samples:       352256 | elapsed time per iteration (ms): 39354.7 | learning rate: 3.430E-05 | global batch size:  1024 | lm loss: 2.273162E+00 | loss scale: 1.0 | grad norm: 0.816 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      345/   25000 | consumed samples:       353280 | elapsed time per iteration (ms): 39369.3 | learning rate: 3.440E-05 | global batch size:  1024 | lm loss: 2.268579E+00 | loss scale: 1.0 | grad norm: 0.756 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      346/   25000 | consumed samples:       354304 | elapsed time per iteration (ms): 39434.9 | learning rate: 3.450E-05 | global batch size:  1024 | lm loss: 2.256876E+00 | loss scale: 1.0 | grad norm: 0.662 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      347/   25000 | consumed samples:       355328 | elapsed time per iteration (ms): 39652.0 | learning rate: 3.460E-05 | global batch size:  1024 | lm loss: 2.266856E+00 | loss scale: 1.0 | grad norm: 0.770 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      348/   25000 | consumed samples:       356352 | elapsed time per iteration (ms): 39219.8 | learning rate: 3.470E-05 | global batch size:  1024 | lm loss: 2.266276E+00 | loss scale: 1.0 | grad norm: 0.927 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      349/   25000 | consumed samples:       357376 | elapsed time per iteration (ms): 39275.9 | learning rate: 3.480E-05 | global batch size:  1024 | lm loss: 2.258590E+00 | loss scale: 1.0 | grad norm: 0.965 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      350/   25000 | consumed samples:       358400 | elapsed time per iteration (ms): 39288.0 | learning rate: 3.490E-05 | global batch size:  1024 | lm loss: 2.268678E+00 | loss scale: 1.0 | grad norm: 0.834 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      351/   25000 | consumed samples:       359424 | elapsed time per iteration (ms): 39224.6 | learning rate: 3.500E-05 | global batch size:  1024 | lm loss: 2.267379E+00 | loss scale: 1.0 | grad norm: 0.955 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      352/   25000 | consumed samples:       360448 | elapsed time per iteration (ms): 39218.8 | learning rate: 3.510E-05 | global batch size:  1024 | lm loss: 2.247361E+00 | loss scale: 1.0 | grad norm: 0.760 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      353/   25000 | consumed samples:       361472 | elapsed time per iteration (ms): 39285.0 | learning rate: 3.520E-05 | global batch size:  1024 | lm loss: 2.247047E+00 | loss scale: 1.0 | grad norm: 0.816 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      354/   25000 | consumed samples:       362496 | elapsed time per iteration (ms): 39376.2 | learning rate: 3.530E-05 | global batch size:  1024 | lm loss: 2.260995E+00 | loss scale: 1.0 | grad norm: 0.808 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      355/   25000 | consumed samples:       363520 | elapsed time per iteration (ms): 39778.0 | learning rate: 3.540E-05 | global batch size:  1024 | lm loss: 2.245879E+00 | loss scale: 1.0 | grad norm: 0.651 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      356/   25000 | consumed samples:       364544 | elapsed time per iteration (ms): 39271.7 | learning rate: 3.550E-05 | global batch size:  1024 | lm loss: 2.252765E+00 | loss scale: 1.0 | grad norm: 0.619 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      357/   25000 | consumed samples:       365568 | elapsed time per iteration (ms): 39186.7 | learning rate: 3.560E-05 | global batch size:  1024 | lm loss: 2.242815E+00 | loss scale: 1.0 | grad norm: 0.695 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      358/   25000 | consumed samples:       366592 | elapsed time per iteration (ms): 39385.6 | learning rate: 3.570E-05 | global batch size:  1024 | lm loss: 2.245743E+00 | loss scale: 1.0 | grad norm: 0.691 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      359/   25000 | consumed samples:       367616 | elapsed time per iteration (ms): 39190.7 | learning rate: 3.580E-05 | global batch size:  1024 | lm loss: 2.238404E+00 | loss scale: 1.0 | grad norm: 0.733 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      360/   25000 | consumed samples:       368640 | elapsed time per iteration (ms): 39179.9 | learning rate: 3.590E-05 | global batch size:  1024 | lm loss: 2.246212E+00 | loss scale: 1.0 | grad norm: 0.680 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      361/   25000 | consumed samples:       369664 | elapsed time per iteration (ms): 39264.2 | learning rate: 3.600E-05 | global batch size:  1024 | lm loss: 2.250749E+00 | loss scale: 1.0 | grad norm: 0.752 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      362/   25000 | consumed samples:       370688 | elapsed time per iteration (ms): 39267.3 | learning rate: 3.610E-05 | global batch size:  1024 | lm loss: 2.237761E+00 | loss scale: 1.0 | grad norm: 0.627 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      363/   25000 | consumed samples:       371712 | elapsed time per iteration (ms): 39747.8 | learning rate: 3.620E-05 | global batch size:  1024 | lm loss: 2.232832E+00 | loss scale: 1.0 | grad norm: 0.710 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      364/   25000 | consumed samples:       372736 | elapsed time per iteration (ms): 39433.2 | learning rate: 3.630E-05 | global batch size:  1024 | lm loss: 2.224364E+00 | loss scale: 1.0 | grad norm: 0.864 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      365/   25000 | consumed samples:       373760 | elapsed time per iteration (ms): 39210.8 | learning rate: 3.640E-05 | global batch size:  1024 | lm loss: 2.245681E+00 | loss scale: 1.0 | grad norm: 1.102 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      366/   25000 | consumed samples:       374784 | elapsed time per iteration (ms): 39294.9 | learning rate: 3.650E-05 | global batch size:  1024 | lm loss: 2.247662E+00 | loss scale: 1.0 | grad norm: 0.858 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      367/   25000 | consumed samples:       375808 | elapsed time per iteration (ms): 39224.5 | learning rate: 3.660E-05 | global batch size:  1024 | lm loss: 2.233135E+00 | loss scale: 1.0 | grad norm: 0.779 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      368/   25000 | consumed samples:       376832 | elapsed time per iteration (ms): 39303.1 | learning rate: 3.670E-05 | global batch size:  1024 | lm loss: 2.225916E+00 | loss scale: 1.0 | grad norm: 0.771 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      369/   25000 | consumed samples:       377856 | elapsed time per iteration (ms): 39231.8 | learning rate: 3.680E-05 | global batch size:  1024 | lm loss: 2.238099E+00 | loss scale: 1.0 | grad norm: 0.916 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      370/   25000 | consumed samples:       378880 | elapsed time per iteration (ms): 39303.1 | learning rate: 3.690E-05 | global batch size:  1024 | lm loss: 2.245812E+00 | loss scale: 1.0 | grad norm: 0.912 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      371/   25000 | consumed samples:       379904 | elapsed time per iteration (ms): 39596.5 | learning rate: 3.700E-05 | global batch size:  1024 | lm loss: 2.230386E+00 | loss scale: 1.0 | grad norm: 0.813 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      372/   25000 | consumed samples:       380928 | elapsed time per iteration (ms): 39541.0 | learning rate: 3.710E-05 | global batch size:  1024 | lm loss: 2.243162E+00 | loss scale: 1.0 | grad norm: 0.949 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      373/   25000 | consumed samples:       381952 | elapsed time per iteration (ms): 39353.4 | learning rate: 3.720E-05 | global batch size:  1024 | lm loss: 2.220741E+00 | loss scale: 1.0 | grad norm: 0.975 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      374/   25000 | consumed samples:       382976 | elapsed time per iteration (ms): 39203.3 | learning rate: 3.730E-05 | global batch size:  1024 | lm loss: 2.229712E+00 | loss scale: 1.0 | grad norm: 0.919 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      375/   25000 | consumed samples:       384000 | elapsed time per iteration (ms): 39202.9 | learning rate: 3.740E-05 | global batch size:  1024 | lm loss: 2.228302E+00 | loss scale: 1.0 | grad norm: 0.844 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      376/   25000 | consumed samples:       385024 | elapsed time per iteration (ms): 39288.8 | learning rate: 3.750E-05 | global batch size:  1024 | lm loss: 2.236820E+00 | loss scale: 1.0 | grad norm: 0.883 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      377/   25000 | consumed samples:       386048 | elapsed time per iteration (ms): 39188.2 | learning rate: 3.760E-05 | global batch size:  1024 | lm loss: 2.209968E+00 | loss scale: 1.0 | grad norm: 0.768 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      378/   25000 | consumed samples:       387072 | elapsed time per iteration (ms): 39353.1 | learning rate: 3.770E-05 | global batch size:  1024 | lm loss: 2.214182E+00 | loss scale: 1.0 | grad norm: 0.882 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      379/   25000 | consumed samples:       388096 | elapsed time per iteration (ms): 39447.2 | learning rate: 3.780E-05 | global batch size:  1024 | lm loss: 2.237613E+00 | loss scale: 1.0 | grad norm: 0.920 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      380/   25000 | consumed samples:       389120 | elapsed time per iteration (ms): 39432.8 | learning rate: 3.790E-05 | global batch size:  1024 | lm loss: 2.216760E+00 | loss scale: 1.0 | grad norm: 0.996 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      381/   25000 | consumed samples:       390144 | elapsed time per iteration (ms): 39265.1 | learning rate: 3.800E-05 | global batch size:  1024 | lm loss: 2.232830E+00 | loss scale: 1.0 | grad norm: 0.907 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      382/   25000 | consumed samples:       391168 | elapsed time per iteration (ms): 39362.1 | learning rate: 3.810E-05 | global batch size:  1024 | lm loss: 2.228537E+00 | loss scale: 1.0 | grad norm: 0.858 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      383/   25000 | consumed samples:       392192 | elapsed time per iteration (ms): 39273.7 | learning rate: 3.820E-05 | global batch size:  1024 | lm loss: 2.210893E+00 | loss scale: 1.0 | grad norm: 0.752 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      384/   25000 | consumed samples:       393216 | elapsed time per iteration (ms): 39224.3 | learning rate: 3.830E-05 | global batch size:  1024 | lm loss: 2.217319E+00 | loss scale: 1.0 | grad norm: 0.825 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      385/   25000 | consumed samples:       394240 | elapsed time per iteration (ms): 39209.1 | learning rate: 3.840E-05 | global batch size:  1024 | lm loss: 2.221528E+00 | loss scale: 1.0 | grad norm: 0.812 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      386/   25000 | consumed samples:       395264 | elapsed time per iteration (ms): 39282.9 | learning rate: 3.850E-05 | global batch size:  1024 | lm loss: 2.231448E+00 | loss scale: 1.0 | grad norm: 0.661 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      387/   25000 | consumed samples:       396288 | elapsed time per iteration (ms): 39619.3 | learning rate: 3.860E-05 | global batch size:  1024 | lm loss: 2.211853E+00 | loss scale: 1.0 | grad norm: 0.828 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      388/   25000 | consumed samples:       397312 | elapsed time per iteration (ms): 39518.6 | learning rate: 3.870E-05 | global batch size:  1024 | lm loss: 2.195087E+00 | loss scale: 1.0 | grad norm: 0.817 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      389/   25000 | consumed samples:       398336 | elapsed time per iteration (ms): 39229.1 | learning rate: 3.880E-05 | global batch size:  1024 | lm loss: 2.205940E+00 | loss scale: 1.0 | grad norm: 0.733 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      390/   25000 | consumed samples:       399360 | elapsed time per iteration (ms): 39316.5 | learning rate: 3.890E-05 | global batch size:  1024 | lm loss: 2.216295E+00 | loss scale: 1.0 | grad norm: 0.900 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      391/   25000 | consumed samples:       400384 | elapsed time per iteration (ms): 39239.7 | learning rate: 3.900E-05 | global batch size:  1024 | lm loss: 2.217841E+00 | loss scale: 1.0 | grad norm: 0.853 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      392/   25000 | consumed samples:       401408 | elapsed time per iteration (ms): 39400.6 | learning rate: 3.910E-05 | global batch size:  1024 | lm loss: 2.196434E+00 | loss scale: 1.0 | grad norm: 0.755 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      393/   25000 | consumed samples:       402432 | elapsed time per iteration (ms): 39320.8 | learning rate: 3.920E-05 | global batch size:  1024 | lm loss: 2.214463E+00 | loss scale: 1.0 | grad norm: 0.935 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      394/   25000 | consumed samples:       403456 | elapsed time per iteration (ms): 39374.0 | learning rate: 3.930E-05 | global batch size:  1024 | lm loss: 2.218458E+00 | loss scale: 1.0 | grad norm: 1.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      395/   25000 | consumed samples:       404480 | elapsed time per iteration (ms): 39501.0 | learning rate: 3.940E-05 | global batch size:  1024 | lm loss: 2.212072E+00 | loss scale: 1.0 | grad norm: 0.737 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      396/   25000 | consumed samples:       405504 | elapsed time per iteration (ms): 39406.7 | learning rate: 3.950E-05 | global batch size:  1024 | lm loss: 2.215682E+00 | loss scale: 1.0 | grad norm: 0.792 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      397/   25000 | consumed samples:       406528 | elapsed time per iteration (ms): 39283.9 | learning rate: 3.960E-05 | global batch size:  1024 | lm loss: 2.198311E+00 | loss scale: 1.0 | grad norm: 0.651 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      398/   25000 | consumed samples:       407552 | elapsed time per iteration (ms): 39367.2 | learning rate: 3.970E-05 | global batch size:  1024 | lm loss: 2.193555E+00 | loss scale: 1.0 | grad norm: 0.759 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      399/   25000 | consumed samples:       408576 | elapsed time per iteration (ms): 39214.4 | learning rate: 3.980E-05 | global batch size:  1024 | lm loss: 2.211158E+00 | loss scale: 1.0 | grad norm: 0.633 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      400/   25000 | consumed samples:       409600 | elapsed time per iteration (ms): 39289.1 | learning rate: 3.990E-05 | global batch size:  1024 | lm loss: 2.197364E+00 | loss scale: 1.0 | grad norm: 0.730 | number of skipped iterations:   0 | number of nan iterations:   0 |
-----------------------------------------------------------------------------------------------
 validation loss at iteration 400 | lm loss value: 2.194256E+00 | lm loss PPL: 8.973325E+00 | 
-----------------------------------------------------------------------------------------------
 iteration      401/   25000 | consumed samples:       410624 | elapsed time per iteration (ms): 173667.5 | learning rate: 4.000E-05 | global batch size:  1024 | lm loss: 2.208604E+00 | loss scale: 1.0 | grad norm: 0.801 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      402/   25000 | consumed samples:       411648 | elapsed time per iteration (ms): 39292.9 | learning rate: 4.010E-05 | global batch size:  1024 | lm loss: 2.205315E+00 | loss scale: 1.0 | grad norm: 0.933 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      403/   25000 | consumed samples:       412672 | elapsed time per iteration (ms): 39664.1 | learning rate: 4.020E-05 | global batch size:  1024 | lm loss: 2.215502E+00 | loss scale: 1.0 | grad norm: 0.979 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      404/   25000 | consumed samples:       413696 | elapsed time per iteration (ms): 39432.3 | learning rate: 4.030E-05 | global batch size:  1024 | lm loss: 2.203465E+00 | loss scale: 1.0 | grad norm: 0.871 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      405/   25000 | consumed samples:       414720 | elapsed time per iteration (ms): 39220.5 | learning rate: 4.040E-05 | global batch size:  1024 | lm loss: 2.196806E+00 | loss scale: 1.0 | grad norm: 1.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      406/   25000 | consumed samples:       415744 | elapsed time per iteration (ms): 39214.8 | learning rate: 4.050E-05 | global batch size:  1024 | lm loss: 2.196300E+00 | loss scale: 1.0 | grad norm: 0.911 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      407/   25000 | consumed samples:       416768 | elapsed time per iteration (ms): 39223.3 | learning rate: 4.060E-05 | global batch size:  1024 | lm loss: 2.195503E+00 | loss scale: 1.0 | grad norm: 0.939 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      408/   25000 | consumed samples:       417792 | elapsed time per iteration (ms): 39291.5 | learning rate: 4.070E-05 | global batch size:  1024 | lm loss: 2.194598E+00 | loss scale: 1.0 | grad norm: 0.897 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      409/   25000 | consumed samples:       418816 | elapsed time per iteration (ms): 39300.4 | learning rate: 4.080E-05 | global batch size:  1024 | lm loss: 2.187484E+00 | loss scale: 1.0 | grad norm: 0.837 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      410/   25000 | consumed samples:       419840 | elapsed time per iteration (ms): 39374.8 | learning rate: 4.090E-05 | global batch size:  1024 | lm loss: 2.193587E+00 | loss scale: 1.0 | grad norm: 0.926 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      411/   25000 | consumed samples:       420864 | elapsed time per iteration (ms): 39443.5 | learning rate: 4.100E-05 | global batch size:  1024 | lm loss: 2.201397E+00 | loss scale: 1.0 | grad norm: 0.734 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      412/   25000 | consumed samples:       421888 | elapsed time per iteration (ms): 39583.8 | learning rate: 4.110E-05 | global batch size:  1024 | lm loss: 2.186229E+00 | loss scale: 1.0 | grad norm: 0.663 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      413/   25000 | consumed samples:       422912 | elapsed time per iteration (ms): 39207.8 | learning rate: 4.120E-05 | global batch size:  1024 | lm loss: 2.190298E+00 | loss scale: 1.0 | grad norm: 0.828 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      414/   25000 | consumed samples:       423936 | elapsed time per iteration (ms): 39205.8 | learning rate: 4.130E-05 | global batch size:  1024 | lm loss: 2.195799E+00 | loss scale: 1.0 | grad norm: 0.711 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      415/   25000 | consumed samples:       424960 | elapsed time per iteration (ms): 39418.3 | learning rate: 4.140E-05 | global batch size:  1024 | lm loss: 2.199831E+00 | loss scale: 1.0 | grad norm: 0.595 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      416/   25000 | consumed samples:       425984 | elapsed time per iteration (ms): 39202.8 | learning rate: 4.150E-05 | global batch size:  1024 | lm loss: 2.175432E+00 | loss scale: 1.0 | grad norm: 0.713 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      417/   25000 | consumed samples:       427008 | elapsed time per iteration (ms): 39289.1 | learning rate: 4.160E-05 | global batch size:  1024 | lm loss: 2.194245E+00 | loss scale: 1.0 | grad norm: 0.674 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      418/   25000 | consumed samples:       428032 | elapsed time per iteration (ms): 39352.9 | learning rate: 4.170E-05 | global batch size:  1024 | lm loss: 2.194980E+00 | loss scale: 1.0 | grad norm: 0.621 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      419/   25000 | consumed samples:       429056 | elapsed time per iteration (ms): 39496.1 | learning rate: 4.180E-05 | global batch size:  1024 | lm loss: 2.172552E+00 | loss scale: 1.0 | grad norm: 0.598 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      420/   25000 | consumed samples:       430080 | elapsed time per iteration (ms): 39548.0 | learning rate: 4.190E-05 | global batch size:  1024 | lm loss: 2.176895E+00 | loss scale: 1.0 | grad norm: 0.726 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      421/   25000 | consumed samples:       431104 | elapsed time per iteration (ms): 39183.0 | learning rate: 4.200E-05 | global batch size:  1024 | lm loss: 2.178485E+00 | loss scale: 1.0 | grad norm: 0.744 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      422/   25000 | consumed samples:       432128 | elapsed time per iteration (ms): 39274.2 | learning rate: 4.210E-05 | global batch size:  1024 | lm loss: 2.191034E+00 | loss scale: 1.0 | grad norm: 0.776 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      423/   25000 | consumed samples:       433152 | elapsed time per iteration (ms): 39195.4 | learning rate: 4.220E-05 | global batch size:  1024 | lm loss: 2.175341E+00 | loss scale: 1.0 | grad norm: 0.880 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      424/   25000 | consumed samples:       434176 | elapsed time per iteration (ms): 39293.0 | learning rate: 4.230E-05 | global batch size:  1024 | lm loss: 2.176426E+00 | loss scale: 1.0 | grad norm: 0.949 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      425/   25000 | consumed samples:       435200 | elapsed time per iteration (ms): 39210.1 | learning rate: 4.240E-05 | global batch size:  1024 | lm loss: 2.178992E+00 | loss scale: 1.0 | grad norm: 0.899 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      426/   25000 | consumed samples:       436224 | elapsed time per iteration (ms): 39275.1 | learning rate: 4.250E-05 | global batch size:  1024 | lm loss: 2.177199E+00 | loss scale: 1.0 | grad norm: 0.854 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      427/   25000 | consumed samples:       437248 | elapsed time per iteration (ms): 39439.1 | learning rate: 4.260E-05 | global batch size:  1024 | lm loss: 2.185879E+00 | loss scale: 1.0 | grad norm: 0.886 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      428/   25000 | consumed samples:       438272 | elapsed time per iteration (ms): 39733.7 | learning rate: 4.270E-05 | global batch size:  1024 | lm loss: 2.176551E+00 | loss scale: 1.0 | grad norm: 1.036 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      429/   25000 | consumed samples:       439296 | elapsed time per iteration (ms): 39277.3 | learning rate: 4.280E-05 | global batch size:  1024 | lm loss: 2.183268E+00 | loss scale: 1.0 | grad norm: 0.779 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      430/   25000 | consumed samples:       440320 | elapsed time per iteration (ms): 39213.8 | learning rate: 4.290E-05 | global batch size:  1024 | lm loss: 2.177949E+00 | loss scale: 1.0 | grad norm: 0.759 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      431/   25000 | consumed samples:       441344 | elapsed time per iteration (ms): 39212.7 | learning rate: 4.300E-05 | global batch size:  1024 | lm loss: 2.168195E+00 | loss scale: 1.0 | grad norm: 0.852 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      432/   25000 | consumed samples:       442368 | elapsed time per iteration (ms): 39295.6 | learning rate: 4.310E-05 | global batch size:  1024 | lm loss: 2.188001E+00 | loss scale: 1.0 | grad norm: 0.794 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      433/   25000 | consumed samples:       443392 | elapsed time per iteration (ms): 39213.7 | learning rate: 4.320E-05 | global batch size:  1024 | lm loss: 2.179545E+00 | loss scale: 1.0 | grad norm: 0.733 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      434/   25000 | consumed samples:       444416 | elapsed time per iteration (ms): 39314.2 | learning rate: 4.330E-05 | global batch size:  1024 | lm loss: 2.168418E+00 | loss scale: 1.0 | grad norm: 0.768 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      435/   25000 | consumed samples:       445440 | elapsed time per iteration (ms): 39370.4 | learning rate: 4.340E-05 | global batch size:  1024 | lm loss: 2.195176E+00 | loss scale: 1.0 | grad norm: 0.755 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      436/   25000 | consumed samples:       446464 | elapsed time per iteration (ms): 39732.9 | learning rate: 4.350E-05 | global batch size:  1024 | lm loss: 2.170902E+00 | loss scale: 1.0 | grad norm: 0.623 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      437/   25000 | consumed samples:       447488 | elapsed time per iteration (ms): 39363.5 | learning rate: 4.360E-05 | global batch size:  1024 | lm loss: 2.155857E+00 | loss scale: 1.0 | grad norm: 0.683 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      438/   25000 | consumed samples:       448512 | elapsed time per iteration (ms): 39192.8 | learning rate: 4.370E-05 | global batch size:  1024 | lm loss: 2.182570E+00 | loss scale: 1.0 | grad norm: 0.822 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      439/   25000 | consumed samples:       449536 | elapsed time per iteration (ms): 39177.6 | learning rate: 4.380E-05 | global batch size:  1024 | lm loss: 2.175891E+00 | loss scale: 1.0 | grad norm: 0.808 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      440/   25000 | consumed samples:       450560 | elapsed time per iteration (ms): 39265.9 | learning rate: 4.390E-05 | global batch size:  1024 | lm loss: 2.168834E+00 | loss scale: 1.0 | grad norm: 0.897 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      441/   25000 | consumed samples:       451584 | elapsed time per iteration (ms): 39197.2 | learning rate: 4.400E-05 | global batch size:  1024 | lm loss: 2.159657E+00 | loss scale: 1.0 | grad norm: 0.795 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      442/   25000 | consumed samples:       452608 | elapsed time per iteration (ms): 39339.8 | learning rate: 4.410E-05 | global batch size:  1024 | lm loss: 2.160898E+00 | loss scale: 1.0 | grad norm: 0.785 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      443/   25000 | consumed samples:       453632 | elapsed time per iteration (ms): 39269.1 | learning rate: 4.420E-05 | global batch size:  1024 | lm loss: 2.159376E+00 | loss scale: 1.0 | grad norm: 0.843 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      444/   25000 | consumed samples:       454656 | elapsed time per iteration (ms): 39624.1 | learning rate: 4.430E-05 | global batch size:  1024 | lm loss: 2.181491E+00 | loss scale: 1.0 | grad norm: 0.743 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      445/   25000 | consumed samples:       455680 | elapsed time per iteration (ms): 39350.3 | learning rate: 4.440E-05 | global batch size:  1024 | lm loss: 2.157332E+00 | loss scale: 1.0 | grad norm: 0.750 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      446/   25000 | consumed samples:       456704 | elapsed time per iteration (ms): 39358.3 | learning rate: 4.450E-05 | global batch size:  1024 | lm loss: 2.158534E+00 | loss scale: 1.0 | grad norm: 0.871 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      447/   25000 | consumed samples:       457728 | elapsed time per iteration (ms): 39286.5 | learning rate: 4.460E-05 | global batch size:  1024 | lm loss: 2.155406E+00 | loss scale: 1.0 | grad norm: 0.805 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      448/   25000 | consumed samples:       458752 | elapsed time per iteration (ms): 39221.8 | learning rate: 4.470E-05 | global batch size:  1024 | lm loss: 2.165846E+00 | loss scale: 1.0 | grad norm: 0.807 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      449/   25000 | consumed samples:       459776 | elapsed time per iteration (ms): 39189.8 | learning rate: 4.480E-05 | global batch size:  1024 | lm loss: 2.152462E+00 | loss scale: 1.0 | grad norm: 0.820 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      450/   25000 | consumed samples:       460800 | elapsed time per iteration (ms): 39197.8 | learning rate: 4.490E-05 | global batch size:  1024 | lm loss: 2.161808E+00 | loss scale: 1.0 | grad norm: 0.708 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      451/   25000 | consumed samples:       461824 | elapsed time per iteration (ms): 39442.4 | learning rate: 4.500E-05 | global batch size:  1024 | lm loss: 2.160903E+00 | loss scale: 1.0 | grad norm: 0.661 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      452/   25000 | consumed samples:       462848 | elapsed time per iteration (ms): 39728.7 | learning rate: 4.510E-05 | global batch size:  1024 | lm loss: 2.177776E+00 | loss scale: 1.0 | grad norm: 0.713 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      453/   25000 | consumed samples:       463872 | elapsed time per iteration (ms): 39210.0 | learning rate: 4.520E-05 | global batch size:  1024 | lm loss: 2.157416E+00 | loss scale: 1.0 | grad norm: 0.714 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      454/   25000 | consumed samples:       464896 | elapsed time per iteration (ms): 39310.3 | learning rate: 4.530E-05 | global batch size:  1024 | lm loss: 2.170449E+00 | loss scale: 1.0 | grad norm: 0.659 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      455/   25000 | consumed samples:       465920 | elapsed time per iteration (ms): 39231.5 | learning rate: 4.540E-05 | global batch size:  1024 | lm loss: 2.159267E+00 | loss scale: 1.0 | grad norm: 0.721 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      456/   25000 | consumed samples:       466944 | elapsed time per iteration (ms): 39363.8 | learning rate: 4.550E-05 | global batch size:  1024 | lm loss: 2.151187E+00 | loss scale: 1.0 | grad norm: 0.775 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      457/   25000 | consumed samples:       467968 | elapsed time per iteration (ms): 39281.5 | learning rate: 4.560E-05 | global batch size:  1024 | lm loss: 2.151427E+00 | loss scale: 1.0 | grad norm: 0.798 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      458/   25000 | consumed samples:       468992 | elapsed time per iteration (ms): 39200.8 | learning rate: 4.570E-05 | global batch size:  1024 | lm loss: 2.163947E+00 | loss scale: 1.0 | grad norm: 0.735 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      459/   25000 | consumed samples:       470016 | elapsed time per iteration (ms): 39349.4 | learning rate: 4.580E-05 | global batch size:  1024 | lm loss: 2.153939E+00 | loss scale: 1.0 | grad norm: 0.643 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      460/   25000 | consumed samples:       471040 | elapsed time per iteration (ms): 39479.4 | learning rate: 4.590E-05 | global batch size:  1024 | lm loss: 2.164710E+00 | loss scale: 1.0 | grad norm: 0.718 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      461/   25000 | consumed samples:       472064 | elapsed time per iteration (ms): 39489.7 | learning rate: 4.600E-05 | global batch size:  1024 | lm loss: 2.167167E+00 | loss scale: 1.0 | grad norm: 0.858 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      462/   25000 | consumed samples:       473088 | elapsed time per iteration (ms): 39184.8 | learning rate: 4.610E-05 | global batch size:  1024 | lm loss: 2.151778E+00 | loss scale: 1.0 | grad norm: 0.934 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      463/   25000 | consumed samples:       474112 | elapsed time per iteration (ms): 39287.8 | learning rate: 4.620E-05 | global batch size:  1024 | lm loss: 2.162636E+00 | loss scale: 1.0 | grad norm: 0.940 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      464/   25000 | consumed samples:       475136 | elapsed time per iteration (ms): 39202.4 | learning rate: 4.630E-05 | global batch size:  1024 | lm loss: 2.155202E+00 | loss scale: 1.0 | grad norm: 0.868 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      465/   25000 | consumed samples:       476160 | elapsed time per iteration (ms): 39258.9 | learning rate: 4.640E-05 | global batch size:  1024 | lm loss: 2.142795E+00 | loss scale: 1.0 | grad norm: 0.867 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      466/   25000 | consumed samples:       477184 | elapsed time per iteration (ms): 39334.7 | learning rate: 4.650E-05 | global batch size:  1024 | lm loss: 2.157685E+00 | loss scale: 1.0 | grad norm: 0.766 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      467/   25000 | consumed samples:       478208 | elapsed time per iteration (ms): 39396.7 | learning rate: 4.660E-05 | global batch size:  1024 | lm loss: 2.145030E+00 | loss scale: 1.0 | grad norm: 0.899 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      468/   25000 | consumed samples:       479232 | elapsed time per iteration (ms): 39430.3 | learning rate: 4.670E-05 | global batch size:  1024 | lm loss: 2.154759E+00 | loss scale: 1.0 | grad norm: 0.836 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      469/   25000 | consumed samples:       480256 | elapsed time per iteration (ms): 39439.4 | learning rate: 4.680E-05 | global batch size:  1024 | lm loss: 2.151019E+00 | loss scale: 1.0 | grad norm: 0.632 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      470/   25000 | consumed samples:       481280 | elapsed time per iteration (ms): 39205.6 | learning rate: 4.690E-05 | global batch size:  1024 | lm loss: 2.145066E+00 | loss scale: 1.0 | grad norm: 0.635 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      471/   25000 | consumed samples:       482304 | elapsed time per iteration (ms): 39285.1 | learning rate: 4.700E-05 | global batch size:  1024 | lm loss: 2.140895E+00 | loss scale: 1.0 | grad norm: 0.668 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      472/   25000 | consumed samples:       483328 | elapsed time per iteration (ms): 39370.8 | learning rate: 4.710E-05 | global batch size:  1024 | lm loss: 2.140716E+00 | loss scale: 1.0 | grad norm: 0.675 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      473/   25000 | consumed samples:       484352 | elapsed time per iteration (ms): 39189.4 | learning rate: 4.720E-05 | global batch size:  1024 | lm loss: 2.131650E+00 | loss scale: 1.0 | grad norm: 0.688 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      474/   25000 | consumed samples:       485376 | elapsed time per iteration (ms): 39263.5 | learning rate: 4.730E-05 | global batch size:  1024 | lm loss: 2.150393E+00 | loss scale: 1.0 | grad norm: 0.629 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      475/   25000 | consumed samples:       486400 | elapsed time per iteration (ms): 39301.4 | learning rate: 4.740E-05 | global batch size:  1024 | lm loss: 2.155001E+00 | loss scale: 1.0 | grad norm: 0.578 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      476/   25000 | consumed samples:       487424 | elapsed time per iteration (ms): 39595.2 | learning rate: 4.750E-05 | global batch size:  1024 | lm loss: 2.147952E+00 | loss scale: 1.0 | grad norm: 0.649 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      477/   25000 | consumed samples:       488448 | elapsed time per iteration (ms): 39514.8 | learning rate: 4.760E-05 | global batch size:  1024 | lm loss: 2.147292E+00 | loss scale: 1.0 | grad norm: 0.708 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      478/   25000 | consumed samples:       489472 | elapsed time per iteration (ms): 39227.7 | learning rate: 4.770E-05 | global batch size:  1024 | lm loss: 2.157540E+00 | loss scale: 1.0 | grad norm: 0.874 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      479/   25000 | consumed samples:       490496 | elapsed time per iteration (ms): 39234.9 | learning rate: 4.780E-05 | global batch size:  1024 | lm loss: 2.159971E+00 | loss scale: 1.0 | grad norm: 1.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      480/   25000 | consumed samples:       491520 | elapsed time per iteration (ms): 39216.4 | learning rate: 4.790E-05 | global batch size:  1024 | lm loss: 2.141788E+00 | loss scale: 1.0 | grad norm: 0.901 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      481/   25000 | consumed samples:       492544 | elapsed time per iteration (ms): 39354.5 | learning rate: 4.800E-05 | global batch size:  1024 | lm loss: 2.156894E+00 | loss scale: 1.0 | grad norm: 0.851 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      482/   25000 | consumed samples:       493568 | elapsed time per iteration (ms): 39273.4 | learning rate: 4.810E-05 | global batch size:  1024 | lm loss: 2.147987E+00 | loss scale: 1.0 | grad norm: 0.852 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      483/   25000 | consumed samples:       494592 | elapsed time per iteration (ms): 39349.1 | learning rate: 4.820E-05 | global batch size:  1024 | lm loss: 2.127743E+00 | loss scale: 1.0 | grad norm: 0.805 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      484/   25000 | consumed samples:       495616 | elapsed time per iteration (ms): 39473.1 | learning rate: 4.830E-05 | global batch size:  1024 | lm loss: 2.141541E+00 | loss scale: 1.0 | grad norm: 0.750 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      485/   25000 | consumed samples:       496640 | elapsed time per iteration (ms): 39398.2 | learning rate: 4.840E-05 | global batch size:  1024 | lm loss: 2.153626E+00 | loss scale: 1.0 | grad norm: 0.772 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      486/   25000 | consumed samples:       497664 | elapsed time per iteration (ms): 39272.3 | learning rate: 4.850E-05 | global batch size:  1024 | lm loss: 2.135794E+00 | loss scale: 1.0 | grad norm: 0.785 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      487/   25000 | consumed samples:       498688 | elapsed time per iteration (ms): 39264.8 | learning rate: 4.860E-05 | global batch size:  1024 | lm loss: 2.132093E+00 | loss scale: 1.0 | grad norm: 0.749 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      488/   25000 | consumed samples:       499712 | elapsed time per iteration (ms): 39287.4 | learning rate: 4.870E-05 | global batch size:  1024 | lm loss: 2.161583E+00 | loss scale: 1.0 | grad norm: 0.646 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      489/   25000 | consumed samples:       500736 | elapsed time per iteration (ms): 39199.9 | learning rate: 4.880E-05 | global batch size:  1024 | lm loss: 2.122936E+00 | loss scale: 1.0 | grad norm: 0.721 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      490/   25000 | consumed samples:       501760 | elapsed time per iteration (ms): 39203.4 | learning rate: 4.890E-05 | global batch size:  1024 | lm loss: 2.128535E+00 | loss scale: 1.0 | grad norm: 0.628 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      491/   25000 | consumed samples:       502784 | elapsed time per iteration (ms): 39445.0 | learning rate: 4.900E-05 | global batch size:  1024 | lm loss: 2.145080E+00 | loss scale: 1.0 | grad norm: 0.598 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      492/   25000 | consumed samples:       503808 | elapsed time per iteration (ms): 39561.0 | learning rate: 4.910E-05 | global batch size:  1024 | lm loss: 2.136884E+00 | loss scale: 1.0 | grad norm: 0.657 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      493/   25000 | consumed samples:       504832 | elapsed time per iteration (ms): 39431.6 | learning rate: 4.920E-05 | global batch size:  1024 | lm loss: 2.119760E+00 | loss scale: 1.0 | grad norm: 0.554 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      494/   25000 | consumed samples:       505856 | elapsed time per iteration (ms): 39194.7 | learning rate: 4.930E-05 | global batch size:  1024 | lm loss: 2.129871E+00 | loss scale: 1.0 | grad norm: 0.573 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      495/   25000 | consumed samples:       506880 | elapsed time per iteration (ms): 39215.3 | learning rate: 4.940E-05 | global batch size:  1024 | lm loss: 2.125834E+00 | loss scale: 1.0 | grad norm: 0.529 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      496/   25000 | consumed samples:       507904 | elapsed time per iteration (ms): 39299.3 | learning rate: 4.950E-05 | global batch size:  1024 | lm loss: 2.137084E+00 | loss scale: 1.0 | grad norm: 0.567 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      497/   25000 | consumed samples:       508928 | elapsed time per iteration (ms): 39234.1 | learning rate: 4.960E-05 | global batch size:  1024 | lm loss: 2.143437E+00 | loss scale: 1.0 | grad norm: 0.563 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      498/   25000 | consumed samples:       509952 | elapsed time per iteration (ms): 39315.2 | learning rate: 4.970E-05 | global batch size:  1024 | lm loss: 2.109665E+00 | loss scale: 1.0 | grad norm: 0.650 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      499/   25000 | consumed samples:       510976 | elapsed time per iteration (ms): 39315.4 | learning rate: 4.980E-05 | global batch size:  1024 | lm loss: 2.132037E+00 | loss scale: 1.0 | grad norm: 0.870 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      500/   25000 | consumed samples:       512000 | elapsed time per iteration (ms): 39628.3 | learning rate: 4.990E-05 | global batch size:  1024 | lm loss: 2.114592E+00 | loss scale: 1.0 | grad norm: 0.980 | number of skipped iterations:   0 | number of nan iterations:   0 |
-----------------------------------------------------------------------------------------------
saving checkpoint at iteration     500 to /bb/llm/gaf51275/llama/checkpoints/Llama-2-7b-base-extended/okazaki_lab_cc-en-updated/tp2-pp2
 validation loss at iteration 500 | lm loss value: 2.123801E+00 | lm loss PPL: 8.362868E+00 | 
-----------------------------------------------------------------------------------------------
  successfully saved checkpoint at iteration     500 to /bb/llm/gaf51275/llama/checkpoints/Llama-2-7b-base-extended/okazaki_lab_cc-en-updated/tp2-pp2
(min, max) time across ranks (ms):
    save-checkpoint ................................: (48685.68, 48687.93)
 iteration      501/   25000 | consumed samples:       513024 | elapsed time per iteration (ms): 224241.8 | learning rate: 5.000E-05 | global batch size:  1024 | lm loss: 2.134678E+00 | loss scale: 1.0 | grad norm: 0.911 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      502/   25000 | consumed samples:       514048 | elapsed time per iteration (ms): 39161.0 | learning rate: 5.010E-05 | global batch size:  1024 | lm loss: 2.122065E+00 | loss scale: 1.0 | grad norm: 0.774 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      503/   25000 | consumed samples:       515072 | elapsed time per iteration (ms): 39246.3 | learning rate: 5.020E-05 | global batch size:  1024 | lm loss: 2.129596E+00 | loss scale: 1.0 | grad norm: 0.800 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      504/   25000 | consumed samples:       516096 | elapsed time per iteration (ms): 39259.9 | learning rate: 5.030E-05 | global batch size:  1024 | lm loss: 2.128219E+00 | loss scale: 1.0 | grad norm: 0.904 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      505/   25000 | consumed samples:       517120 | elapsed time per iteration (ms): 39194.6 | learning rate: 5.040E-05 | global batch size:  1024 | lm loss: 2.135094E+00 | loss scale: 1.0 | grad norm: 0.970 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      506/   25000 | consumed samples:       518144 | elapsed time per iteration (ms): 39270.7 | learning rate: 5.050E-05 | global batch size:  1024 | lm loss: 2.128612E+00 | loss scale: 1.0 | grad norm: 0.959 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      507/   25000 | consumed samples:       519168 | elapsed time per iteration (ms): 39275.4 | learning rate: 5.060E-05 | global batch size:  1024 | lm loss: 2.128157E+00 | loss scale: 1.0 | grad norm: 0.803 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      508/   25000 | consumed samples:       520192 | elapsed time per iteration (ms): 39511.8 | learning rate: 5.070E-05 | global batch size:  1024 | lm loss: 2.107866E+00 | loss scale: 1.0 | grad norm: 0.784 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      509/   25000 | consumed samples:       521216 | elapsed time per iteration (ms): 39714.0 | learning rate: 5.080E-05 | global batch size:  1024 | lm loss: 2.130805E+00 | loss scale: 1.0 | grad norm: 0.589 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      510/   25000 | consumed samples:       522240 | elapsed time per iteration (ms): 39359.7 | learning rate: 5.090E-05 | global batch size:  1024 | lm loss: 2.131582E+00 | loss scale: 1.0 | grad norm: 0.664 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      511/   25000 | consumed samples:       523264 | elapsed time per iteration (ms): 39235.7 | learning rate: 5.100E-05 | global batch size:  1024 | lm loss: 2.124286E+00 | loss scale: 1.0 | grad norm: 0.800 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      512/   25000 | consumed samples:       524288 | elapsed time per iteration (ms): 39232.7 | learning rate: 5.110E-05 | global batch size:  1024 | lm loss: 2.125449E+00 | loss scale: 1.0 | grad norm: 0.601 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      513/   25000 | consumed samples:       525312 | elapsed time per iteration (ms): 39234.7 | learning rate: 5.120E-05 | global batch size:  1024 | lm loss: 2.117062E+00 | loss scale: 1.0 | grad norm: 0.474 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      514/   25000 | consumed samples:       526336 | elapsed time per iteration (ms): 39349.8 | learning rate: 5.130E-05 | global batch size:  1024 | lm loss: 2.129413E+00 | loss scale: 1.0 | grad norm: 0.660 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      515/   25000 | consumed samples:       527360 | elapsed time per iteration (ms): 39434.6 | learning rate: 5.140E-05 | global batch size:  1024 | lm loss: 2.090228E+00 | loss scale: 1.0 | grad norm: 0.579 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      516/   25000 | consumed samples:       528384 | elapsed time per iteration (ms): 39448.5 | learning rate: 5.150E-05 | global batch size:  1024 | lm loss: 2.120705E+00 | loss scale: 1.0 | grad norm: 0.537 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      517/   25000 | consumed samples:       529408 | elapsed time per iteration (ms): 39512.4 | learning rate: 5.160E-05 | global batch size:  1024 | lm loss: 2.131922E+00 | loss scale: 1.0 | grad norm: 0.663 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      518/   25000 | consumed samples:       530432 | elapsed time per iteration (ms): 39401.1 | learning rate: 5.170E-05 | global batch size:  1024 | lm loss: 2.116043E+00 | loss scale: 1.0 | grad norm: 0.611 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      519/   25000 | consumed samples:       531456 | elapsed time per iteration (ms): 39427.8 | learning rate: 5.180E-05 | global batch size:  1024 | lm loss: 2.138936E+00 | loss scale: 1.0 | grad norm: 0.754 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      520/   25000 | consumed samples:       532480 | elapsed time per iteration (ms): 39269.7 | learning rate: 5.190E-05 | global batch size:  1024 | lm loss: 2.122900E+00 | loss scale: 1.0 | grad norm: 0.855 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      521/   25000 | consumed samples:       533504 | elapsed time per iteration (ms): 39199.5 | learning rate: 5.200E-05 | global batch size:  1024 | lm loss: 2.123474E+00 | loss scale: 1.0 | grad norm: 0.835 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      522/   25000 | consumed samples:       534528 | elapsed time per iteration (ms): 39196.8 | learning rate: 5.210E-05 | global batch size:  1024 | lm loss: 2.116789E+00 | loss scale: 1.0 | grad norm: 0.738 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      523/   25000 | consumed samples:       535552 | elapsed time per iteration (ms): 39281.8 | learning rate: 5.220E-05 | global batch size:  1024 | lm loss: 2.131709E+00 | loss scale: 1.0 | grad norm: 0.762 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      524/   25000 | consumed samples:       536576 | elapsed time per iteration (ms): 39337.3 | learning rate: 5.230E-05 | global batch size:  1024 | lm loss: 2.123499E+00 | loss scale: 1.0 | grad norm: 0.700 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      525/   25000 | consumed samples:       537600 | elapsed time per iteration (ms): 39759.4 | learning rate: 5.240E-05 | global batch size:  1024 | lm loss: 2.104877E+00 | loss scale: 1.0 | grad norm: 0.643 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      526/   25000 | consumed samples:       538624 | elapsed time per iteration (ms): 39195.8 | learning rate: 5.250E-05 | global batch size:  1024 | lm loss: 2.119378E+00 | loss scale: 1.0 | grad norm: 0.672 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      527/   25000 | consumed samples:       539648 | elapsed time per iteration (ms): 39372.8 | learning rate: 5.260E-05 | global batch size:  1024 | lm loss: 2.104039E+00 | loss scale: 1.0 | grad norm: 0.740 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      528/   25000 | consumed samples:       540672 | elapsed time per iteration (ms): 39214.0 | learning rate: 5.270E-05 | global batch size:  1024 | lm loss: 2.110653E+00 | loss scale: 1.0 | grad norm: 0.729 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      529/   25000 | consumed samples:       541696 | elapsed time per iteration (ms): 39284.8 | learning rate: 5.280E-05 | global batch size:  1024 | lm loss: 2.112400E+00 | loss scale: 1.0 | grad norm: 0.711 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      530/   25000 | consumed samples:       542720 | elapsed time per iteration (ms): 39406.1 | learning rate: 5.290E-05 | global batch size:  1024 | lm loss: 2.105940E+00 | loss scale: 1.0 | grad norm: 0.744 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      531/   25000 | consumed samples:       543744 | elapsed time per iteration (ms): 39271.8 | learning rate: 5.300E-05 | global batch size:  1024 | lm loss: 2.124479E+00 | loss scale: 1.0 | grad norm: 0.633 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      532/   25000 | consumed samples:       544768 | elapsed time per iteration (ms): 39368.2 | learning rate: 5.310E-05 | global batch size:  1024 | lm loss: 2.102233E+00 | loss scale: 1.0 | grad norm: 0.593 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      533/   25000 | consumed samples:       545792 | elapsed time per iteration (ms): 39580.6 | learning rate: 5.320E-05 | global batch size:  1024 | lm loss: 2.102746E+00 | loss scale: 1.0 | grad norm: 0.579 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      534/   25000 | consumed samples:       546816 | elapsed time per iteration (ms): 39216.8 | learning rate: 5.330E-05 | global batch size:  1024 | lm loss: 2.099422E+00 | loss scale: 1.0 | grad norm: 0.617 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      535/   25000 | consumed samples:       547840 | elapsed time per iteration (ms): 39497.8 | learning rate: 5.340E-05 | global batch size:  1024 | lm loss: 2.110646E+00 | loss scale: 1.0 | grad norm: 0.683 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      536/   25000 | consumed samples:       548864 | elapsed time per iteration (ms): 39487.8 | learning rate: 5.350E-05 | global batch size:  1024 | lm loss: 2.101825E+00 | loss scale: 1.0 | grad norm: 0.661 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      537/   25000 | consumed samples:       549888 | elapsed time per iteration (ms): 39218.8 | learning rate: 5.360E-05 | global batch size:  1024 | lm loss: 2.094614E+00 | loss scale: 1.0 | grad norm: 0.750 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      538/   25000 | consumed samples:       550912 | elapsed time per iteration (ms): 39297.3 | learning rate: 5.370E-05 | global batch size:  1024 | lm loss: 2.103161E+00 | loss scale: 1.0 | grad norm: 0.863 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      539/   25000 | consumed samples:       551936 | elapsed time per iteration (ms): 39310.0 | learning rate: 5.380E-05 | global batch size:  1024 | lm loss: 2.099841E+00 | loss scale: 1.0 | grad norm: 1.017 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      540/   25000 | consumed samples:       552960 | elapsed time per iteration (ms): 39386.0 | learning rate: 5.390E-05 | global batch size:  1024 | lm loss: 2.113464E+00 | loss scale: 1.0 | grad norm: 0.833 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      541/   25000 | consumed samples:       553984 | elapsed time per iteration (ms): 39717.1 | learning rate: 5.400E-05 | global batch size:  1024 | lm loss: 2.111842E+00 | loss scale: 1.0 | grad norm: 0.737 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      542/   25000 | consumed samples:       555008 | elapsed time per iteration (ms): 39288.6 | learning rate: 5.410E-05 | global batch size:  1024 | lm loss: 2.099010E+00 | loss scale: 1.0 | grad norm: 0.758 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      543/   25000 | consumed samples:       556032 | elapsed time per iteration (ms): 39210.1 | learning rate: 5.420E-05 | global batch size:  1024 | lm loss: 2.095651E+00 | loss scale: 1.0 | grad norm: 0.761 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      544/   25000 | consumed samples:       557056 | elapsed time per iteration (ms): 39207.7 | learning rate: 5.430E-05 | global batch size:  1024 | lm loss: 2.093783E+00 | loss scale: 1.0 | grad norm: 0.754 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      545/   25000 | consumed samples:       558080 | elapsed time per iteration (ms): 39432.0 | learning rate: 5.440E-05 | global batch size:  1024 | lm loss: 2.091404E+00 | loss scale: 1.0 | grad norm: 0.609 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      546/   25000 | consumed samples:       559104 | elapsed time per iteration (ms): 39343.1 | learning rate: 5.450E-05 | global batch size:  1024 | lm loss: 2.090320E+00 | loss scale: 1.0 | grad norm: 0.631 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      547/   25000 | consumed samples:       560128 | elapsed time per iteration (ms): 39349.5 | learning rate: 5.460E-05 | global batch size:  1024 | lm loss: 2.095345E+00 | loss scale: 1.0 | grad norm: 0.680 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      548/   25000 | consumed samples:       561152 | elapsed time per iteration (ms): 39270.6 | learning rate: 5.470E-05 | global batch size:  1024 | lm loss: 2.104810E+00 | loss scale: 1.0 | grad norm: 0.693 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      549/   25000 | consumed samples:       562176 | elapsed time per iteration (ms): 39564.8 | learning rate: 5.480E-05 | global batch size:  1024 | lm loss: 2.104276E+00 | loss scale: 1.0 | grad norm: 0.705 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      550/   25000 | consumed samples:       563200 | elapsed time per iteration (ms): 39357.4 | learning rate: 5.490E-05 | global batch size:  1024 | lm loss: 2.116873E+00 | loss scale: 1.0 | grad norm: 0.768 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      551/   25000 | consumed samples:       564224 | elapsed time per iteration (ms): 39347.2 | learning rate: 5.500E-05 | global batch size:  1024 | lm loss: 2.108092E+00 | loss scale: 1.0 | grad norm: 0.873 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      552/   25000 | consumed samples:       565248 | elapsed time per iteration (ms): 39203.3 | learning rate: 5.510E-05 | global batch size:  1024 | lm loss: 2.105035E+00 | loss scale: 1.0 | grad norm: 0.849 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      553/   25000 | consumed samples:       566272 | elapsed time per iteration (ms): 39213.8 | learning rate: 5.520E-05 | global batch size:  1024 | lm loss: 2.098591E+00 | loss scale: 1.0 | grad norm: 0.728 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      554/   25000 | consumed samples:       567296 | elapsed time per iteration (ms): 39392.1 | learning rate: 5.530E-05 | global batch size:  1024 | lm loss: 2.088166E+00 | loss scale: 1.0 | grad norm: 0.627 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      555/   25000 | consumed samples:       568320 | elapsed time per iteration (ms): 39408.5 | learning rate: 5.540E-05 | global batch size:  1024 | lm loss: 2.111226E+00 | loss scale: 1.0 | grad norm: 0.615 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      556/   25000 | consumed samples:       569344 | elapsed time per iteration (ms): 39494.0 | learning rate: 5.550E-05 | global batch size:  1024 | lm loss: 2.091578E+00 | loss scale: 1.0 | grad norm: 0.662 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      557/   25000 | consumed samples:       570368 | elapsed time per iteration (ms): 39415.8 | learning rate: 5.560E-05 | global batch size:  1024 | lm loss: 2.100225E+00 | loss scale: 1.0 | grad norm: 0.695 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      558/   25000 | consumed samples:       571392 | elapsed time per iteration (ms): 39416.2 | learning rate: 5.570E-05 | global batch size:  1024 | lm loss: 2.105941E+00 | loss scale: 1.0 | grad norm: 0.622 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      559/   25000 | consumed samples:       572416 | elapsed time per iteration (ms): 39211.5 | learning rate: 5.580E-05 | global batch size:  1024 | lm loss: 2.104532E+00 | loss scale: 1.0 | grad norm: 0.643 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      560/   25000 | consumed samples:       573440 | elapsed time per iteration (ms): 39301.4 | learning rate: 5.590E-05 | global batch size:  1024 | lm loss: 2.099931E+00 | loss scale: 1.0 | grad norm: 0.574 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      561/   25000 | consumed samples:       574464 | elapsed time per iteration (ms): 39330.9 | learning rate: 5.600E-05 | global batch size:  1024 | lm loss: 2.096707E+00 | loss scale: 1.0 | grad norm: 0.548 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      562/   25000 | consumed samples:       575488 | elapsed time per iteration (ms): 39366.9 | learning rate: 5.610E-05 | global batch size:  1024 | lm loss: 2.075008E+00 | loss scale: 1.0 | grad norm: 0.572 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      563/   25000 | consumed samples:       576512 | elapsed time per iteration (ms): 39463.8 | learning rate: 5.620E-05 | global batch size:  1024 | lm loss: 2.086523E+00 | loss scale: 1.0 | grad norm: 0.607 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      564/   25000 | consumed samples:       577536 | elapsed time per iteration (ms): 39359.1 | learning rate: 5.630E-05 | global batch size:  1024 | lm loss: 2.070975E+00 | loss scale: 1.0 | grad norm: 0.654 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      565/   25000 | consumed samples:       578560 | elapsed time per iteration (ms): 39482.3 | learning rate: 5.640E-05 | global batch size:  1024 | lm loss: 2.092788E+00 | loss scale: 1.0 | grad norm: 0.709 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      566/   25000 | consumed samples:       579584 | elapsed time per iteration (ms): 39392.2 | learning rate: 5.650E-05 | global batch size:  1024 | lm loss: 2.086642E+00 | loss scale: 1.0 | grad norm: 0.777 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      567/   25000 | consumed samples:       580608 | elapsed time per iteration (ms): 39345.4 | learning rate: 5.660E-05 | global batch size:  1024 | lm loss: 2.075852E+00 | loss scale: 1.0 | grad norm: 0.804 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      568/   25000 | consumed samples:       581632 | elapsed time per iteration (ms): 39186.1 | learning rate: 5.670E-05 | global batch size:  1024 | lm loss: 2.084649E+00 | loss scale: 1.0 | grad norm: 0.713 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      569/   25000 | consumed samples:       582656 | elapsed time per iteration (ms): 39255.4 | learning rate: 5.680E-05 | global batch size:  1024 | lm loss: 2.084393E+00 | loss scale: 1.0 | grad norm: 0.565 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      570/   25000 | consumed samples:       583680 | elapsed time per iteration (ms): 39182.8 | learning rate: 5.690E-05 | global batch size:  1024 | lm loss: 2.093209E+00 | loss scale: 1.0 | grad norm: 0.621 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      571/   25000 | consumed samples:       584704 | elapsed time per iteration (ms): 39264.3 | learning rate: 5.700E-05 | global batch size:  1024 | lm loss: 2.085931E+00 | loss scale: 1.0 | grad norm: 0.708 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      572/   25000 | consumed samples:       585728 | elapsed time per iteration (ms): 39583.1 | learning rate: 5.710E-05 | global batch size:  1024 | lm loss: 2.088282E+00 | loss scale: 1.0 | grad norm: 0.644 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      573/   25000 | consumed samples:       586752 | elapsed time per iteration (ms): 39420.5 | learning rate: 5.720E-05 | global batch size:  1024 | lm loss: 2.083607E+00 | loss scale: 1.0 | grad norm: 0.772 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      574/   25000 | consumed samples:       587776 | elapsed time per iteration (ms): 39642.1 | learning rate: 5.730E-05 | global batch size:  1024 | lm loss: 2.096897E+00 | loss scale: 1.0 | grad norm: 0.966 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      575/   25000 | consumed samples:       588800 | elapsed time per iteration (ms): 39209.1 | learning rate: 5.740E-05 | global batch size:  1024 | lm loss: 2.105716E+00 | loss scale: 1.0 | grad norm: 1.034 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      576/   25000 | consumed samples:       589824 | elapsed time per iteration (ms): 39207.9 | learning rate: 5.750E-05 | global batch size:  1024 | lm loss: 2.101877E+00 | loss scale: 1.0 | grad norm: 0.962 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      577/   25000 | consumed samples:       590848 | elapsed time per iteration (ms): 39209.2 | learning rate: 5.760E-05 | global batch size:  1024 | lm loss: 2.116275E+00 | loss scale: 1.0 | grad norm: 0.808 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      578/   25000 | consumed samples:       591872 | elapsed time per iteration (ms): 39339.7 | learning rate: 5.770E-05 | global batch size:  1024 | lm loss: 2.081993E+00 | loss scale: 1.0 | grad norm: 0.846 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      579/   25000 | consumed samples:       592896 | elapsed time per iteration (ms): 39301.7 | learning rate: 5.780E-05 | global batch size:  1024 | lm loss: 2.081825E+00 | loss scale: 1.0 | grad norm: 0.809 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      580/   25000 | consumed samples:       593920 | elapsed time per iteration (ms): 39288.3 | learning rate: 5.790E-05 | global batch size:  1024 | lm loss: 2.077167E+00 | loss scale: 1.0 | grad norm: 0.662 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      581/   25000 | consumed samples:       594944 | elapsed time per iteration (ms): 39518.4 | learning rate: 5.800E-05 | global batch size:  1024 | lm loss: 2.086579E+00 | loss scale: 1.0 | grad norm: 0.690 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      582/   25000 | consumed samples:       595968 | elapsed time per iteration (ms): 39616.6 | learning rate: 5.810E-05 | global batch size:  1024 | lm loss: 2.102211E+00 | loss scale: 1.0 | grad norm: 0.695 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      583/   25000 | consumed samples:       596992 | elapsed time per iteration (ms): 39437.9 | learning rate: 5.820E-05 | global batch size:  1024 | lm loss: 2.074809E+00 | loss scale: 1.0 | grad norm: 0.605 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      584/   25000 | consumed samples:       598016 | elapsed time per iteration (ms): 39305.8 | learning rate: 5.830E-05 | global batch size:  1024 | lm loss: 2.073584E+00 | loss scale: 1.0 | grad norm: 0.601 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      585/   25000 | consumed samples:       599040 | elapsed time per iteration (ms): 39191.5 | learning rate: 5.840E-05 | global batch size:  1024 | lm loss: 2.082633E+00 | loss scale: 1.0 | grad norm: 0.596 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      586/   25000 | consumed samples:       600064 | elapsed time per iteration (ms): 39195.0 | learning rate: 5.850E-05 | global batch size:  1024 | lm loss: 2.078761E+00 | loss scale: 1.0 | grad norm: 0.538 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      587/   25000 | consumed samples:       601088 | elapsed time per iteration (ms): 39183.3 | learning rate: 5.860E-05 | global batch size:  1024 | lm loss: 2.079158E+00 | loss scale: 1.0 | grad norm: 0.534 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      588/   25000 | consumed samples:       602112 | elapsed time per iteration (ms): 39405.0 | learning rate: 5.870E-05 | global batch size:  1024 | lm loss: 2.067419E+00 | loss scale: 1.0 | grad norm: 0.578 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      589/   25000 | consumed samples:       603136 | elapsed time per iteration (ms): 39556.4 | learning rate: 5.880E-05 | global batch size:  1024 | lm loss: 2.083626E+00 | loss scale: 1.0 | grad norm: 0.576 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      590/   25000 | consumed samples:       604160 | elapsed time per iteration (ms): 39413.5 | learning rate: 5.890E-05 | global batch size:  1024 | lm loss: 2.079613E+00 | loss scale: 1.0 | grad norm: 0.645 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      591/   25000 | consumed samples:       605184 | elapsed time per iteration (ms): 39344.5 | learning rate: 5.900E-05 | global batch size:  1024 | lm loss: 2.090777E+00 | loss scale: 1.0 | grad norm: 0.742 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      592/   25000 | consumed samples:       606208 | elapsed time per iteration (ms): 39255.2 | learning rate: 5.910E-05 | global batch size:  1024 | lm loss: 2.080067E+00 | loss scale: 1.0 | grad norm: 0.838 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      593/   25000 | consumed samples:       607232 | elapsed time per iteration (ms): 39189.4 | learning rate: 5.920E-05 | global batch size:  1024 | lm loss: 2.068507E+00 | loss scale: 1.0 | grad norm: 0.935 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      594/   25000 | consumed samples:       608256 | elapsed time per iteration (ms): 39464.7 | learning rate: 5.930E-05 | global batch size:  1024 | lm loss: 2.065904E+00 | loss scale: 1.0 | grad norm: 0.896 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      595/   25000 | consumed samples:       609280 | elapsed time per iteration (ms): 39215.1 | learning rate: 5.940E-05 | global batch size:  1024 | lm loss: 2.082631E+00 | loss scale: 1.0 | grad norm: 0.650 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      596/   25000 | consumed samples:       610304 | elapsed time per iteration (ms): 39295.6 | learning rate: 5.950E-05 | global batch size:  1024 | lm loss: 2.065723E+00 | loss scale: 1.0 | grad norm: 0.633 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      597/   25000 | consumed samples:       611328 | elapsed time per iteration (ms): 39505.5 | learning rate: 5.960E-05 | global batch size:  1024 | lm loss: 2.089155E+00 | loss scale: 1.0 | grad norm: 0.778 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      598/   25000 | consumed samples:       612352 | elapsed time per iteration (ms): 39435.5 | learning rate: 5.970E-05 | global batch size:  1024 | lm loss: 2.085939E+00 | loss scale: 1.0 | grad norm: 0.713 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      599/   25000 | consumed samples:       613376 | elapsed time per iteration (ms): 39424.7 | learning rate: 5.980E-05 | global batch size:  1024 | lm loss: 2.077334E+00 | loss scale: 1.0 | grad norm: 0.497 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      600/   25000 | consumed samples:       614400 | elapsed time per iteration (ms): 39359.9 | learning rate: 5.990E-05 | global batch size:  1024 | lm loss: 2.079204E+00 | loss scale: 1.0 | grad norm: 0.719 | number of skipped iterations:   0 | number of nan iterations:   0 |
-----------------------------------------------------------------------------------------------
 validation loss at iteration 600 | lm loss value: 2.064939E+00 | lm loss PPL: 7.884813E+00 | 
-----------------------------------------------------------------------------------------------
 iteration      601/   25000 | consumed samples:       615424 | elapsed time per iteration (ms): 174027.6 | learning rate: 6.000E-05 | global batch size:  1024 | lm loss: 2.075185E+00 | loss scale: 1.0 | grad norm: 0.741 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      602/   25000 | consumed samples:       616448 | elapsed time per iteration (ms): 39299.7 | learning rate: 6.010E-05 | global batch size:  1024 | lm loss: 2.058964E+00 | loss scale: 1.0 | grad norm: 0.770 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      603/   25000 | consumed samples:       617472 | elapsed time per iteration (ms): 39224.5 | learning rate: 6.020E-05 | global batch size:  1024 | lm loss: 2.073329E+00 | loss scale: 1.0 | grad norm: 0.955 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      604/   25000 | consumed samples:       618496 | elapsed time per iteration (ms): 39543.4 | learning rate: 6.030E-05 | global batch size:  1024 | lm loss: 2.080713E+00 | loss scale: 1.0 | grad norm: 0.918 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      605/   25000 | consumed samples:       619520 | elapsed time per iteration (ms): 39477.5 | learning rate: 6.040E-05 | global batch size:  1024 | lm loss: 2.071352E+00 | loss scale: 1.0 | grad norm: 0.729 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      606/   25000 | consumed samples:       620544 | elapsed time per iteration (ms): 39391.9 | learning rate: 6.050E-05 | global batch size:  1024 | lm loss: 2.086562E+00 | loss scale: 1.0 | grad norm: 0.725 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      607/   25000 | consumed samples:       621568 | elapsed time per iteration (ms): 39172.2 | learning rate: 6.060E-05 | global batch size:  1024 | lm loss: 2.058937E+00 | loss scale: 1.0 | grad norm: 0.691 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      608/   25000 | consumed samples:       622592 | elapsed time per iteration (ms): 39174.6 | learning rate: 6.070E-05 | global batch size:  1024 | lm loss: 2.081974E+00 | loss scale: 1.0 | grad norm: 0.805 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      609/   25000 | consumed samples:       623616 | elapsed time per iteration (ms): 39550.0 | learning rate: 6.080E-05 | global batch size:  1024 | lm loss: 2.084513E+00 | loss scale: 1.0 | grad norm: 0.615 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      610/   25000 | consumed samples:       624640 | elapsed time per iteration (ms): 39314.1 | learning rate: 6.090E-05 | global batch size:  1024 | lm loss: 2.077683E+00 | loss scale: 1.0 | grad norm: 0.612 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      611/   25000 | consumed samples:       625664 | elapsed time per iteration (ms): 39263.1 | learning rate: 6.100E-05 | global batch size:  1024 | lm loss: 2.082961E+00 | loss scale: 1.0 | grad norm: 0.699 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      612/   25000 | consumed samples:       626688 | elapsed time per iteration (ms): 39285.2 | learning rate: 6.110E-05 | global batch size:  1024 | lm loss: 2.062651E+00 | loss scale: 1.0 | grad norm: 0.704 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      613/   25000 | consumed samples:       627712 | elapsed time per iteration (ms): 39618.3 | learning rate: 6.120E-05 | global batch size:  1024 | lm loss: 2.073913E+00 | loss scale: 1.0 | grad norm: 0.639 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      614/   25000 | consumed samples:       628736 | elapsed time per iteration (ms): 39412.6 | learning rate: 6.130E-05 | global batch size:  1024 | lm loss: 2.084444E+00 | loss scale: 1.0 | grad norm: 0.528 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      615/   25000 | consumed samples:       629760 | elapsed time per iteration (ms): 39377.1 | learning rate: 6.140E-05 | global batch size:  1024 | lm loss: 2.073837E+00 | loss scale: 1.0 | grad norm: 0.611 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      616/   25000 | consumed samples:       630784 | elapsed time per iteration (ms): 39193.9 | learning rate: 6.150E-05 | global batch size:  1024 | lm loss: 2.069750E+00 | loss scale: 1.0 | grad norm: 0.590 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      617/   25000 | consumed samples:       631808 | elapsed time per iteration (ms): 39199.8 | learning rate: 6.160E-05 | global batch size:  1024 | lm loss: 2.071255E+00 | loss scale: 1.0 | grad norm: 0.630 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      618/   25000 | consumed samples:       632832 | elapsed time per iteration (ms): 39496.0 | learning rate: 6.170E-05 | global batch size:  1024 | lm loss: 2.067092E+00 | loss scale: 1.0 | grad norm: 0.681 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      619/   25000 | consumed samples:       633856 | elapsed time per iteration (ms): 39215.5 | learning rate: 6.180E-05 | global batch size:  1024 | lm loss: 2.067073E+00 | loss scale: 1.0 | grad norm: 0.740 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      620/   25000 | consumed samples:       634880 | elapsed time per iteration (ms): 39488.9 | learning rate: 6.190E-05 | global batch size:  1024 | lm loss: 2.071183E+00 | loss scale: 1.0 | grad norm: 0.725 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      621/   25000 | consumed samples:       635904 | elapsed time per iteration (ms): 39349.6 | learning rate: 6.200E-05 | global batch size:  1024 | lm loss: 2.056383E+00 | loss scale: 1.0 | grad norm: 0.630 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      622/   25000 | consumed samples:       636928 | elapsed time per iteration (ms): 39570.1 | learning rate: 6.210E-05 | global batch size:  1024 | lm loss: 2.056294E+00 | loss scale: 1.0 | grad norm: 0.630 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      623/   25000 | consumed samples:       637952 | elapsed time per iteration (ms): 39346.4 | learning rate: 6.220E-05 | global batch size:  1024 | lm loss: 2.053445E+00 | loss scale: 1.0 | grad norm: 0.704 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      624/   25000 | consumed samples:       638976 | elapsed time per iteration (ms): 39232.4 | learning rate: 6.230E-05 | global batch size:  1024 | lm loss: 2.070382E+00 | loss scale: 1.0 | grad norm: 0.550 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      625/   25000 | consumed samples:       640000 | elapsed time per iteration (ms): 39278.8 | learning rate: 6.240E-05 | global batch size:  1024 | lm loss: 2.053499E+00 | loss scale: 1.0 | grad norm: 0.550 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      626/   25000 | consumed samples:       641024 | elapsed time per iteration (ms): 39254.1 | learning rate: 6.250E-05 | global batch size:  1024 | lm loss: 2.057885E+00 | loss scale: 1.0 | grad norm: 0.703 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      627/   25000 | consumed samples:       642048 | elapsed time per iteration (ms): 39360.1 | learning rate: 6.260E-05 | global batch size:  1024 | lm loss: 2.068285E+00 | loss scale: 1.0 | grad norm: 0.732 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      628/   25000 | consumed samples:       643072 | elapsed time per iteration (ms): 39392.4 | learning rate: 6.270E-05 | global batch size:  1024 | lm loss: 2.062243E+00 | loss scale: 1.0 | grad norm: 0.737 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      629/   25000 | consumed samples:       644096 | elapsed time per iteration (ms): 39325.8 | learning rate: 6.280E-05 | global batch size:  1024 | lm loss: 2.063651E+00 | loss scale: 1.0 | grad norm: 0.742 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      630/   25000 | consumed samples:       645120 | elapsed time per iteration (ms): 39542.7 | learning rate: 6.290E-05 | global batch size:  1024 | lm loss: 2.067721E+00 | loss scale: 1.0 | grad norm: 0.700 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      631/   25000 | consumed samples:       646144 | elapsed time per iteration (ms): 39359.0 | learning rate: 6.300E-05 | global batch size:  1024 | lm loss: 2.071558E+00 | loss scale: 1.0 | grad norm: 0.685 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      632/   25000 | consumed samples:       647168 | elapsed time per iteration (ms): 39187.4 | learning rate: 6.310E-05 | global batch size:  1024 | lm loss: 2.075254E+00 | loss scale: 1.0 | grad norm: 0.711 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      633/   25000 | consumed samples:       648192 | elapsed time per iteration (ms): 39298.2 | learning rate: 6.320E-05 | global batch size:  1024 | lm loss: 2.052405E+00 | loss scale: 1.0 | grad norm: 0.689 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      634/   25000 | consumed samples:       649216 | elapsed time per iteration (ms): 39156.4 | learning rate: 6.330E-05 | global batch size:  1024 | lm loss: 2.063906E+00 | loss scale: 1.0 | grad norm: 0.604 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      635/   25000 | consumed samples:       650240 | elapsed time per iteration (ms): 39178.1 | learning rate: 6.340E-05 | global batch size:  1024 | lm loss: 2.065425E+00 | loss scale: 1.0 | grad norm: 0.549 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      636/   25000 | consumed samples:       651264 | elapsed time per iteration (ms): 39588.9 | learning rate: 6.350E-05 | global batch size:  1024 | lm loss: 2.056594E+00 | loss scale: 1.0 | grad norm: 0.634 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      637/   25000 | consumed samples:       652288 | elapsed time per iteration (ms): 39331.3 | learning rate: 6.360E-05 | global batch size:  1024 | lm loss: 2.059010E+00 | loss scale: 1.0 | grad norm: 0.620 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      638/   25000 | consumed samples:       653312 | elapsed time per iteration (ms): 39678.7 | learning rate: 6.370E-05 | global batch size:  1024 | lm loss: 2.050098E+00 | loss scale: 1.0 | grad norm: 0.570 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      639/   25000 | consumed samples:       654336 | elapsed time per iteration (ms): 39284.7 | learning rate: 6.380E-05 | global batch size:  1024 | lm loss: 2.073687E+00 | loss scale: 1.0 | grad norm: 0.589 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      640/   25000 | consumed samples:       655360 | elapsed time per iteration (ms): 39200.8 | learning rate: 6.390E-05 | global batch size:  1024 | lm loss: 2.052973E+00 | loss scale: 1.0 | grad norm: 0.612 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      641/   25000 | consumed samples:       656384 | elapsed time per iteration (ms): 39385.5 | learning rate: 6.400E-05 | global batch size:  1024 | lm loss: 2.053819E+00 | loss scale: 1.0 | grad norm: 0.614 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      642/   25000 | consumed samples:       657408 | elapsed time per iteration (ms): 39218.5 | learning rate: 6.410E-05 | global batch size:  1024 | lm loss: 2.053108E+00 | loss scale: 1.0 | grad norm: 0.604 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      643/   25000 | consumed samples:       658432 | elapsed time per iteration (ms): 39364.0 | learning rate: 6.420E-05 | global batch size:  1024 | lm loss: 2.048803E+00 | loss scale: 1.0 | grad norm: 0.527 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      644/   25000 | consumed samples:       659456 | elapsed time per iteration (ms): 39322.4 | learning rate: 6.430E-05 | global batch size:  1024 | lm loss: 2.058277E+00 | loss scale: 1.0 | grad norm: 0.571 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      645/   25000 | consumed samples:       660480 | elapsed time per iteration (ms): 39558.8 | learning rate: 6.440E-05 | global batch size:  1024 | lm loss: 2.045119E+00 | loss scale: 1.0 | grad norm: 0.661 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      646/   25000 | consumed samples:       661504 | elapsed time per iteration (ms): 39597.4 | learning rate: 6.450E-05 | global batch size:  1024 | lm loss: 2.050797E+00 | loss scale: 1.0 | grad norm: 0.655 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      647/   25000 | consumed samples:       662528 | elapsed time per iteration (ms): 39438.6 | learning rate: 6.460E-05 | global batch size:  1024 | lm loss: 2.049271E+00 | loss scale: 1.0 | grad norm: 0.653 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      648/   25000 | consumed samples:       663552 | elapsed time per iteration (ms): 39363.0 | learning rate: 6.470E-05 | global batch size:  1024 | lm loss: 2.063300E+00 | loss scale: 1.0 | grad norm: 0.790 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      649/   25000 | consumed samples:       664576 | elapsed time per iteration (ms): 39211.4 | learning rate: 6.480E-05 | global batch size:  1024 | lm loss: 2.056885E+00 | loss scale: 1.0 | grad norm: 0.777 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      650/   25000 | consumed samples:       665600 | elapsed time per iteration (ms): 39213.5 | learning rate: 6.490E-05 | global batch size:  1024 | lm loss: 2.063807E+00 | loss scale: 1.0 | grad norm: 0.686 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      651/   25000 | consumed samples:       666624 | elapsed time per iteration (ms): 39214.4 | learning rate: 6.500E-05 | global batch size:  1024 | lm loss: 2.042074E+00 | loss scale: 1.0 | grad norm: 0.760 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      652/   25000 | consumed samples:       667648 | elapsed time per iteration (ms): 39453.9 | learning rate: 6.510E-05 | global batch size:  1024 | lm loss: 2.054357E+00 | loss scale: 1.0 | grad norm: 0.796 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      653/   25000 | consumed samples:       668672 | elapsed time per iteration (ms): 39423.7 | learning rate: 6.520E-05 | global batch size:  1024 | lm loss: 2.061205E+00 | loss scale: 1.0 | grad norm: 0.723 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      654/   25000 | consumed samples:       669696 | elapsed time per iteration (ms): 39662.5 | learning rate: 6.530E-05 | global batch size:  1024 | lm loss: 2.044800E+00 | loss scale: 1.0 | grad norm: 0.760 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      655/   25000 | consumed samples:       670720 | elapsed time per iteration (ms): 39370.2 | learning rate: 6.540E-05 | global batch size:  1024 | lm loss: 2.053697E+00 | loss scale: 1.0 | grad norm: 0.753 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      656/   25000 | consumed samples:       671744 | elapsed time per iteration (ms): 39289.8 | learning rate: 6.550E-05 | global batch size:  1024 | lm loss: 2.059253E+00 | loss scale: 1.0 | grad norm: 0.724 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      657/   25000 | consumed samples:       672768 | elapsed time per iteration (ms): 39358.8 | learning rate: 6.560E-05 | global batch size:  1024 | lm loss: 2.064799E+00 | loss scale: 1.0 | grad norm: 0.676 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      658/   25000 | consumed samples:       673792 | elapsed time per iteration (ms): 39353.3 | learning rate: 6.570E-05 | global batch size:  1024 | lm loss: 2.067776E+00 | loss scale: 1.0 | grad norm: 0.600 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      659/   25000 | consumed samples:       674816 | elapsed time per iteration (ms): 39204.9 | learning rate: 6.580E-05 | global batch size:  1024 | lm loss: 2.063072E+00 | loss scale: 1.0 | grad norm: 0.585 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      660/   25000 | consumed samples:       675840 | elapsed time per iteration (ms): 39279.2 | learning rate: 6.590E-05 | global batch size:  1024 | lm loss: 2.043074E+00 | loss scale: 1.0 | grad norm: 0.673 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      661/   25000 | consumed samples:       676864 | elapsed time per iteration (ms): 39270.2 | learning rate: 6.600E-05 | global batch size:  1024 | lm loss: 2.041257E+00 | loss scale: 1.0 | grad norm: 0.630 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      662/   25000 | consumed samples:       677888 | elapsed time per iteration (ms): 39547.5 | learning rate: 6.610E-05 | global batch size:  1024 | lm loss: 2.050592E+00 | loss scale: 1.0 | grad norm: 0.618 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      663/   25000 | consumed samples:       678912 | elapsed time per iteration (ms): 39759.4 | learning rate: 6.620E-05 | global batch size:  1024 | lm loss: 2.033252E+00 | loss scale: 1.0 | grad norm: 0.633 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      664/   25000 | consumed samples:       679936 | elapsed time per iteration (ms): 39217.1 | learning rate: 6.630E-05 | global batch size:  1024 | lm loss: 2.044219E+00 | loss scale: 1.0 | grad norm: 0.765 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      665/   25000 | consumed samples:       680960 | elapsed time per iteration (ms): 39287.5 | learning rate: 6.640E-05 | global batch size:  1024 | lm loss: 2.049303E+00 | loss scale: 1.0 | grad norm: 0.649 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      666/   25000 | consumed samples:       681984 | elapsed time per iteration (ms): 39218.7 | learning rate: 6.650E-05 | global batch size:  1024 | lm loss: 2.060854E+00 | loss scale: 1.0 | grad norm: 0.736 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      667/   25000 | consumed samples:       683008 | elapsed time per iteration (ms): 39351.4 | learning rate: 6.660E-05 | global batch size:  1024 | lm loss: 2.041078E+00 | loss scale: 1.0 | grad norm: 0.617 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      668/   25000 | consumed samples:       684032 | elapsed time per iteration (ms): 39475.9 | learning rate: 6.670E-05 | global batch size:  1024 | lm loss: 2.043270E+00 | loss scale: 1.0 | grad norm: 0.638 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      669/   25000 | consumed samples:       685056 | elapsed time per iteration (ms): 39286.6 | learning rate: 6.680E-05 | global batch size:  1024 | lm loss: 2.044979E+00 | loss scale: 1.0 | grad norm: 0.628 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      670/   25000 | consumed samples:       686080 | elapsed time per iteration (ms): 39437.8 | learning rate: 6.690E-05 | global batch size:  1024 | lm loss: 2.040179E+00 | loss scale: 1.0 | grad norm: 0.549 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      671/   25000 | consumed samples:       687104 | elapsed time per iteration (ms): 39444.0 | learning rate: 6.700E-05 | global batch size:  1024 | lm loss: 2.049343E+00 | loss scale: 1.0 | grad norm: 0.545 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      672/   25000 | consumed samples:       688128 | elapsed time per iteration (ms): 39348.5 | learning rate: 6.710E-05 | global batch size:  1024 | lm loss: 2.046690E+00 | loss scale: 1.0 | grad norm: 0.524 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      673/   25000 | consumed samples:       689152 | elapsed time per iteration (ms): 39464.0 | learning rate: 6.720E-05 | global batch size:  1024 | lm loss: 2.042108E+00 | loss scale: 1.0 | grad norm: 0.503 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      674/   25000 | consumed samples:       690176 | elapsed time per iteration (ms): 39293.6 | learning rate: 6.730E-05 | global batch size:  1024 | lm loss: 2.022007E+00 | loss scale: 1.0 | grad norm: 0.499 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      675/   25000 | consumed samples:       691200 | elapsed time per iteration (ms): 39231.4 | learning rate: 6.740E-05 | global batch size:  1024 | lm loss: 2.041146E+00 | loss scale: 1.0 | grad norm: 0.460 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      676/   25000 | consumed samples:       692224 | elapsed time per iteration (ms): 39305.6 | learning rate: 6.750E-05 | global batch size:  1024 | lm loss: 2.030099E+00 | loss scale: 1.0 | grad norm: 0.468 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      677/   25000 | consumed samples:       693248 | elapsed time per iteration (ms): 39371.4 | learning rate: 6.760E-05 | global batch size:  1024 | lm loss: 2.034950E+00 | loss scale: 1.0 | grad norm: 0.579 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      678/   25000 | consumed samples:       694272 | elapsed time per iteration (ms): 39537.1 | learning rate: 6.770E-05 | global batch size:  1024 | lm loss: 2.042072E+00 | loss scale: 1.0 | grad norm: 0.646 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      679/   25000 | consumed samples:       695296 | elapsed time per iteration (ms): 39530.7 | learning rate: 6.780E-05 | global batch size:  1024 | lm loss: 2.058842E+00 | loss scale: 1.0 | grad norm: 0.848 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      680/   25000 | consumed samples:       696320 | elapsed time per iteration (ms): 39311.8 | learning rate: 6.790E-05 | global batch size:  1024 | lm loss: 2.047450E+00 | loss scale: 1.0 | grad norm: 1.122 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      681/   25000 | consumed samples:       697344 | elapsed time per iteration (ms): 39225.0 | learning rate: 6.800E-05 | global batch size:  1024 | lm loss: 2.031961E+00 | loss scale: 1.0 | grad norm: 0.759 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      682/   25000 | consumed samples:       698368 | elapsed time per iteration (ms): 39475.8 | learning rate: 6.810E-05 | global batch size:  1024 | lm loss: 2.057702E+00 | loss scale: 1.0 | grad norm: 0.579 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      683/   25000 | consumed samples:       699392 | elapsed time per iteration (ms): 39228.6 | learning rate: 6.820E-05 | global batch size:  1024 | lm loss: 2.050584E+00 | loss scale: 1.0 | grad norm: 0.798 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      684/   25000 | consumed samples:       700416 | elapsed time per iteration (ms): 39491.5 | learning rate: 6.830E-05 | global batch size:  1024 | lm loss: 2.039368E+00 | loss scale: 1.0 | grad norm: 0.788 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      685/   25000 | consumed samples:       701440 | elapsed time per iteration (ms): 39297.4 | learning rate: 6.840E-05 | global batch size:  1024 | lm loss: 2.038304E+00 | loss scale: 1.0 | grad norm: 0.762 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      686/   25000 | consumed samples:       702464 | elapsed time per iteration (ms): 39444.2 | learning rate: 6.850E-05 | global batch size:  1024 | lm loss: 2.050255E+00 | loss scale: 1.0 | grad norm: 0.763 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      687/   25000 | consumed samples:       703488 | elapsed time per iteration (ms): 39594.0 | learning rate: 6.860E-05 | global batch size:  1024 | lm loss: 2.023226E+00 | loss scale: 1.0 | grad norm: 0.621 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      688/   25000 | consumed samples:       704512 | elapsed time per iteration (ms): 39231.2 | learning rate: 6.870E-05 | global batch size:  1024 | lm loss: 2.037237E+00 | loss scale: 1.0 | grad norm: 0.525 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      689/   25000 | consumed samples:       705536 | elapsed time per iteration (ms): 39338.9 | learning rate: 6.880E-05 | global batch size:  1024 | lm loss: 2.030933E+00 | loss scale: 1.0 | grad norm: 0.653 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      690/   25000 | consumed samples:       706560 | elapsed time per iteration (ms): 39234.3 | learning rate: 6.890E-05 | global batch size:  1024 | lm loss: 2.017031E+00 | loss scale: 1.0 | grad norm: 0.571 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      691/   25000 | consumed samples:       707584 | elapsed time per iteration (ms): 39385.3 | learning rate: 6.900E-05 | global batch size:  1024 | lm loss: 2.017410E+00 | loss scale: 1.0 | grad norm: 0.543 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      692/   25000 | consumed samples:       708608 | elapsed time per iteration (ms): 39376.5 | learning rate: 6.910E-05 | global batch size:  1024 | lm loss: 2.036127E+00 | loss scale: 1.0 | grad norm: 0.529 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      693/   25000 | consumed samples:       709632 | elapsed time per iteration (ms): 39288.7 | learning rate: 6.920E-05 | global batch size:  1024 | lm loss: 2.039285E+00 | loss scale: 1.0 | grad norm: 0.547 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      694/   25000 | consumed samples:       710656 | elapsed time per iteration (ms): 39695.0 | learning rate: 6.930E-05 | global batch size:  1024 | lm loss: 2.023550E+00 | loss scale: 1.0 | grad norm: 0.520 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      695/   25000 | consumed samples:       711680 | elapsed time per iteration (ms): 39453.4 | learning rate: 6.940E-05 | global batch size:  1024 | lm loss: 2.033652E+00 | loss scale: 1.0 | grad norm: 0.555 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      696/   25000 | consumed samples:       712704 | elapsed time per iteration (ms): 39208.1 | learning rate: 6.950E-05 | global batch size:  1024 | lm loss: 2.033263E+00 | loss scale: 1.0 | grad norm: 0.638 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      697/   25000 | consumed samples:       713728 | elapsed time per iteration (ms): 39275.4 | learning rate: 6.960E-05 | global batch size:  1024 | lm loss: 2.035828E+00 | loss scale: 1.0 | grad norm: 0.673 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      698/   25000 | consumed samples:       714752 | elapsed time per iteration (ms): 40032.3 | learning rate: 6.970E-05 | global batch size:  1024 | lm loss: 2.044762E+00 | loss scale: 1.0 | grad norm: 0.633 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      699/   25000 | consumed samples:       715776 | elapsed time per iteration (ms): 39190.9 | learning rate: 6.980E-05 | global batch size:  1024 | lm loss: 2.021735E+00 | loss scale: 1.0 | grad norm: 0.583 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      700/   25000 | consumed samples:       716800 | elapsed time per iteration (ms): 39621.3 | learning rate: 6.990E-05 | global batch size:  1024 | lm loss: 2.051554E+00 | loss scale: 1.0 | grad norm: 0.592 | number of skipped iterations:   0 | number of nan iterations:   0 |
-----------------------------------------------------------------------------------------------
 validation loss at iteration 700 | lm loss value: 2.025114E+00 | lm loss PPL: 7.576977E+00 | 
-----------------------------------------------------------------------------------------------
 iteration      701/   25000 | consumed samples:       717824 | elapsed time per iteration (ms): 174694.5 | learning rate: 7.000E-05 | global batch size:  1024 | lm loss: 2.044390E+00 | loss scale: 1.0 | grad norm: 0.605 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      702/   25000 | consumed samples:       718848 | elapsed time per iteration (ms): 39633.7 | learning rate: 7.010E-05 | global batch size:  1024 | lm loss: 2.029080E+00 | loss scale: 1.0 | grad norm: 0.560 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      703/   25000 | consumed samples:       719872 | elapsed time per iteration (ms): 39441.6 | learning rate: 7.020E-05 | global batch size:  1024 | lm loss: 2.040818E+00 | loss scale: 1.0 | grad norm: 0.609 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      704/   25000 | consumed samples:       720896 | elapsed time per iteration (ms): 39218.0 | learning rate: 7.030E-05 | global batch size:  1024 | lm loss: 2.037205E+00 | loss scale: 1.0 | grad norm: 0.662 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      705/   25000 | consumed samples:       721920 | elapsed time per iteration (ms): 39389.7 | learning rate: 7.040E-05 | global batch size:  1024 | lm loss: 2.043728E+00 | loss scale: 1.0 | grad norm: 0.737 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      706/   25000 | consumed samples:       722944 | elapsed time per iteration (ms): 39206.2 | learning rate: 7.050E-05 | global batch size:  1024 | lm loss: 2.035869E+00 | loss scale: 1.0 | grad norm: 0.715 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      707/   25000 | consumed samples:       723968 | elapsed time per iteration (ms): 39334.5 | learning rate: 7.060E-05 | global batch size:  1024 | lm loss: 2.031706E+00 | loss scale: 1.0 | grad norm: 0.632 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      708/   25000 | consumed samples:       724992 | elapsed time per iteration (ms): 39308.0 | learning rate: 7.070E-05 | global batch size:  1024 | lm loss: 2.033456E+00 | loss scale: 1.0 | grad norm: 0.595 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      709/   25000 | consumed samples:       726016 | elapsed time per iteration (ms): 39370.8 | learning rate: 7.080E-05 | global batch size:  1024 | lm loss: 2.035538E+00 | loss scale: 1.0 | grad norm: 0.520 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      710/   25000 | consumed samples:       727040 | elapsed time per iteration (ms): 39701.8 | learning rate: 7.090E-05 | global batch size:  1024 | lm loss: 2.033318E+00 | loss scale: 1.0 | grad norm: 0.501 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      711/   25000 | consumed samples:       728064 | elapsed time per iteration (ms): 39646.9 | learning rate: 7.100E-05 | global batch size:  1024 | lm loss: 2.023286E+00 | loss scale: 1.0 | grad norm: 0.525 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      712/   25000 | consumed samples:       729088 | elapsed time per iteration (ms): 39234.8 | learning rate: 7.110E-05 | global batch size:  1024 | lm loss: 2.016424E+00 | loss scale: 1.0 | grad norm: 0.503 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      713/   25000 | consumed samples:       730112 | elapsed time per iteration (ms): 39225.6 | learning rate: 7.120E-05 | global batch size:  1024 | lm loss: 2.028684E+00 | loss scale: 1.0 | grad norm: 0.540 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      714/   25000 | consumed samples:       731136 | elapsed time per iteration (ms): 39227.9 | learning rate: 7.130E-05 | global batch size:  1024 | lm loss: 2.011918E+00 | loss scale: 1.0 | grad norm: 0.553 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      715/   25000 | consumed samples:       732160 | elapsed time per iteration (ms): 39282.8 | learning rate: 7.140E-05 | global batch size:  1024 | lm loss: 2.025123E+00 | loss scale: 1.0 | grad norm: 0.493 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      716/   25000 | consumed samples:       733184 | elapsed time per iteration (ms): 39451.6 | learning rate: 7.150E-05 | global batch size:  1024 | lm loss: 2.026318E+00 | loss scale: 1.0 | grad norm: 0.632 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      717/   25000 | consumed samples:       734208 | elapsed time per iteration (ms): 39226.8 | learning rate: 7.160E-05 | global batch size:  1024 | lm loss: 2.020096E+00 | loss scale: 1.0 | grad norm: 0.761 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      718/   25000 | consumed samples:       735232 | elapsed time per iteration (ms): 39498.4 | learning rate: 7.170E-05 | global batch size:  1024 | lm loss: 2.027564E+00 | loss scale: 1.0 | grad norm: 0.889 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      719/   25000 | consumed samples:       736256 | elapsed time per iteration (ms): 39568.4 | learning rate: 7.180E-05 | global batch size:  1024 | lm loss: 2.018009E+00 | loss scale: 1.0 | grad norm: 0.892 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      720/   25000 | consumed samples:       737280 | elapsed time per iteration (ms): 39268.5 | learning rate: 7.190E-05 | global batch size:  1024 | lm loss: 2.033421E+00 | loss scale: 1.0 | grad norm: 0.805 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      721/   25000 | consumed samples:       738304 | elapsed time per iteration (ms): 39442.0 | learning rate: 7.200E-05 | global batch size:  1024 | lm loss: 2.021367E+00 | loss scale: 1.0 | grad norm: 0.866 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      722/   25000 | consumed samples:       739328 | elapsed time per iteration (ms): 39178.5 | learning rate: 7.210E-05 | global batch size:  1024 | lm loss: 2.000271E+00 | loss scale: 1.0 | grad norm: 0.697 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      723/   25000 | consumed samples:       740352 | elapsed time per iteration (ms): 39199.7 | learning rate: 7.220E-05 | global batch size:  1024 | lm loss: 2.024250E+00 | loss scale: 1.0 | grad norm: 0.836 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      724/   25000 | consumed samples:       741376 | elapsed time per iteration (ms): 39279.6 | learning rate: 7.230E-05 | global batch size:  1024 | lm loss: 2.021274E+00 | loss scale: 1.0 | grad norm: 0.723 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      725/   25000 | consumed samples:       742400 | elapsed time per iteration (ms): 39214.1 | learning rate: 7.240E-05 | global batch size:  1024 | lm loss: 2.017098E+00 | loss scale: 1.0 | grad norm: 0.559 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      726/   25000 | consumed samples:       743424 | elapsed time per iteration (ms): 39632.7 | learning rate: 7.250E-05 | global batch size:  1024 | lm loss: 2.036148E+00 | loss scale: 1.0 | grad norm: 0.785 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      727/   25000 | consumed samples:       744448 | elapsed time per iteration (ms): 39759.7 | learning rate: 7.260E-05 | global batch size:  1024 | lm loss: 2.009116E+00 | loss scale: 1.0 | grad norm: 0.557 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      728/   25000 | consumed samples:       745472 | elapsed time per iteration (ms): 39211.8 | learning rate: 7.270E-05 | global batch size:  1024 | lm loss: 2.014838E+00 | loss scale: 1.0 | grad norm: 0.552 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      729/   25000 | consumed samples:       746496 | elapsed time per iteration (ms): 39273.5 | learning rate: 7.280E-05 | global batch size:  1024 | lm loss: 2.003871E+00 | loss scale: 1.0 | grad norm: 0.588 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      730/   25000 | consumed samples:       747520 | elapsed time per iteration (ms): 39210.8 | learning rate: 7.290E-05 | global batch size:  1024 | lm loss: 2.016398E+00 | loss scale: 1.0 | grad norm: 0.466 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      731/   25000 | consumed samples:       748544 | elapsed time per iteration (ms): 39532.1 | learning rate: 7.300E-05 | global batch size:  1024 | lm loss: 2.013231E+00 | loss scale: 1.0 | grad norm: 0.487 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      732/   25000 | consumed samples:       749568 | elapsed time per iteration (ms): 39547.3 | learning rate: 7.310E-05 | global batch size:  1024 | lm loss: 2.029969E+00 | loss scale: 1.0 | grad norm: 0.420 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      733/   25000 | consumed samples:       750592 | elapsed time per iteration (ms): 39218.4 | learning rate: 7.320E-05 | global batch size:  1024 | lm loss: 2.010021E+00 | loss scale: 1.0 | grad norm: 0.418 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      734/   25000 | consumed samples:       751616 | elapsed time per iteration (ms): 39371.9 | learning rate: 7.330E-05 | global batch size:  1024 | lm loss: 2.009721E+00 | loss scale: 1.0 | grad norm: 0.410 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      735/   25000 | consumed samples:       752640 | elapsed time per iteration (ms): 39514.1 | learning rate: 7.340E-05 | global batch size:  1024 | lm loss: 2.016265E+00 | loss scale: 1.0 | grad norm: 0.425 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      736/   25000 | consumed samples:       753664 | elapsed time per iteration (ms): 39516.5 | learning rate: 7.350E-05 | global batch size:  1024 | lm loss: 2.021243E+00 | loss scale: 1.0 | grad norm: 0.509 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      737/   25000 | consumed samples:       754688 | elapsed time per iteration (ms): 39343.4 | learning rate: 7.360E-05 | global batch size:  1024 | lm loss: 2.017043E+00 | loss scale: 1.0 | grad norm: 0.589 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      738/   25000 | consumed samples:       755712 | elapsed time per iteration (ms): 39289.5 | learning rate: 7.370E-05 | global batch size:  1024 | lm loss: 2.017400E+00 | loss scale: 1.0 | grad norm: 0.672 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      739/   25000 | consumed samples:       756736 | elapsed time per iteration (ms): 39290.4 | learning rate: 7.380E-05 | global batch size:  1024 | lm loss: 2.017502E+00 | loss scale: 1.0 | grad norm: 0.736 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      740/   25000 | consumed samples:       757760 | elapsed time per iteration (ms): 39309.2 | learning rate: 7.390E-05 | global batch size:  1024 | lm loss: 2.033448E+00 | loss scale: 1.0 | grad norm: 0.802 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      741/   25000 | consumed samples:       758784 | elapsed time per iteration (ms): 39352.7 | learning rate: 7.400E-05 | global batch size:  1024 | lm loss: 2.025892E+00 | loss scale: 1.0 | grad norm: 0.875 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      742/   25000 | consumed samples:       759808 | elapsed time per iteration (ms): 39486.6 | learning rate: 7.410E-05 | global batch size:  1024 | lm loss: 2.026046E+00 | loss scale: 1.0 | grad norm: 0.876 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      743/   25000 | consumed samples:       760832 | elapsed time per iteration (ms): 39435.8 | learning rate: 7.420E-05 | global batch size:  1024 | lm loss: 2.029898E+00 | loss scale: 1.0 | grad norm: 0.789 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      744/   25000 | consumed samples:       761856 | elapsed time per iteration (ms): 39350.1 | learning rate: 7.430E-05 | global batch size:  1024 | lm loss: 2.023764E+00 | loss scale: 1.0 | grad norm: 0.567 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      745/   25000 | consumed samples:       762880 | elapsed time per iteration (ms): 39348.4 | learning rate: 7.440E-05 | global batch size:  1024 | lm loss: 2.025198E+00 | loss scale: 1.0 | grad norm: 0.688 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      746/   25000 | consumed samples:       763904 | elapsed time per iteration (ms): 39323.3 | learning rate: 7.450E-05 | global batch size:  1024 | lm loss: 2.022520E+00 | loss scale: 1.0 | grad norm: 0.621 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      747/   25000 | consumed samples:       764928 | elapsed time per iteration (ms): 39372.9 | learning rate: 7.460E-05 | global batch size:  1024 | lm loss: 2.014953E+00 | loss scale: 1.0 | grad norm: 0.586 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      748/   25000 | consumed samples:       765952 | elapsed time per iteration (ms): 39346.8 | learning rate: 7.470E-05 | global batch size:  1024 | lm loss: 2.007333E+00 | loss scale: 1.0 | grad norm: 0.561 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      749/   25000 | consumed samples:       766976 | elapsed time per iteration (ms): 39220.9 | learning rate: 7.480E-05 | global batch size:  1024 | lm loss: 2.022152E+00 | loss scale: 1.0 | grad norm: 0.568 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      750/   25000 | consumed samples:       768000 | elapsed time per iteration (ms): 39297.5 | learning rate: 7.490E-05 | global batch size:  1024 | lm loss: 2.004277E+00 | loss scale: 1.0 | grad norm: 0.493 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      751/   25000 | consumed samples:       769024 | elapsed time per iteration (ms): 39640.9 | learning rate: 7.500E-05 | global batch size:  1024 | lm loss: 2.000550E+00 | loss scale: 1.0 | grad norm: 0.528 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      752/   25000 | consumed samples:       770048 | elapsed time per iteration (ms): 39412.9 | learning rate: 7.510E-05 | global batch size:  1024 | lm loss: 2.013679E+00 | loss scale: 1.0 | grad norm: 0.486 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      753/   25000 | consumed samples:       771072 | elapsed time per iteration (ms): 39354.0 | learning rate: 7.520E-05 | global batch size:  1024 | lm loss: 2.014590E+00 | loss scale: 1.0 | grad norm: 0.501 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      754/   25000 | consumed samples:       772096 | elapsed time per iteration (ms): 39375.7 | learning rate: 7.530E-05 | global batch size:  1024 | lm loss: 2.008853E+00 | loss scale: 1.0 | grad norm: 0.538 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      755/   25000 | consumed samples:       773120 | elapsed time per iteration (ms): 39217.1 | learning rate: 7.540E-05 | global batch size:  1024 | lm loss: 2.031598E+00 | loss scale: 1.0 | grad norm: 0.591 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      756/   25000 | consumed samples:       774144 | elapsed time per iteration (ms): 39377.4 | learning rate: 7.550E-05 | global batch size:  1024 | lm loss: 1.997047E+00 | loss scale: 1.0 | grad norm: 0.599 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      757/   25000 | consumed samples:       775168 | elapsed time per iteration (ms): 39304.3 | learning rate: 7.560E-05 | global batch size:  1024 | lm loss: 2.015160E+00 | loss scale: 1.0 | grad norm: 0.685 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      758/   25000 | consumed samples:       776192 | elapsed time per iteration (ms): 39434.6 | learning rate: 7.570E-05 | global batch size:  1024 | lm loss: 2.022491E+00 | loss scale: 1.0 | grad norm: 0.668 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      759/   25000 | consumed samples:       777216 | elapsed time per iteration (ms): 39522.1 | learning rate: 7.580E-05 | global batch size:  1024 | lm loss: 1.985748E+00 | loss scale: 1.0 | grad norm: 0.648 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      760/   25000 | consumed samples:       778240 | elapsed time per iteration (ms): 39379.4 | learning rate: 7.590E-05 | global batch size:  1024 | lm loss: 2.002189E+00 | loss scale: 1.0 | grad norm: 0.602 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      761/   25000 | consumed samples:       779264 | elapsed time per iteration (ms): 39361.4 | learning rate: 7.600E-05 | global batch size:  1024 | lm loss: 2.002700E+00 | loss scale: 1.0 | grad norm: 0.646 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      762/   25000 | consumed samples:       780288 | elapsed time per iteration (ms): 39234.1 | learning rate: 7.610E-05 | global batch size:  1024 | lm loss: 2.014437E+00 | loss scale: 1.0 | grad norm: 0.656 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      763/   25000 | consumed samples:       781312 | elapsed time per iteration (ms): 39524.7 | learning rate: 7.620E-05 | global batch size:  1024 | lm loss: 2.017569E+00 | loss scale: 1.0 | grad norm: 0.676 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      764/   25000 | consumed samples:       782336 | elapsed time per iteration (ms): 39231.1 | learning rate: 7.630E-05 | global batch size:  1024 | lm loss: 2.015393E+00 | loss scale: 1.0 | grad norm: 0.727 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      765/   25000 | consumed samples:       783360 | elapsed time per iteration (ms): 39437.8 | learning rate: 7.640E-05 | global batch size:  1024 | lm loss: 2.019880E+00 | loss scale: 1.0 | grad norm: 0.743 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      766/   25000 | consumed samples:       784384 | elapsed time per iteration (ms): 39376.7 | learning rate: 7.650E-05 | global batch size:  1024 | lm loss: 1.988302E+00 | loss scale: 1.0 | grad norm: 0.725 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      767/   25000 | consumed samples:       785408 | elapsed time per iteration (ms): 39453.2 | learning rate: 7.660E-05 | global batch size:  1024 | lm loss: 2.006728E+00 | loss scale: 1.0 | grad norm: 0.613 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      768/   25000 | consumed samples:       786432 | elapsed time per iteration (ms): 39500.8 | learning rate: 7.670E-05 | global batch size:  1024 | lm loss: 2.013921E+00 | loss scale: 1.0 | grad norm: 0.571 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      769/   25000 | consumed samples:       787456 | elapsed time per iteration (ms): 39323.7 | learning rate: 7.680E-05 | global batch size:  1024 | lm loss: 2.013337E+00 | loss scale: 1.0 | grad norm: 0.706 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      770/   25000 | consumed samples:       788480 | elapsed time per iteration (ms): 39318.2 | learning rate: 7.690E-05 | global batch size:  1024 | lm loss: 2.033803E+00 | loss scale: 1.0 | grad norm: 0.707 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      771/   25000 | consumed samples:       789504 | elapsed time per iteration (ms): 39198.6 | learning rate: 7.700E-05 | global batch size:  1024 | lm loss: 2.014542E+00 | loss scale: 1.0 | grad norm: 0.566 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      772/   25000 | consumed samples:       790528 | elapsed time per iteration (ms): 39204.3 | learning rate: 7.710E-05 | global batch size:  1024 | lm loss: 2.002786E+00 | loss scale: 1.0 | grad norm: 0.544 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      773/   25000 | consumed samples:       791552 | elapsed time per iteration (ms): 39443.4 | learning rate: 7.720E-05 | global batch size:  1024 | lm loss: 1.998928E+00 | loss scale: 1.0 | grad norm: 0.529 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      774/   25000 | consumed samples:       792576 | elapsed time per iteration (ms): 39464.4 | learning rate: 7.730E-05 | global batch size:  1024 | lm loss: 2.009419E+00 | loss scale: 1.0 | grad norm: 0.489 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      775/   25000 | consumed samples:       793600 | elapsed time per iteration (ms): 39569.3 | learning rate: 7.740E-05 | global batch size:  1024 | lm loss: 2.007130E+00 | loss scale: 1.0 | grad norm: 0.513 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      776/   25000 | consumed samples:       794624 | elapsed time per iteration (ms): 39417.2 | learning rate: 7.750E-05 | global batch size:  1024 | lm loss: 1.993654E+00 | loss scale: 1.0 | grad norm: 0.526 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      777/   25000 | consumed samples:       795648 | elapsed time per iteration (ms): 39202.0 | learning rate: 7.760E-05 | global batch size:  1024 | lm loss: 2.024084E+00 | loss scale: 1.0 | grad norm: 0.481 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      778/   25000 | consumed samples:       796672 | elapsed time per iteration (ms): 39203.8 | learning rate: 7.770E-05 | global batch size:  1024 | lm loss: 1.998031E+00 | loss scale: 1.0 | grad norm: 0.463 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      779/   25000 | consumed samples:       797696 | elapsed time per iteration (ms): 39390.1 | learning rate: 7.780E-05 | global batch size:  1024 | lm loss: 1.999510E+00 | loss scale: 1.0 | grad norm: 0.471 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      780/   25000 | consumed samples:       798720 | elapsed time per iteration (ms): 39341.5 | learning rate: 7.790E-05 | global batch size:  1024 | lm loss: 2.010456E+00 | loss scale: 1.0 | grad norm: 0.496 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      781/   25000 | consumed samples:       799744 | elapsed time per iteration (ms): 39273.2 | learning rate: 7.800E-05 | global batch size:  1024 | lm loss: 1.998665E+00 | loss scale: 1.0 | grad norm: 0.525 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      782/   25000 | consumed samples:       800768 | elapsed time per iteration (ms): 39463.1 | learning rate: 7.810E-05 | global batch size:  1024 | lm loss: 2.015918E+00 | loss scale: 1.0 | grad norm: 0.642 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      783/   25000 | consumed samples:       801792 | elapsed time per iteration (ms): 39434.4 | learning rate: 7.820E-05 | global batch size:  1024 | lm loss: 2.014432E+00 | loss scale: 1.0 | grad norm: 0.706 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      784/   25000 | consumed samples:       802816 | elapsed time per iteration (ms): 39626.4 | learning rate: 7.830E-05 | global batch size:  1024 | lm loss: 2.003888E+00 | loss scale: 1.0 | grad norm: 0.741 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      785/   25000 | consumed samples:       803840 | elapsed time per iteration (ms): 39508.0 | learning rate: 7.840E-05 | global batch size:  1024 | lm loss: 1.997532E+00 | loss scale: 1.0 | grad norm: 0.808 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      786/   25000 | consumed samples:       804864 | elapsed time per iteration (ms): 39197.3 | learning rate: 7.850E-05 | global batch size:  1024 | lm loss: 2.008177E+00 | loss scale: 1.0 | grad norm: 0.805 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      787/   25000 | consumed samples:       805888 | elapsed time per iteration (ms): 39190.3 | learning rate: 7.860E-05 | global batch size:  1024 | lm loss: 1.995957E+00 | loss scale: 1.0 | grad norm: 0.690 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      788/   25000 | consumed samples:       806912 | elapsed time per iteration (ms): 39210.5 | learning rate: 7.870E-05 | global batch size:  1024 | lm loss: 2.018581E+00 | loss scale: 1.0 | grad norm: 0.588 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      789/   25000 | consumed samples:       807936 | elapsed time per iteration (ms): 39271.5 | learning rate: 7.880E-05 | global batch size:  1024 | lm loss: 1.992126E+00 | loss scale: 1.0 | grad norm: 0.628 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      790/   25000 | consumed samples:       808960 | elapsed time per iteration (ms): 39601.1 | learning rate: 7.890E-05 | global batch size:  1024 | lm loss: 1.989600E+00 | loss scale: 1.0 | grad norm: 0.513 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      791/   25000 | consumed samples:       809984 | elapsed time per iteration (ms): 39579.7 | learning rate: 7.900E-05 | global batch size:  1024 | lm loss: 2.010613E+00 | loss scale: 1.0 | grad norm: 0.608 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      792/   25000 | consumed samples:       811008 | elapsed time per iteration (ms): 39422.1 | learning rate: 7.910E-05 | global batch size:  1024 | lm loss: 2.016317E+00 | loss scale: 1.0 | grad norm: 0.665 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      793/   25000 | consumed samples:       812032 | elapsed time per iteration (ms): 39267.7 | learning rate: 7.920E-05 | global batch size:  1024 | lm loss: 2.015005E+00 | loss scale: 1.0 | grad norm: 0.675 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      794/   25000 | consumed samples:       813056 | elapsed time per iteration (ms): 39209.1 | learning rate: 7.930E-05 | global batch size:  1024 | lm loss: 2.007548E+00 | loss scale: 1.0 | grad norm: 0.682 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      795/   25000 | consumed samples:       814080 | elapsed time per iteration (ms): 39462.0 | learning rate: 7.940E-05 | global batch size:  1024 | lm loss: 1.997457E+00 | loss scale: 1.0 | grad norm: 0.673 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      796/   25000 | consumed samples:       815104 | elapsed time per iteration (ms): 39185.9 | learning rate: 7.950E-05 | global batch size:  1024 | lm loss: 1.999717E+00 | loss scale: 1.0 | grad norm: 0.642 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      797/   25000 | consumed samples:       816128 | elapsed time per iteration (ms): 39248.5 | learning rate: 7.960E-05 | global batch size:  1024 | lm loss: 2.015170E+00 | loss scale: 1.0 | grad norm: 0.673 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      798/   25000 | consumed samples:       817152 | elapsed time per iteration (ms): 39231.5 | learning rate: 7.970E-05 | global batch size:  1024 | lm loss: 1.997095E+00 | loss scale: 1.0 | grad norm: 0.551 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      799/   25000 | consumed samples:       818176 | elapsed time per iteration (ms): 39379.8 | learning rate: 7.980E-05 | global batch size:  1024 | lm loss: 2.001819E+00 | loss scale: 1.0 | grad norm: 0.564 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      800/   25000 | consumed samples:       819200 | elapsed time per iteration (ms): 39795.5 | learning rate: 7.990E-05 | global batch size:  1024 | lm loss: 1.994426E+00 | loss scale: 1.0 | grad norm: 0.557 | number of skipped iterations:   0 | number of nan iterations:   0 |
-----------------------------------------------------------------------------------------------
 validation loss at iteration 800 | lm loss value: 1.998114E+00 | lm loss PPL: 7.375130E+00 | 
-----------------------------------------------------------------------------------------------
 iteration      801/   25000 | consumed samples:       820224 | elapsed time per iteration (ms): 174059.7 | learning rate: 8.000E-05 | global batch size:  1024 | lm loss: 2.027704E+00 | loss scale: 1.0 | grad norm: 0.468 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      802/   25000 | consumed samples:       821248 | elapsed time per iteration (ms): 39252.0 | learning rate: 8.010E-05 | global batch size:  1024 | lm loss: 2.000550E+00 | loss scale: 1.0 | grad norm: 0.416 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      803/   25000 | consumed samples:       822272 | elapsed time per iteration (ms): 39172.6 | learning rate: 8.020E-05 | global batch size:  1024 | lm loss: 1.999388E+00 | loss scale: 1.0 | grad norm: 0.510 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      804/   25000 | consumed samples:       823296 | elapsed time per iteration (ms): 39181.5 | learning rate: 8.030E-05 | global batch size:  1024 | lm loss: 2.000199E+00 | loss scale: 1.0 | grad norm: 0.560 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      805/   25000 | consumed samples:       824320 | elapsed time per iteration (ms): 39378.0 | learning rate: 8.040E-05 | global batch size:  1024 | lm loss: 1.991825E+00 | loss scale: 1.0 | grad norm: 0.571 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      806/   25000 | consumed samples:       825344 | elapsed time per iteration (ms): 39445.0 | learning rate: 8.050E-05 | global batch size:  1024 | lm loss: 2.002167E+00 | loss scale: 1.0 | grad norm: 0.577 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      807/   25000 | consumed samples:       826368 | elapsed time per iteration (ms): 39406.1 | learning rate: 8.060E-05 | global batch size:  1024 | lm loss: 1.999472E+00 | loss scale: 1.0 | grad norm: 0.660 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      808/   25000 | consumed samples:       827392 | elapsed time per iteration (ms): 39423.7 | learning rate: 8.070E-05 | global batch size:  1024 | lm loss: 2.008106E+00 | loss scale: 1.0 | grad norm: 0.714 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      809/   25000 | consumed samples:       828416 | elapsed time per iteration (ms): 39352.9 | learning rate: 8.080E-05 | global batch size:  1024 | lm loss: 2.002590E+00 | loss scale: 1.0 | grad norm: 0.630 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      810/   25000 | consumed samples:       829440 | elapsed time per iteration (ms): 39328.3 | learning rate: 8.090E-05 | global batch size:  1024 | lm loss: 2.000002E+00 | loss scale: 1.0 | grad norm: 0.561 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      811/   25000 | consumed samples:       830464 | elapsed time per iteration (ms): 39462.8 | learning rate: 8.100E-05 | global batch size:  1024 | lm loss: 1.982491E+00 | loss scale: 1.0 | grad norm: 0.571 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      812/   25000 | consumed samples:       831488 | elapsed time per iteration (ms): 39209.2 | learning rate: 8.110E-05 | global batch size:  1024 | lm loss: 1.993274E+00 | loss scale: 1.0 | grad norm: 0.615 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      813/   25000 | consumed samples:       832512 | elapsed time per iteration (ms): 39279.9 | learning rate: 8.120E-05 | global batch size:  1024 | lm loss: 2.008092E+00 | loss scale: 1.0 | grad norm: 0.599 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      814/   25000 | consumed samples:       833536 | elapsed time per iteration (ms): 39296.9 | learning rate: 8.130E-05 | global batch size:  1024 | lm loss: 1.979371E+00 | loss scale: 1.0 | grad norm: 0.603 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      815/   25000 | consumed samples:       834560 | elapsed time per iteration (ms): 39486.4 | learning rate: 8.140E-05 | global batch size:  1024 | lm loss: 2.001741E+00 | loss scale: 1.0 | grad norm: 0.574 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      816/   25000 | consumed samples:       835584 | elapsed time per iteration (ms): 39670.1 | learning rate: 8.150E-05 | global batch size:  1024 | lm loss: 1.999814E+00 | loss scale: 1.0 | grad norm: 0.472 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      817/   25000 | consumed samples:       836608 | elapsed time per iteration (ms): 39217.2 | learning rate: 8.160E-05 | global batch size:  1024 | lm loss: 1.980237E+00 | loss scale: 1.0 | grad norm: 0.470 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      818/   25000 | consumed samples:       837632 | elapsed time per iteration (ms): 39377.7 | learning rate: 8.170E-05 | global batch size:  1024 | lm loss: 2.006657E+00 | loss scale: 1.0 | grad norm: 0.492 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      819/   25000 | consumed samples:       838656 | elapsed time per iteration (ms): 39296.9 | learning rate: 8.180E-05 | global batch size:  1024 | lm loss: 1.998389E+00 | loss scale: 1.0 | grad norm: 0.521 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      820/   25000 | consumed samples:       839680 | elapsed time per iteration (ms): 39268.3 | learning rate: 8.190E-05 | global batch size:  1024 | lm loss: 2.002060E+00 | loss scale: 1.0 | grad norm: 0.552 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      821/   25000 | consumed samples:       840704 | elapsed time per iteration (ms): 39407.1 | learning rate: 8.200E-05 | global batch size:  1024 | lm loss: 2.001254E+00 | loss scale: 1.0 | grad norm: 0.580 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      822/   25000 | consumed samples:       841728 | elapsed time per iteration (ms): 39453.8 | learning rate: 8.210E-05 | global batch size:  1024 | lm loss: 2.000338E+00 | loss scale: 1.0 | grad norm: 0.736 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      823/   25000 | consumed samples:       842752 | elapsed time per iteration (ms): 39339.8 | learning rate: 8.220E-05 | global batch size:  1024 | lm loss: 1.978131E+00 | loss scale: 1.0 | grad norm: 0.680 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      824/   25000 | consumed samples:       843776 | elapsed time per iteration (ms): 39596.7 | learning rate: 8.230E-05 | global batch size:  1024 | lm loss: 1.985539E+00 | loss scale: 1.0 | grad norm: 0.606 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      825/   25000 | consumed samples:       844800 | elapsed time per iteration (ms): 39165.8 | learning rate: 8.240E-05 | global batch size:  1024 | lm loss: 2.003278E+00 | loss scale: 1.0 | grad norm: 0.626 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      826/   25000 | consumed samples:       845824 | elapsed time per iteration (ms): 39169.5 | learning rate: 8.250E-05 | global batch size:  1024 | lm loss: 1.991097E+00 | loss scale: 1.0 | grad norm: 0.629 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      827/   25000 | consumed samples:       846848 | elapsed time per iteration (ms): 39512.6 | learning rate: 8.260E-05 | global batch size:  1024 | lm loss: 1.990077E+00 | loss scale: 1.0 | grad norm: 0.620 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      828/   25000 | consumed samples:       847872 | elapsed time per iteration (ms): 39186.6 | learning rate: 8.270E-05 | global batch size:  1024 | lm loss: 1.995368E+00 | loss scale: 1.0 | grad norm: 0.644 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      829/   25000 | consumed samples:       848896 | elapsed time per iteration (ms): 39377.6 | learning rate: 8.280E-05 | global batch size:  1024 | lm loss: 1.994524E+00 | loss scale: 1.0 | grad norm: 0.622 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      830/   25000 | consumed samples:       849920 | elapsed time per iteration (ms): 39245.0 | learning rate: 8.290E-05 | global batch size:  1024 | lm loss: 1.970645E+00 | loss scale: 1.0 | grad norm: 0.571 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      831/   25000 | consumed samples:       850944 | elapsed time per iteration (ms): 39411.9 | learning rate: 8.300E-05 | global batch size:  1024 | lm loss: 1.993948E+00 | loss scale: 1.0 | grad norm: 0.513 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      832/   25000 | consumed samples:       851968 | elapsed time per iteration (ms): 39683.2 | learning rate: 8.310E-05 | global batch size:  1024 | lm loss: 1.989126E+00 | loss scale: 1.0 | grad norm: 0.529 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      833/   25000 | consumed samples:       852992 | elapsed time per iteration (ms): 39193.5 | learning rate: 8.320E-05 | global batch size:  1024 | lm loss: 1.996650E+00 | loss scale: 1.0 | grad norm: 0.505 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      834/   25000 | consumed samples:       854016 | elapsed time per iteration (ms): 39320.5 | learning rate: 8.330E-05 | global batch size:  1024 | lm loss: 1.979749E+00 | loss scale: 1.0 | grad norm: 0.544 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      835/   25000 | consumed samples:       855040 | elapsed time per iteration (ms): 39198.4 | learning rate: 8.340E-05 | global batch size:  1024 | lm loss: 1.981401E+00 | loss scale: 1.0 | grad norm: 0.577 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      836/   25000 | consumed samples:       856064 | elapsed time per iteration (ms): 39348.6 | learning rate: 8.350E-05 | global batch size:  1024 | lm loss: 1.983115E+00 | loss scale: 1.0 | grad norm: 0.699 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      837/   25000 | consumed samples:       857088 | elapsed time per iteration (ms): 39385.5 | learning rate: 8.360E-05 | global batch size:  1024 | lm loss: 1.990091E+00 | loss scale: 1.0 | grad norm: 0.751 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      838/   25000 | consumed samples:       858112 | elapsed time per iteration (ms): 39277.4 | learning rate: 8.370E-05 | global batch size:  1024 | lm loss: 1.976436E+00 | loss scale: 1.0 | grad norm: 0.638 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      839/   25000 | consumed samples:       859136 | elapsed time per iteration (ms): 39561.5 | learning rate: 8.380E-05 | global batch size:  1024 | lm loss: 1.976740E+00 | loss scale: 1.0 | grad norm: 0.657 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      840/   25000 | consumed samples:       860160 | elapsed time per iteration (ms): 39488.3 | learning rate: 8.390E-05 | global batch size:  1024 | lm loss: 1.983136E+00 | loss scale: 1.0 | grad norm: 0.682 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      841/   25000 | consumed samples:       861184 | elapsed time per iteration (ms): 39270.7 | learning rate: 8.400E-05 | global batch size:  1024 | lm loss: 1.993556E+00 | loss scale: 1.0 | grad norm: 0.490 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      842/   25000 | consumed samples:       862208 | elapsed time per iteration (ms): 39204.1 | learning rate: 8.410E-05 | global batch size:  1024 | lm loss: 1.999018E+00 | loss scale: 1.0 | grad norm: 0.546 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      843/   25000 | consumed samples:       863232 | elapsed time per iteration (ms): 39388.3 | learning rate: 8.420E-05 | global batch size:  1024 | lm loss: 1.980122E+00 | loss scale: 1.0 | grad norm: 0.580 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      844/   25000 | consumed samples:       864256 | elapsed time per iteration (ms): 39339.8 | learning rate: 8.430E-05 | global batch size:  1024 | lm loss: 1.990789E+00 | loss scale: 1.0 | grad norm: 0.524 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      845/   25000 | consumed samples:       865280 | elapsed time per iteration (ms): 39421.3 | learning rate: 8.440E-05 | global batch size:  1024 | lm loss: 1.982583E+00 | loss scale: 1.0 | grad norm: 0.513 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      846/   25000 | consumed samples:       866304 | elapsed time per iteration (ms): 39225.0 | learning rate: 8.450E-05 | global batch size:  1024 | lm loss: 1.974999E+00 | loss scale: 1.0 | grad norm: 0.440 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      847/   25000 | consumed samples:       867328 | elapsed time per iteration (ms): 39345.7 | learning rate: 8.460E-05 | global batch size:  1024 | lm loss: 1.987125E+00 | loss scale: 1.0 | grad norm: 0.485 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      848/   25000 | consumed samples:       868352 | elapsed time per iteration (ms): 39682.5 | learning rate: 8.470E-05 | global batch size:  1024 | lm loss: 1.972345E+00 | loss scale: 1.0 | grad norm: 0.448 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      849/   25000 | consumed samples:       869376 | elapsed time per iteration (ms): 39464.9 | learning rate: 8.480E-05 | global batch size:  1024 | lm loss: 1.971724E+00 | loss scale: 1.0 | grad norm: 0.427 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      850/   25000 | consumed samples:       870400 | elapsed time per iteration (ms): 39209.7 | learning rate: 8.490E-05 | global batch size:  1024 | lm loss: 1.977325E+00 | loss scale: 1.0 | grad norm: 0.544 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      851/   25000 | consumed samples:       871424 | elapsed time per iteration (ms): 39207.6 | learning rate: 8.500E-05 | global batch size:  1024 | lm loss: 1.993923E+00 | loss scale: 1.0 | grad norm: 0.582 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      852/   25000 | consumed samples:       872448 | elapsed time per iteration (ms): 39196.3 | learning rate: 8.510E-05 | global batch size:  1024 | lm loss: 1.983425E+00 | loss scale: 1.0 | grad norm: 0.586 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      853/   25000 | consumed samples:       873472 | elapsed time per iteration (ms): 39381.6 | learning rate: 8.520E-05 | global batch size:  1024 | lm loss: 2.000822E+00 | loss scale: 1.0 | grad norm: 0.544 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      854/   25000 | consumed samples:       874496 | elapsed time per iteration (ms): 39529.7 | learning rate: 8.530E-05 | global batch size:  1024 | lm loss: 1.993154E+00 | loss scale: 1.0 | grad norm: 0.648 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      855/   25000 | consumed samples:       875520 | elapsed time per iteration (ms): 39307.6 | learning rate: 8.540E-05 | global batch size:  1024 | lm loss: 1.997117E+00 | loss scale: 1.0 | grad norm: 0.687 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      856/   25000 | consumed samples:       876544 | elapsed time per iteration (ms): 39401.4 | learning rate: 8.550E-05 | global batch size:  1024 | lm loss: 1.969375E+00 | loss scale: 1.0 | grad norm: 0.717 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      857/   25000 | consumed samples:       877568 | elapsed time per iteration (ms): 39385.3 | learning rate: 8.560E-05 | global batch size:  1024 | lm loss: 1.990070E+00 | loss scale: 1.0 | grad norm: 0.795 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      858/   25000 | consumed samples:       878592 | elapsed time per iteration (ms): 39168.6 | learning rate: 8.570E-05 | global batch size:  1024 | lm loss: 1.980814E+00 | loss scale: 1.0 | grad norm: 0.754 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      859/   25000 | consumed samples:       879616 | elapsed time per iteration (ms): 39488.1 | learning rate: 8.580E-05 | global batch size:  1024 | lm loss: 1.975140E+00 | loss scale: 1.0 | grad norm: 0.753 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      860/   25000 | consumed samples:       880640 | elapsed time per iteration (ms): 39182.6 | learning rate: 8.590E-05 | global batch size:  1024 | lm loss: 1.998954E+00 | loss scale: 1.0 | grad norm: 0.815 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      861/   25000 | consumed samples:       881664 | elapsed time per iteration (ms): 39252.1 | learning rate: 8.600E-05 | global batch size:  1024 | lm loss: 1.977899E+00 | loss scale: 1.0 | grad norm: 0.697 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      862/   25000 | consumed samples:       882688 | elapsed time per iteration (ms): 39166.2 | learning rate: 8.610E-05 | global batch size:  1024 | lm loss: 1.987798E+00 | loss scale: 1.0 | grad norm: 0.620 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      863/   25000 | consumed samples:       883712 | elapsed time per iteration (ms): 39261.3 | learning rate: 8.620E-05 | global batch size:  1024 | lm loss: 1.976906E+00 | loss scale: 1.0 | grad norm: 0.579 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      864/   25000 | consumed samples:       884736 | elapsed time per iteration (ms): 39952.8 | learning rate: 8.630E-05 | global batch size:  1024 | lm loss: 1.966263E+00 | loss scale: 1.0 | grad norm: 0.520 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      865/   25000 | consumed samples:       885760 | elapsed time per iteration (ms): 39320.1 | learning rate: 8.640E-05 | global batch size:  1024 | lm loss: 1.984131E+00 | loss scale: 1.0 | grad norm: 0.543 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      866/   25000 | consumed samples:       886784 | elapsed time per iteration (ms): 39251.7 | learning rate: 8.650E-05 | global batch size:  1024 | lm loss: 1.998529E+00 | loss scale: 1.0 | grad norm: 0.443 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      867/   25000 | consumed samples:       887808 | elapsed time per iteration (ms): 39191.3 | learning rate: 8.660E-05 | global batch size:  1024 | lm loss: 1.984529E+00 | loss scale: 1.0 | grad norm: 0.467 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      868/   25000 | consumed samples:       888832 | elapsed time per iteration (ms): 39197.2 | learning rate: 8.670E-05 | global batch size:  1024 | lm loss: 1.980329E+00 | loss scale: 1.0 | grad norm: 0.494 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      869/   25000 | consumed samples:       889856 | elapsed time per iteration (ms): 39566.0 | learning rate: 8.680E-05 | global batch size:  1024 | lm loss: 1.977157E+00 | loss scale: 1.0 | grad norm: 0.509 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      870/   25000 | consumed samples:       890880 | elapsed time per iteration (ms): 39177.1 | learning rate: 8.690E-05 | global batch size:  1024 | lm loss: 1.982595E+00 | loss scale: 1.0 | grad norm: 0.473 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      871/   25000 | consumed samples:       891904 | elapsed time per iteration (ms): 39254.1 | learning rate: 8.700E-05 | global batch size:  1024 | lm loss: 1.986873E+00 | loss scale: 1.0 | grad norm: 0.452 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      872/   25000 | consumed samples:       892928 | elapsed time per iteration (ms): 39487.7 | learning rate: 8.710E-05 | global batch size:  1024 | lm loss: 1.980756E+00 | loss scale: 1.0 | grad norm: 0.479 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      873/   25000 | consumed samples:       893952 | elapsed time per iteration (ms): 39634.8 | learning rate: 8.720E-05 | global batch size:  1024 | lm loss: 1.989470E+00 | loss scale: 1.0 | grad norm: 0.531 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      874/   25000 | consumed samples:       894976 | elapsed time per iteration (ms): 39317.1 | learning rate: 8.730E-05 | global batch size:  1024 | lm loss: 1.987318E+00 | loss scale: 1.0 | grad norm: 0.576 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      875/   25000 | consumed samples:       896000 | elapsed time per iteration (ms): 39353.0 | learning rate: 8.740E-05 | global batch size:  1024 | lm loss: 1.974555E+00 | loss scale: 1.0 | grad norm: 0.565 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      876/   25000 | consumed samples:       897024 | elapsed time per iteration (ms): 39229.1 | learning rate: 8.750E-05 | global batch size:  1024 | lm loss: 1.968863E+00 | loss scale: 1.0 | grad norm: 0.509 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      877/   25000 | consumed samples:       898048 | elapsed time per iteration (ms): 39282.3 | learning rate: 8.760E-05 | global batch size:  1024 | lm loss: 1.985576E+00 | loss scale: 1.0 | grad norm: 0.508 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      878/   25000 | consumed samples:       899072 | elapsed time per iteration (ms): 39340.8 | learning rate: 8.770E-05 | global batch size:  1024 | lm loss: 1.991265E+00 | loss scale: 1.0 | grad norm: 0.543 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      879/   25000 | consumed samples:       900096 | elapsed time per iteration (ms): 39312.9 | learning rate: 8.780E-05 | global batch size:  1024 | lm loss: 1.968015E+00 | loss scale: 1.0 | grad norm: 0.540 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      880/   25000 | consumed samples:       901120 | elapsed time per iteration (ms): 39624.2 | learning rate: 8.790E-05 | global batch size:  1024 | lm loss: 1.992667E+00 | loss scale: 1.0 | grad norm: 0.629 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      881/   25000 | consumed samples:       902144 | elapsed time per iteration (ms): 39426.4 | learning rate: 8.800E-05 | global batch size:  1024 | lm loss: 1.977217E+00 | loss scale: 1.0 | grad norm: 0.700 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      882/   25000 | consumed samples:       903168 | elapsed time per iteration (ms): 39361.3 | learning rate: 8.810E-05 | global batch size:  1024 | lm loss: 1.963654E+00 | loss scale: 1.0 | grad norm: 0.683 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      883/   25000 | consumed samples:       904192 | elapsed time per iteration (ms): 39345.4 | learning rate: 8.820E-05 | global batch size:  1024 | lm loss: 1.984949E+00 | loss scale: 1.0 | grad norm: 0.675 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      884/   25000 | consumed samples:       905216 | elapsed time per iteration (ms): 39205.9 | learning rate: 8.830E-05 | global batch size:  1024 | lm loss: 1.982884E+00 | loss scale: 1.0 | grad norm: 0.676 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      885/   25000 | consumed samples:       906240 | elapsed time per iteration (ms): 39521.0 | learning rate: 8.840E-05 | global batch size:  1024 | lm loss: 1.975543E+00 | loss scale: 1.0 | grad norm: 0.606 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      886/   25000 | consumed samples:       907264 | elapsed time per iteration (ms): 39174.5 | learning rate: 8.850E-05 | global batch size:  1024 | lm loss: 1.976001E+00 | loss scale: 1.0 | grad norm: 0.565 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      887/   25000 | consumed samples:       908288 | elapsed time per iteration (ms): 39258.2 | learning rate: 8.860E-05 | global batch size:  1024 | lm loss: 1.969933E+00 | loss scale: 1.0 | grad norm: 0.445 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      888/   25000 | consumed samples:       909312 | elapsed time per iteration (ms): 39523.6 | learning rate: 8.870E-05 | global batch size:  1024 | lm loss: 1.976708E+00 | loss scale: 1.0 | grad norm: 0.509 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      889/   25000 | consumed samples:       910336 | elapsed time per iteration (ms): 39398.4 | learning rate: 8.880E-05 | global batch size:  1024 | lm loss: 1.972091E+00 | loss scale: 1.0 | grad norm: 0.562 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      890/   25000 | consumed samples:       911360 | elapsed time per iteration (ms): 39305.9 | learning rate: 8.890E-05 | global batch size:  1024 | lm loss: 1.976066E+00 | loss scale: 1.0 | grad norm: 0.500 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      891/   25000 | consumed samples:       912384 | elapsed time per iteration (ms): 39425.0 | learning rate: 8.900E-05 | global batch size:  1024 | lm loss: 1.972708E+00 | loss scale: 1.0 | grad norm: 0.486 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      892/   25000 | consumed samples:       913408 | elapsed time per iteration (ms): 39216.3 | learning rate: 8.910E-05 | global batch size:  1024 | lm loss: 1.962501E+00 | loss scale: 1.0 | grad norm: 0.446 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      893/   25000 | consumed samples:       914432 | elapsed time per iteration (ms): 39410.9 | learning rate: 8.920E-05 | global batch size:  1024 | lm loss: 1.973784E+00 | loss scale: 1.0 | grad norm: 0.503 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      894/   25000 | consumed samples:       915456 | elapsed time per iteration (ms): 39255.9 | learning rate: 8.930E-05 | global batch size:  1024 | lm loss: 1.959100E+00 | loss scale: 1.0 | grad norm: 0.656 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      895/   25000 | consumed samples:       916480 | elapsed time per iteration (ms): 39285.9 | learning rate: 8.940E-05 | global batch size:  1024 | lm loss: 1.979799E+00 | loss scale: 1.0 | grad norm: 1.262 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      896/   25000 | consumed samples:       917504 | elapsed time per iteration (ms): 39532.4 | learning rate: 8.950E-05 | global batch size:  1024 | lm loss: 1.983210E+00 | loss scale: 1.0 | grad norm: 0.513 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      897/   25000 | consumed samples:       918528 | elapsed time per iteration (ms): 39495.2 | learning rate: 8.960E-05 | global batch size:  1024 | lm loss: 1.960900E+00 | loss scale: 1.0 | grad norm: 0.538 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      898/   25000 | consumed samples:       919552 | elapsed time per iteration (ms): 39332.6 | learning rate: 8.970E-05 | global batch size:  1024 | lm loss: 1.986266E+00 | loss scale: 1.0 | grad norm: 0.708 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      899/   25000 | consumed samples:       920576 | elapsed time per iteration (ms): 39207.6 | learning rate: 8.980E-05 | global batch size:  1024 | lm loss: 1.963153E+00 | loss scale: 1.0 | grad norm: 0.686 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      900/   25000 | consumed samples:       921600 | elapsed time per iteration (ms): 39358.0 | learning rate: 8.990E-05 | global batch size:  1024 | lm loss: 1.970781E+00 | loss scale: 1.0 | grad norm: 0.560 | number of skipped iterations:   0 | number of nan iterations:   0 |
-----------------------------------------------------------------------------------------------
 validation loss at iteration 900 | lm loss value: 1.966159E+00 | lm loss PPL: 7.143186E+00 | 
-----------------------------------------------------------------------------------------------
 iteration      901/   25000 | consumed samples:       922624 | elapsed time per iteration (ms): 174655.0 | learning rate: 9.000E-05 | global batch size:  1024 | lm loss: 1.976470E+00 | loss scale: 1.0 | grad norm: 0.493 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      902/   25000 | consumed samples:       923648 | elapsed time per iteration (ms): 39221.1 | learning rate: 9.010E-05 | global batch size:  1024 | lm loss: 1.972263E+00 | loss scale: 1.0 | grad norm: 0.606 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      903/   25000 | consumed samples:       924672 | elapsed time per iteration (ms): 39502.9 | learning rate: 9.020E-05 | global batch size:  1024 | lm loss: 1.949284E+00 | loss scale: 1.0 | grad norm: 0.588 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      904/   25000 | consumed samples:       925696 | elapsed time per iteration (ms): 39366.5 | learning rate: 9.030E-05 | global batch size:  1024 | lm loss: 1.972511E+00 | loss scale: 1.0 | grad norm: 0.518 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      905/   25000 | consumed samples:       926720 | elapsed time per iteration (ms): 39489.1 | learning rate: 9.040E-05 | global batch size:  1024 | lm loss: 1.985848E+00 | loss scale: 1.0 | grad norm: 0.634 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      906/   25000 | consumed samples:       927744 | elapsed time per iteration (ms): 39339.0 | learning rate: 9.050E-05 | global batch size:  1024 | lm loss: 1.961377E+00 | loss scale: 1.0 | grad norm: 0.636 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      907/   25000 | consumed samples:       928768 | elapsed time per iteration (ms): 39298.5 | learning rate: 9.060E-05 | global batch size:  1024 | lm loss: 1.973981E+00 | loss scale: 1.0 | grad norm: 0.599 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      908/   25000 | consumed samples:       929792 | elapsed time per iteration (ms): 39342.2 | learning rate: 9.070E-05 | global batch size:  1024 | lm loss: 1.958203E+00 | loss scale: 1.0 | grad norm: 0.657 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      909/   25000 | consumed samples:       930816 | elapsed time per iteration (ms): 39462.7 | learning rate: 9.080E-05 | global batch size:  1024 | lm loss: 1.978221E+00 | loss scale: 1.0 | grad norm: 0.645 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      910/   25000 | consumed samples:       931840 | elapsed time per iteration (ms): 39206.7 | learning rate: 9.090E-05 | global batch size:  1024 | lm loss: 1.970000E+00 | loss scale: 1.0 | grad norm: 0.506 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      911/   25000 | consumed samples:       932864 | elapsed time per iteration (ms): 39287.0 | learning rate: 9.100E-05 | global batch size:  1024 | lm loss: 1.946642E+00 | loss scale: 1.0 | grad norm: 0.396 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      912/   25000 | consumed samples:       933888 | elapsed time per iteration (ms): 39625.5 | learning rate: 9.110E-05 | global batch size:  1024 | lm loss: 1.979751E+00 | loss scale: 1.0 | grad norm: 0.525 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      913/   25000 | consumed samples:       934912 | elapsed time per iteration (ms): 39637.1 | learning rate: 9.120E-05 | global batch size:  1024 | lm loss: 1.957975E+00 | loss scale: 1.0 | grad norm: 0.573 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      914/   25000 | consumed samples:       935936 | elapsed time per iteration (ms): 39345.7 | learning rate: 9.130E-05 | global batch size:  1024 | lm loss: 1.955202E+00 | loss scale: 1.0 | grad norm: 0.575 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      915/   25000 | consumed samples:       936960 | elapsed time per iteration (ms): 39213.1 | learning rate: 9.140E-05 | global batch size:  1024 | lm loss: 1.967474E+00 | loss scale: 1.0 | grad norm: 0.526 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      916/   25000 | consumed samples:       937984 | elapsed time per iteration (ms): 39205.0 | learning rate: 9.150E-05 | global batch size:  1024 | lm loss: 1.972617E+00 | loss scale: 1.0 | grad norm: 0.477 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      917/   25000 | consumed samples:       939008 | elapsed time per iteration (ms): 39467.5 | learning rate: 9.160E-05 | global batch size:  1024 | lm loss: 1.962197E+00 | loss scale: 1.0 | grad norm: 0.539 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      918/   25000 | consumed samples:       940032 | elapsed time per iteration (ms): 39472.6 | learning rate: 9.170E-05 | global batch size:  1024 | lm loss: 1.967748E+00 | loss scale: 1.0 | grad norm: 0.525 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      919/   25000 | consumed samples:       941056 | elapsed time per iteration (ms): 39280.5 | learning rate: 9.180E-05 | global batch size:  1024 | lm loss: 1.952734E+00 | loss scale: 1.0 | grad norm: 0.479 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      920/   25000 | consumed samples:       942080 | elapsed time per iteration (ms): 39326.8 | learning rate: 9.190E-05 | global batch size:  1024 | lm loss: 1.957024E+00 | loss scale: 1.0 | grad norm: 0.497 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      921/   25000 | consumed samples:       943104 | elapsed time per iteration (ms): 39555.8 | learning rate: 9.200E-05 | global batch size:  1024 | lm loss: 1.978857E+00 | loss scale: 1.0 | grad norm: 0.517 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      922/   25000 | consumed samples:       944128 | elapsed time per iteration (ms): 40333.0 | learning rate: 9.210E-05 | global batch size:  1024 | lm loss: 1.942378E+00 | loss scale: 1.0 | grad norm: 0.468 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      923/   25000 | consumed samples:       945152 | elapsed time per iteration (ms): 39276.5 | learning rate: 9.220E-05 | global batch size:  1024 | lm loss: 1.962636E+00 | loss scale: 1.0 | grad norm: 0.546 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      924/   25000 | consumed samples:       946176 | elapsed time per iteration (ms): 39213.8 | learning rate: 9.230E-05 | global batch size:  1024 | lm loss: 1.970235E+00 | loss scale: 1.0 | grad norm: 0.672 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      925/   25000 | consumed samples:       947200 | elapsed time per iteration (ms): 39224.5 | learning rate: 9.240E-05 | global batch size:  1024 | lm loss: 1.953247E+00 | loss scale: 1.0 | grad norm: 0.705 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      926/   25000 | consumed samples:       948224 | elapsed time per iteration (ms): 39288.7 | learning rate: 9.250E-05 | global batch size:  1024 | lm loss: 1.960240E+00 | loss scale: 1.0 | grad norm: 0.636 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      927/   25000 | consumed samples:       949248 | elapsed time per iteration (ms): 39585.7 | learning rate: 9.260E-05 | global batch size:  1024 | lm loss: 1.961723E+00 | loss scale: 1.0 | grad norm: 0.566 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      928/   25000 | consumed samples:       950272 | elapsed time per iteration (ms): 39521.2 | learning rate: 9.270E-05 | global batch size:  1024 | lm loss: 1.969468E+00 | loss scale: 1.0 | grad norm: 0.616 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      929/   25000 | consumed samples:       951296 | elapsed time per iteration (ms): 39525.9 | learning rate: 9.280E-05 | global batch size:  1024 | lm loss: 1.965212E+00 | loss scale: 1.0 | grad norm: 0.595 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      930/   25000 | consumed samples:       952320 | elapsed time per iteration (ms): 39296.3 | learning rate: 9.290E-05 | global batch size:  1024 | lm loss: 1.971022E+00 | loss scale: 1.0 | grad norm: 0.603 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      931/   25000 | consumed samples:       953344 | elapsed time per iteration (ms): 39234.2 | learning rate: 9.300E-05 | global batch size:  1024 | lm loss: 1.969449E+00 | loss scale: 1.0 | grad norm: 0.623 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      932/   25000 | consumed samples:       954368 | elapsed time per iteration (ms): 39345.1 | learning rate: 9.310E-05 | global batch size:  1024 | lm loss: 1.983471E+00 | loss scale: 1.0 | grad norm: 0.720 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      933/   25000 | consumed samples:       955392 | elapsed time per iteration (ms): 39400.6 | learning rate: 9.320E-05 | global batch size:  1024 | lm loss: 1.955750E+00 | loss scale: 1.0 | grad norm: 0.734 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      934/   25000 | consumed samples:       956416 | elapsed time per iteration (ms): 39323.9 | learning rate: 9.330E-05 | global batch size:  1024 | lm loss: 1.957359E+00 | loss scale: 1.0 | grad norm: 0.578 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      935/   25000 | consumed samples:       957440 | elapsed time per iteration (ms): 39307.2 | learning rate: 9.340E-05 | global batch size:  1024 | lm loss: 1.965054E+00 | loss scale: 1.0 | grad norm: 0.537 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      936/   25000 | consumed samples:       958464 | elapsed time per iteration (ms): 39515.1 | learning rate: 9.350E-05 | global batch size:  1024 | lm loss: 1.947691E+00 | loss scale: 1.0 | grad norm: 0.629 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      937/   25000 | consumed samples:       959488 | elapsed time per iteration (ms): 39571.9 | learning rate: 9.360E-05 | global batch size:  1024 | lm loss: 1.961434E+00 | loss scale: 1.0 | grad norm: 0.615 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      938/   25000 | consumed samples:       960512 | elapsed time per iteration (ms): 39489.0 | learning rate: 9.370E-05 | global batch size:  1024 | lm loss: 1.964378E+00 | loss scale: 1.0 | grad norm: 0.516 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      939/   25000 | consumed samples:       961536 | elapsed time per iteration (ms): 39305.6 | learning rate: 9.380E-05 | global batch size:  1024 | lm loss: 1.979305E+00 | loss scale: 1.0 | grad norm: 0.517 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      940/   25000 | consumed samples:       962560 | elapsed time per iteration (ms): 39245.9 | learning rate: 9.390E-05 | global batch size:  1024 | lm loss: 1.946845E+00 | loss scale: 1.0 | grad norm: 0.508 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      941/   25000 | consumed samples:       963584 | elapsed time per iteration (ms): 39229.4 | learning rate: 9.400E-05 | global batch size:  1024 | lm loss: 1.965515E+00 | loss scale: 1.0 | grad norm: 0.503 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      942/   25000 | consumed samples:       964608 | elapsed time per iteration (ms): 39425.4 | learning rate: 9.410E-05 | global batch size:  1024 | lm loss: 1.962680E+00 | loss scale: 1.0 | grad norm: 0.468 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      943/   25000 | consumed samples:       965632 | elapsed time per iteration (ms): 39404.5 | learning rate: 9.420E-05 | global batch size:  1024 | lm loss: 1.953490E+00 | loss scale: 1.0 | grad norm: 0.452 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      944/   25000 | consumed samples:       966656 | elapsed time per iteration (ms): 39444.5 | learning rate: 9.430E-05 | global batch size:  1024 | lm loss: 1.957273E+00 | loss scale: 1.0 | grad norm: 0.464 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      945/   25000 | consumed samples:       967680 | elapsed time per iteration (ms): 39602.6 | learning rate: 9.440E-05 | global batch size:  1024 | lm loss: 1.959613E+00 | loss scale: 1.0 | grad norm: 0.460 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      946/   25000 | consumed samples:       968704 | elapsed time per iteration (ms): 39308.8 | learning rate: 9.450E-05 | global batch size:  1024 | lm loss: 1.952408E+00 | loss scale: 1.0 | grad norm: 0.486 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      947/   25000 | consumed samples:       969728 | elapsed time per iteration (ms): 39363.3 | learning rate: 9.460E-05 | global batch size:  1024 | lm loss: 1.973747E+00 | loss scale: 1.0 | grad norm: 0.586 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      948/   25000 | consumed samples:       970752 | elapsed time per iteration (ms): 39307.8 | learning rate: 9.470E-05 | global batch size:  1024 | lm loss: 1.959203E+00 | loss scale: 1.0 | grad norm: 0.661 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      949/   25000 | consumed samples:       971776 | elapsed time per iteration (ms): 39390.5 | learning rate: 9.480E-05 | global batch size:  1024 | lm loss: 1.962215E+00 | loss scale: 1.0 | grad norm: 0.648 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      950/   25000 | consumed samples:       972800 | elapsed time per iteration (ms): 39348.7 | learning rate: 9.490E-05 | global batch size:  1024 | lm loss: 1.967146E+00 | loss scale: 1.0 | grad norm: 0.633 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      951/   25000 | consumed samples:       973824 | elapsed time per iteration (ms): 39256.3 | learning rate: 9.500E-05 | global batch size:  1024 | lm loss: 1.956464E+00 | loss scale: 1.0 | grad norm: 0.560 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      952/   25000 | consumed samples:       974848 | elapsed time per iteration (ms): 39461.5 | learning rate: 9.510E-05 | global batch size:  1024 | lm loss: 1.969909E+00 | loss scale: 1.0 | grad norm: 0.500 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      953/   25000 | consumed samples:       975872 | elapsed time per iteration (ms): 39401.5 | learning rate: 9.520E-05 | global batch size:  1024 | lm loss: 1.953873E+00 | loss scale: 1.0 | grad norm: 0.511 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      954/   25000 | consumed samples:       976896 | elapsed time per iteration (ms): 39458.1 | learning rate: 9.530E-05 | global batch size:  1024 | lm loss: 1.965403E+00 | loss scale: 1.0 | grad norm: 0.478 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      955/   25000 | consumed samples:       977920 | elapsed time per iteration (ms): 39368.8 | learning rate: 9.540E-05 | global batch size:  1024 | lm loss: 1.964022E+00 | loss scale: 1.0 | grad norm: 0.412 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      956/   25000 | consumed samples:       978944 | elapsed time per iteration (ms): 39188.0 | learning rate: 9.550E-05 | global batch size:  1024 | lm loss: 1.955255E+00 | loss scale: 1.0 | grad norm: 0.441 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      957/   25000 | consumed samples:       979968 | elapsed time per iteration (ms): 39316.2 | learning rate: 9.560E-05 | global batch size:  1024 | lm loss: 1.953474E+00 | loss scale: 1.0 | grad norm: 0.437 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      958/   25000 | consumed samples:       980992 | elapsed time per iteration (ms): 39358.4 | learning rate: 9.570E-05 | global batch size:  1024 | lm loss: 1.962843E+00 | loss scale: 1.0 | grad norm: 0.401 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      959/   25000 | consumed samples:       982016 | elapsed time per iteration (ms): 39400.3 | learning rate: 9.580E-05 | global batch size:  1024 | lm loss: 1.969233E+00 | loss scale: 1.0 | grad norm: 0.470 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      960/   25000 | consumed samples:       983040 | elapsed time per iteration (ms): 39352.4 | learning rate: 9.590E-05 | global batch size:  1024 | lm loss: 1.955599E+00 | loss scale: 1.0 | grad norm: 0.464 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      961/   25000 | consumed samples:       984064 | elapsed time per iteration (ms): 39420.8 | learning rate: 9.600E-05 | global batch size:  1024 | lm loss: 1.949686E+00 | loss scale: 1.0 | grad norm: 0.480 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      962/   25000 | consumed samples:       985088 | elapsed time per iteration (ms): 39473.0 | learning rate: 9.610E-05 | global batch size:  1024 | lm loss: 1.963013E+00 | loss scale: 1.0 | grad norm: 0.519 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      963/   25000 | consumed samples:       986112 | elapsed time per iteration (ms): 39201.2 | learning rate: 9.620E-05 | global batch size:  1024 | lm loss: 1.958701E+00 | loss scale: 1.0 | grad norm: 0.603 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      964/   25000 | consumed samples:       987136 | elapsed time per iteration (ms): 39369.2 | learning rate: 9.630E-05 | global batch size:  1024 | lm loss: 1.960737E+00 | loss scale: 1.0 | grad norm: 0.623 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      965/   25000 | consumed samples:       988160 | elapsed time per iteration (ms): 39330.2 | learning rate: 9.640E-05 | global batch size:  1024 | lm loss: 1.955357E+00 | loss scale: 1.0 | grad norm: 0.638 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      966/   25000 | consumed samples:       989184 | elapsed time per iteration (ms): 39294.6 | learning rate: 9.650E-05 | global batch size:  1024 | lm loss: 1.950582E+00 | loss scale: 1.0 | grad norm: 0.579 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      967/   25000 | consumed samples:       990208 | elapsed time per iteration (ms): 39472.8 | learning rate: 9.660E-05 | global batch size:  1024 | lm loss: 1.968119E+00 | loss scale: 1.0 | grad norm: 0.576 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      968/   25000 | consumed samples:       991232 | elapsed time per iteration (ms): 39289.9 | learning rate: 9.670E-05 | global batch size:  1024 | lm loss: 1.950150E+00 | loss scale: 1.0 | grad norm: 0.624 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      969/   25000 | consumed samples:       992256 | elapsed time per iteration (ms): 39428.0 | learning rate: 9.680E-05 | global batch size:  1024 | lm loss: 1.969709E+00 | loss scale: 1.0 | grad norm: 0.543 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      970/   25000 | consumed samples:       993280 | elapsed time per iteration (ms): 39470.1 | learning rate: 9.690E-05 | global batch size:  1024 | lm loss: 1.957398E+00 | loss scale: 1.0 | grad norm: 0.588 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      971/   25000 | consumed samples:       994304 | elapsed time per iteration (ms): 39276.3 | learning rate: 9.700E-05 | global batch size:  1024 | lm loss: 1.944830E+00 | loss scale: 1.0 | grad norm: 0.568 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      972/   25000 | consumed samples:       995328 | elapsed time per iteration (ms): 39257.8 | learning rate: 9.710E-05 | global batch size:  1024 | lm loss: 1.958563E+00 | loss scale: 1.0 | grad norm: 0.496 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      973/   25000 | consumed samples:       996352 | elapsed time per iteration (ms): 39353.5 | learning rate: 9.720E-05 | global batch size:  1024 | lm loss: 1.953889E+00 | loss scale: 1.0 | grad norm: 0.582 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      974/   25000 | consumed samples:       997376 | elapsed time per iteration (ms): 39248.8 | learning rate: 9.730E-05 | global batch size:  1024 | lm loss: 1.950252E+00 | loss scale: 1.0 | grad norm: 0.584 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      975/   25000 | consumed samples:       998400 | elapsed time per iteration (ms): 39449.0 | learning rate: 9.740E-05 | global batch size:  1024 | lm loss: 1.951745E+00 | loss scale: 1.0 | grad norm: 0.567 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      976/   25000 | consumed samples:       999424 | elapsed time per iteration (ms): 39325.4 | learning rate: 9.750E-05 | global batch size:  1024 | lm loss: 1.953364E+00 | loss scale: 1.0 | grad norm: 0.602 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      977/   25000 | consumed samples:      1000448 | elapsed time per iteration (ms): 39466.8 | learning rate: 9.760E-05 | global batch size:  1024 | lm loss: 1.948089E+00 | loss scale: 1.0 | grad norm: 0.574 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      978/   25000 | consumed samples:      1001472 | elapsed time per iteration (ms): 39399.6 | learning rate: 9.770E-05 | global batch size:  1024 | lm loss: 1.943299E+00 | loss scale: 1.0 | grad norm: 0.522 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      979/   25000 | consumed samples:      1002496 | elapsed time per iteration (ms): 39184.9 | learning rate: 9.780E-05 | global batch size:  1024 | lm loss: 1.962897E+00 | loss scale: 1.0 | grad norm: 0.522 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      980/   25000 | consumed samples:      1003520 | elapsed time per iteration (ms): 39260.4 | learning rate: 9.790E-05 | global batch size:  1024 | lm loss: 1.950925E+00 | loss scale: 1.0 | grad norm: 0.474 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      981/   25000 | consumed samples:      1004544 | elapsed time per iteration (ms): 39423.0 | learning rate: 9.800E-05 | global batch size:  1024 | lm loss: 1.941665E+00 | loss scale: 1.0 | grad norm: 0.367 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      982/   25000 | consumed samples:      1005568 | elapsed time per iteration (ms): 39407.7 | learning rate: 9.810E-05 | global batch size:  1024 | lm loss: 1.937237E+00 | loss scale: 1.0 | grad norm: 0.353 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      983/   25000 | consumed samples:      1006592 | elapsed time per iteration (ms): 39255.0 | learning rate: 9.820E-05 | global batch size:  1024 | lm loss: 1.961477E+00 | loss scale: 1.0 | grad norm: 0.393 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      984/   25000 | consumed samples:      1007616 | elapsed time per iteration (ms): 39194.8 | learning rate: 9.830E-05 | global batch size:  1024 | lm loss: 1.967701E+00 | loss scale: 1.0 | grad norm: 0.443 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      985/   25000 | consumed samples:      1008640 | elapsed time per iteration (ms): 39319.2 | learning rate: 9.840E-05 | global batch size:  1024 | lm loss: 1.959759E+00 | loss scale: 1.0 | grad norm: 0.493 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      986/   25000 | consumed samples:      1009664 | elapsed time per iteration (ms): 39797.5 | learning rate: 9.850E-05 | global batch size:  1024 | lm loss: 1.949101E+00 | loss scale: 1.0 | grad norm: 0.601 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      987/   25000 | consumed samples:      1010688 | elapsed time per iteration (ms): 39183.4 | learning rate: 9.860E-05 | global batch size:  1024 | lm loss: 1.956223E+00 | loss scale: 1.0 | grad norm: 0.710 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      988/   25000 | consumed samples:      1011712 | elapsed time per iteration (ms): 39172.1 | learning rate: 9.870E-05 | global batch size:  1024 | lm loss: 1.953097E+00 | loss scale: 1.0 | grad norm: 0.757 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      989/   25000 | consumed samples:      1012736 | elapsed time per iteration (ms): 39112.2 | learning rate: 9.880E-05 | global batch size:  1024 | lm loss: 1.947630E+00 | loss scale: 1.0 | grad norm: 0.727 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      990/   25000 | consumed samples:      1013760 | elapsed time per iteration (ms): 39184.5 | learning rate: 9.890E-05 | global batch size:  1024 | lm loss: 1.950986E+00 | loss scale: 1.0 | grad norm: 0.578 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      991/   25000 | consumed samples:      1014784 | elapsed time per iteration (ms): 39584.6 | learning rate: 9.900E-05 | global batch size:  1024 | lm loss: 1.953029E+00 | loss scale: 1.0 | grad norm: 0.497 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      992/   25000 | consumed samples:      1015808 | elapsed time per iteration (ms): 39217.3 | learning rate: 9.910E-05 | global batch size:  1024 | lm loss: 1.956761E+00 | loss scale: 1.0 | grad norm: 0.576 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      993/   25000 | consumed samples:      1016832 | elapsed time per iteration (ms): 39281.9 | learning rate: 9.920E-05 | global batch size:  1024 | lm loss: 1.954698E+00 | loss scale: 1.0 | grad norm: 0.606 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      994/   25000 | consumed samples:      1017856 | elapsed time per iteration (ms): 39509.8 | learning rate: 9.930E-05 | global batch size:  1024 | lm loss: 1.953691E+00 | loss scale: 1.0 | grad norm: 0.569 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      995/   25000 | consumed samples:      1018880 | elapsed time per iteration (ms): 39167.7 | learning rate: 9.940E-05 | global batch size:  1024 | lm loss: 1.939010E+00 | loss scale: 1.0 | grad norm: 0.539 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      996/   25000 | consumed samples:      1019904 | elapsed time per iteration (ms): 39407.2 | learning rate: 9.950E-05 | global batch size:  1024 | lm loss: 1.958278E+00 | loss scale: 1.0 | grad norm: 0.522 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      997/   25000 | consumed samples:      1020928 | elapsed time per iteration (ms): 39233.5 | learning rate: 9.960E-05 | global batch size:  1024 | lm loss: 1.964812E+00 | loss scale: 1.0 | grad norm: 0.400 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      998/   25000 | consumed samples:      1021952 | elapsed time per iteration (ms): 39229.6 | learning rate: 9.970E-05 | global batch size:  1024 | lm loss: 1.927885E+00 | loss scale: 1.0 | grad norm: 0.384 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      999/   25000 | consumed samples:      1022976 | elapsed time per iteration (ms): 39260.8 | learning rate: 9.980E-05 | global batch size:  1024 | lm loss: 1.945509E+00 | loss scale: 1.0 | grad norm: 0.432 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1000/   25000 | consumed samples:      1024000 | elapsed time per iteration (ms): 39396.8 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.944300E+00 | loss scale: 1.0 | grad norm: 0.504 | number of skipped iterations:   0 | number of nan iterations:   0 |
------------------------------------------------------------------------------------------------
 validation loss at iteration 1000 | lm loss value: 1.948300E+00 | lm loss PPL: 7.016749E+00 | 
------------------------------------------------------------------------------------------------
saving checkpoint at iteration    1000 to /bb/llm/gaf51275/llama/checkpoints/Llama-2-7b-base-extended/okazaki_lab_cc-en-updated/tp2-pp2
  successfully saved checkpoint at iteration    1000 to /bb/llm/gaf51275/llama/checkpoints/Llama-2-7b-base-extended/okazaki_lab_cc-en-updated/tp2-pp2
(min, max) time across ranks (ms):
    save-checkpoint ................................: (45205.31, 45206.99)
 iteration     1001/   25000 | consumed samples:      1025024 | elapsed time per iteration (ms): 219674.4 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.946432E+00 | loss scale: 1.0 | grad norm: 0.482 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1002/   25000 | consumed samples:      1026048 | elapsed time per iteration (ms): 39614.7 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.951789E+00 | loss scale: 1.0 | grad norm: 0.410 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1003/   25000 | consumed samples:      1027072 | elapsed time per iteration (ms): 39237.9 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.946007E+00 | loss scale: 1.0 | grad norm: 0.475 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1004/   25000 | consumed samples:      1028096 | elapsed time per iteration (ms): 39177.4 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.945323E+00 | loss scale: 1.0 | grad norm: 0.641 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1005/   25000 | consumed samples:      1029120 | elapsed time per iteration (ms): 39169.9 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.965470E+00 | loss scale: 1.0 | grad norm: 0.895 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1006/   25000 | consumed samples:      1030144 | elapsed time per iteration (ms): 39399.0 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.944752E+00 | loss scale: 1.0 | grad norm: 0.924 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1007/   25000 | consumed samples:      1031168 | elapsed time per iteration (ms): 39374.1 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.934646E+00 | loss scale: 1.0 | grad norm: 0.731 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1008/   25000 | consumed samples:      1032192 | elapsed time per iteration (ms): 39261.2 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.949211E+00 | loss scale: 1.0 | grad norm: 0.826 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1009/   25000 | consumed samples:      1033216 | elapsed time per iteration (ms): 39498.8 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.957693E+00 | loss scale: 1.0 | grad norm: 0.559 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1010/   25000 | consumed samples:      1034240 | elapsed time per iteration (ms): 39475.4 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.940328E+00 | loss scale: 1.0 | grad norm: 0.612 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1011/   25000 | consumed samples:      1035264 | elapsed time per iteration (ms): 39322.0 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.953803E+00 | loss scale: 1.0 | grad norm: 0.625 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1012/   25000 | consumed samples:      1036288 | elapsed time per iteration (ms): 39465.4 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.958919E+00 | loss scale: 1.0 | grad norm: 0.443 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1013/   25000 | consumed samples:      1037312 | elapsed time per iteration (ms): 39189.4 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.942486E+00 | loss scale: 1.0 | grad norm: 0.559 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1014/   25000 | consumed samples:      1038336 | elapsed time per iteration (ms): 39261.8 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.939444E+00 | loss scale: 1.0 | grad norm: 0.466 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1015/   25000 | consumed samples:      1039360 | elapsed time per iteration (ms): 39198.8 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.931038E+00 | loss scale: 1.0 | grad norm: 0.520 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1016/   25000 | consumed samples:      1040384 | elapsed time per iteration (ms): 39411.3 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.923832E+00 | loss scale: 1.0 | grad norm: 0.454 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1017/   25000 | consumed samples:      1041408 | elapsed time per iteration (ms): 39394.8 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.947266E+00 | loss scale: 1.0 | grad norm: 0.435 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1018/   25000 | consumed samples:      1042432 | elapsed time per iteration (ms): 39758.5 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.937192E+00 | loss scale: 1.0 | grad norm: 0.450 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1019/   25000 | consumed samples:      1043456 | elapsed time per iteration (ms): 39195.0 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.955626E+00 | loss scale: 1.0 | grad norm: 0.444 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1020/   25000 | consumed samples:      1044480 | elapsed time per iteration (ms): 39178.9 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.948082E+00 | loss scale: 1.0 | grad norm: 0.428 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1021/   25000 | consumed samples:      1045504 | elapsed time per iteration (ms): 39285.7 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.937803E+00 | loss scale: 1.0 | grad norm: 0.492 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1022/   25000 | consumed samples:      1046528 | elapsed time per iteration (ms): 39305.4 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.953170E+00 | loss scale: 1.0 | grad norm: 0.527 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1023/   25000 | consumed samples:      1047552 | elapsed time per iteration (ms): 39356.1 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.952089E+00 | loss scale: 1.0 | grad norm: 0.517 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1024/   25000 | consumed samples:      1048576 | elapsed time per iteration (ms): 39248.4 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.944534E+00 | loss scale: 1.0 | grad norm: 0.542 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1025/   25000 | consumed samples:      1049600 | elapsed time per iteration (ms): 39320.9 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.959634E+00 | loss scale: 1.0 | grad norm: 0.528 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1026/   25000 | consumed samples:      1050624 | elapsed time per iteration (ms): 39589.0 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.927733E+00 | loss scale: 1.0 | grad norm: 0.461 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1027/   25000 | consumed samples:      1051648 | elapsed time per iteration (ms): 39324.6 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.933657E+00 | loss scale: 1.0 | grad norm: 0.413 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1028/   25000 | consumed samples:      1052672 | elapsed time per iteration (ms): 39381.8 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.925290E+00 | loss scale: 1.0 | grad norm: 0.414 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1029/   25000 | consumed samples:      1053696 | elapsed time per iteration (ms): 39217.6 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.940390E+00 | loss scale: 1.0 | grad norm: 0.415 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1030/   25000 | consumed samples:      1054720 | elapsed time per iteration (ms): 39430.0 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.936135E+00 | loss scale: 1.0 | grad norm: 0.446 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1031/   25000 | consumed samples:      1055744 | elapsed time per iteration (ms): 39305.1 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.946798E+00 | loss scale: 1.0 | grad norm: 0.408 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1032/   25000 | consumed samples:      1056768 | elapsed time per iteration (ms): 39308.0 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.944201E+00 | loss scale: 1.0 | grad norm: 0.529 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1033/   25000 | consumed samples:      1057792 | elapsed time per iteration (ms): 39430.4 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.939403E+00 | loss scale: 1.0 | grad norm: 0.684 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1034/   25000 | consumed samples:      1058816 | elapsed time per iteration (ms): 39643.1 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.938716E+00 | loss scale: 1.0 | grad norm: 0.687 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1035/   25000 | consumed samples:      1059840 | elapsed time per iteration (ms): 39350.8 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.942551E+00 | loss scale: 1.0 | grad norm: 0.702 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1036/   25000 | consumed samples:      1060864 | elapsed time per iteration (ms): 39377.3 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.921707E+00 | loss scale: 1.0 | grad norm: 0.619 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1037/   25000 | consumed samples:      1061888 | elapsed time per iteration (ms): 39169.0 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.943691E+00 | loss scale: 1.0 | grad norm: 0.402 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1038/   25000 | consumed samples:      1062912 | elapsed time per iteration (ms): 39305.7 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.944344E+00 | loss scale: 1.0 | grad norm: 0.415 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1039/   25000 | consumed samples:      1063936 | elapsed time per iteration (ms): 39311.8 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.928196E+00 | loss scale: 1.0 | grad norm: 0.578 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1040/   25000 | consumed samples:      1064960 | elapsed time per iteration (ms): 39394.3 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.935838E+00 | loss scale: 1.0 | grad norm: 0.533 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1041/   25000 | consumed samples:      1065984 | elapsed time per iteration (ms): 39255.1 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.942695E+00 | loss scale: 1.0 | grad norm: 0.455 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1042/   25000 | consumed samples:      1067008 | elapsed time per iteration (ms): 39416.2 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.942665E+00 | loss scale: 1.0 | grad norm: 0.618 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1043/   25000 | consumed samples:      1068032 | elapsed time per iteration (ms): 39271.1 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.959384E+00 | loss scale: 1.0 | grad norm: 0.744 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1044/   25000 | consumed samples:      1069056 | elapsed time per iteration (ms): 39377.7 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.932476E+00 | loss scale: 1.0 | grad norm: 0.636 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1045/   25000 | consumed samples:      1070080 | elapsed time per iteration (ms): 39315.3 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.948926E+00 | loss scale: 1.0 | grad norm: 0.519 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1046/   25000 | consumed samples:      1071104 | elapsed time per iteration (ms): 39427.2 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.957268E+00 | loss scale: 1.0 | grad norm: 0.622 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1047/   25000 | consumed samples:      1072128 | elapsed time per iteration (ms): 39198.1 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.936069E+00 | loss scale: 1.0 | grad norm: 0.581 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1048/   25000 | consumed samples:      1073152 | elapsed time per iteration (ms): 39280.7 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.931787E+00 | loss scale: 1.0 | grad norm: 0.421 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1049/   25000 | consumed samples:      1074176 | elapsed time per iteration (ms): 39392.2 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.948599E+00 | loss scale: 1.0 | grad norm: 0.482 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1050/   25000 | consumed samples:      1075200 | elapsed time per iteration (ms): 39647.4 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.946200E+00 | loss scale: 1.0 | grad norm: 0.507 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1051/   25000 | consumed samples:      1076224 | elapsed time per iteration (ms): 39279.3 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.947766E+00 | loss scale: 1.0 | grad norm: 0.456 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1052/   25000 | consumed samples:      1077248 | elapsed time per iteration (ms): 39187.8 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.936136E+00 | loss scale: 1.0 | grad norm: 0.484 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1053/   25000 | consumed samples:      1078272 | elapsed time per iteration (ms): 39169.1 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.939560E+00 | loss scale: 1.0 | grad norm: 0.493 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1054/   25000 | consumed samples:      1079296 | elapsed time per iteration (ms): 39324.1 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.934693E+00 | loss scale: 1.0 | grad norm: 0.470 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1055/   25000 | consumed samples:      1080320 | elapsed time per iteration (ms): 39602.8 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.941588E+00 | loss scale: 1.0 | grad norm: 0.455 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1056/   25000 | consumed samples:      1081344 | elapsed time per iteration (ms): 39257.2 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.930759E+00 | loss scale: 1.0 | grad norm: 0.390 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1057/   25000 | consumed samples:      1082368 | elapsed time per iteration (ms): 39326.4 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.934132E+00 | loss scale: 1.0 | grad norm: 0.412 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1058/   25000 | consumed samples:      1083392 | elapsed time per iteration (ms): 39486.6 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.919535E+00 | loss scale: 1.0 | grad norm: 0.425 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1059/   25000 | consumed samples:      1084416 | elapsed time per iteration (ms): 39297.9 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.939060E+00 | loss scale: 1.0 | grad norm: 0.423 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1060/   25000 | consumed samples:      1085440 | elapsed time per iteration (ms): 39536.9 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.933806E+00 | loss scale: 1.0 | grad norm: 0.524 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1061/   25000 | consumed samples:      1086464 | elapsed time per iteration (ms): 39204.1 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.929210E+00 | loss scale: 1.0 | grad norm: 0.682 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1062/   25000 | consumed samples:      1087488 | elapsed time per iteration (ms): 39255.2 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.942791E+00 | loss scale: 1.0 | grad norm: 0.802 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1063/   25000 | consumed samples:      1088512 | elapsed time per iteration (ms): 39181.7 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.947450E+00 | loss scale: 1.0 | grad norm: 0.646 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1064/   25000 | consumed samples:      1089536 | elapsed time per iteration (ms): 39414.0 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.944147E+00 | loss scale: 1.0 | grad norm: 0.589 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1065/   25000 | consumed samples:      1090560 | elapsed time per iteration (ms): 39646.9 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.921419E+00 | loss scale: 1.0 | grad norm: 0.601 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1066/   25000 | consumed samples:      1091584 | elapsed time per iteration (ms): 39267.5 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.931196E+00 | loss scale: 1.0 | grad norm: 0.432 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1067/   25000 | consumed samples:      1092608 | elapsed time per iteration (ms): 39523.1 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.939415E+00 | loss scale: 1.0 | grad norm: 0.477 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1068/   25000 | consumed samples:      1093632 | elapsed time per iteration (ms): 39206.5 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.920588E+00 | loss scale: 1.0 | grad norm: 0.442 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1069/   25000 | consumed samples:      1094656 | elapsed time per iteration (ms): 39203.8 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.938776E+00 | loss scale: 1.0 | grad norm: 0.406 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1070/   25000 | consumed samples:      1095680 | elapsed time per iteration (ms): 39455.7 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.941667E+00 | loss scale: 1.0 | grad norm: 0.387 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1071/   25000 | consumed samples:      1096704 | elapsed time per iteration (ms): 39538.5 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.950204E+00 | loss scale: 1.0 | grad norm: 0.335 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1072/   25000 | consumed samples:      1097728 | elapsed time per iteration (ms): 39385.8 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.929505E+00 | loss scale: 1.0 | grad norm: 0.347 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1073/   25000 | consumed samples:      1098752 | elapsed time per iteration (ms): 39357.4 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.930446E+00 | loss scale: 1.0 | grad norm: 0.337 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1074/   25000 | consumed samples:      1099776 | elapsed time per iteration (ms): 39361.4 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.931458E+00 | loss scale: 1.0 | grad norm: 0.338 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1075/   25000 | consumed samples:      1100800 | elapsed time per iteration (ms): 39620.9 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.937064E+00 | loss scale: 1.0 | grad norm: 0.348 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1076/   25000 | consumed samples:      1101824 | elapsed time per iteration (ms): 39466.9 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.915433E+00 | loss scale: 1.0 | grad norm: 0.303 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1077/   25000 | consumed samples:      1102848 | elapsed time per iteration (ms): 39208.0 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.939368E+00 | loss scale: 1.0 | grad norm: 0.296 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1078/   25000 | consumed samples:      1103872 | elapsed time per iteration (ms): 39204.7 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.928985E+00 | loss scale: 1.0 | grad norm: 0.322 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1079/   25000 | consumed samples:      1104896 | elapsed time per iteration (ms): 39363.4 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.947664E+00 | loss scale: 1.0 | grad norm: 0.394 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1080/   25000 | consumed samples:      1105920 | elapsed time per iteration (ms): 39280.6 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.938539E+00 | loss scale: 1.0 | grad norm: 0.481 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1081/   25000 | consumed samples:      1106944 | elapsed time per iteration (ms): 39397.9 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.945817E+00 | loss scale: 1.0 | grad norm: 0.576 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1082/   25000 | consumed samples:      1107968 | elapsed time per iteration (ms): 39529.1 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.953769E+00 | loss scale: 1.0 | grad norm: 0.617 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1083/   25000 | consumed samples:      1108992 | elapsed time per iteration (ms): 39448.5 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.934991E+00 | loss scale: 1.0 | grad norm: 0.639 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1084/   25000 | consumed samples:      1110016 | elapsed time per iteration (ms): 39362.9 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.911889E+00 | loss scale: 1.0 | grad norm: 0.743 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1085/   25000 | consumed samples:      1111040 | elapsed time per iteration (ms): 39299.9 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.924370E+00 | loss scale: 1.0 | grad norm: 0.652 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1086/   25000 | consumed samples:      1112064 | elapsed time per iteration (ms): 39281.8 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.935391E+00 | loss scale: 1.0 | grad norm: 0.549 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1087/   25000 | consumed samples:      1113088 | elapsed time per iteration (ms): 39438.9 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.936069E+00 | loss scale: 1.0 | grad norm: 0.613 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1088/   25000 | consumed samples:      1114112 | elapsed time per iteration (ms): 39318.4 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.914828E+00 | loss scale: 1.0 | grad norm: 0.483 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1089/   25000 | consumed samples:      1115136 | elapsed time per iteration (ms): 39342.0 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.938330E+00 | loss scale: 1.0 | grad norm: 0.425 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1090/   25000 | consumed samples:      1116160 | elapsed time per iteration (ms): 39381.0 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.926825E+00 | loss scale: 1.0 | grad norm: 0.338 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1091/   25000 | consumed samples:      1117184 | elapsed time per iteration (ms): 39672.1 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.935180E+00 | loss scale: 1.0 | grad norm: 0.375 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1092/   25000 | consumed samples:      1118208 | elapsed time per iteration (ms): 39434.6 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.928570E+00 | loss scale: 1.0 | grad norm: 0.388 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1093/   25000 | consumed samples:      1119232 | elapsed time per iteration (ms): 39386.5 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.922580E+00 | loss scale: 1.0 | grad norm: 0.382 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1094/   25000 | consumed samples:      1120256 | elapsed time per iteration (ms): 39354.6 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.928382E+00 | loss scale: 1.0 | grad norm: 0.351 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1095/   25000 | consumed samples:      1121280 | elapsed time per iteration (ms): 39293.3 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.924914E+00 | loss scale: 1.0 | grad norm: 0.370 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1096/   25000 | consumed samples:      1122304 | elapsed time per iteration (ms): 39305.4 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.924202E+00 | loss scale: 1.0 | grad norm: 0.447 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1097/   25000 | consumed samples:      1123328 | elapsed time per iteration (ms): 39422.2 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.921831E+00 | loss scale: 1.0 | grad norm: 0.493 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1098/   25000 | consumed samples:      1124352 | elapsed time per iteration (ms): 39373.6 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.942519E+00 | loss scale: 1.0 | grad norm: 0.539 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1099/   25000 | consumed samples:      1125376 | elapsed time per iteration (ms): 39625.1 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.929290E+00 | loss scale: 1.0 | grad norm: 0.538 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1100/   25000 | consumed samples:      1126400 | elapsed time per iteration (ms): 39356.7 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.921095E+00 | loss scale: 1.0 | grad norm: 0.529 | number of skipped iterations:   0 | number of nan iterations:   0 |
------------------------------------------------------------------------------------------------
 validation loss at iteration 1100 | lm loss value: 1.933466E+00 | lm loss PPL: 6.913433E+00 | 
------------------------------------------------------------------------------------------------
 iteration     1101/   25000 | consumed samples:      1127424 | elapsed time per iteration (ms): 173998.5 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.902330E+00 | loss scale: 1.0 | grad norm: 0.468 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1102/   25000 | consumed samples:      1128448 | elapsed time per iteration (ms): 39264.3 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.909228E+00 | loss scale: 1.0 | grad norm: 0.412 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1103/   25000 | consumed samples:      1129472 | elapsed time per iteration (ms): 39408.9 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.911931E+00 | loss scale: 1.0 | grad norm: 0.447 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1104/   25000 | consumed samples:      1130496 | elapsed time per iteration (ms): 39477.4 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.940160E+00 | loss scale: 1.0 | grad norm: 0.502 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1105/   25000 | consumed samples:      1131520 | elapsed time per iteration (ms): 39218.4 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.920686E+00 | loss scale: 1.0 | grad norm: 0.473 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1106/   25000 | consumed samples:      1132544 | elapsed time per iteration (ms): 39369.5 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.925877E+00 | loss scale: 1.0 | grad norm: 0.426 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1107/   25000 | consumed samples:      1133568 | elapsed time per iteration (ms): 39568.0 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.928670E+00 | loss scale: 1.0 | grad norm: 0.477 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1108/   25000 | consumed samples:      1134592 | elapsed time per iteration (ms): 39340.3 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.918835E+00 | loss scale: 1.0 | grad norm: 0.528 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1109/   25000 | consumed samples:      1135616 | elapsed time per iteration (ms): 39497.1 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.924832E+00 | loss scale: 1.0 | grad norm: 0.568 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1110/   25000 | consumed samples:      1136640 | elapsed time per iteration (ms): 39215.4 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.912326E+00 | loss scale: 1.0 | grad norm: 0.578 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1111/   25000 | consumed samples:      1137664 | elapsed time per iteration (ms): 39284.4 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.933876E+00 | loss scale: 1.0 | grad norm: 0.558 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1112/   25000 | consumed samples:      1138688 | elapsed time per iteration (ms): 39294.4 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.905921E+00 | loss scale: 1.0 | grad norm: 0.449 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1113/   25000 | consumed samples:      1139712 | elapsed time per iteration (ms): 39486.5 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.912096E+00 | loss scale: 1.0 | grad norm: 0.452 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1114/   25000 | consumed samples:      1140736 | elapsed time per iteration (ms): 39406.1 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.909796E+00 | loss scale: 1.0 | grad norm: 0.506 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1115/   25000 | consumed samples:      1141760 | elapsed time per iteration (ms): 39494.7 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.900826E+00 | loss scale: 1.0 | grad norm: 0.441 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1116/   25000 | consumed samples:      1142784 | elapsed time per iteration (ms): 39226.6 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.937033E+00 | loss scale: 1.0 | grad norm: 0.380 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1117/   25000 | consumed samples:      1143808 | elapsed time per iteration (ms): 39225.9 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.914467E+00 | loss scale: 1.0 | grad norm: 0.411 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1118/   25000 | consumed samples:      1144832 | elapsed time per iteration (ms): 39490.9 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.928082E+00 | loss scale: 1.0 | grad norm: 0.417 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1119/   25000 | consumed samples:      1145856 | elapsed time per iteration (ms): 39564.9 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.921095E+00 | loss scale: 1.0 | grad norm: 0.362 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1120/   25000 | consumed samples:      1146880 | elapsed time per iteration (ms): 39307.7 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.908874E+00 | loss scale: 1.0 | grad norm: 0.339 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1121/   25000 | consumed samples:      1147904 | elapsed time per iteration (ms): 39229.3 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.933542E+00 | loss scale: 1.0 | grad norm: 0.333 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1122/   25000 | consumed samples:      1148928 | elapsed time per iteration (ms): 39448.3 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.918660E+00 | loss scale: 1.0 | grad norm: 0.434 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1123/   25000 | consumed samples:      1149952 | elapsed time per iteration (ms): 39587.9 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.927553E+00 | loss scale: 1.0 | grad norm: 0.473 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1124/   25000 | consumed samples:      1150976 | elapsed time per iteration (ms): 39474.8 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.921087E+00 | loss scale: 1.0 | grad norm: 0.576 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1125/   25000 | consumed samples:      1152000 | elapsed time per iteration (ms): 39221.8 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.915881E+00 | loss scale: 1.0 | grad norm: 0.675 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1126/   25000 | consumed samples:      1153024 | elapsed time per iteration (ms): 39244.1 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.919247E+00 | loss scale: 1.0 | grad norm: 0.655 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1127/   25000 | consumed samples:      1154048 | elapsed time per iteration (ms): 39468.4 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.924397E+00 | loss scale: 1.0 | grad norm: 0.480 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1128/   25000 | consumed samples:      1155072 | elapsed time per iteration (ms): 39389.7 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.930372E+00 | loss scale: 1.0 | grad norm: 0.471 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1129/   25000 | consumed samples:      1156096 | elapsed time per iteration (ms): 39489.6 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.923247E+00 | loss scale: 1.0 | grad norm: 0.500 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1130/   25000 | consumed samples:      1157120 | elapsed time per iteration (ms): 39386.5 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.920806E+00 | loss scale: 1.0 | grad norm: 0.429 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1131/   25000 | consumed samples:      1158144 | elapsed time per iteration (ms): 39598.1 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.924312E+00 | loss scale: 1.0 | grad norm: 0.414 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1132/   25000 | consumed samples:      1159168 | elapsed time per iteration (ms): 39249.9 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.922572E+00 | loss scale: 1.0 | grad norm: 0.432 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1133/   25000 | consumed samples:      1160192 | elapsed time per iteration (ms): 39346.5 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.910364E+00 | loss scale: 1.0 | grad norm: 0.379 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1134/   25000 | consumed samples:      1161216 | elapsed time per iteration (ms): 39390.1 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.918225E+00 | loss scale: 1.0 | grad norm: 0.356 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1135/   25000 | consumed samples:      1162240 | elapsed time per iteration (ms): 39282.4 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.915289E+00 | loss scale: 1.0 | grad norm: 0.347 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1136/   25000 | consumed samples:      1163264 | elapsed time per iteration (ms): 39364.7 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.915522E+00 | loss scale: 1.0 | grad norm: 0.330 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1137/   25000 | consumed samples:      1164288 | elapsed time per iteration (ms): 39277.9 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.928203E+00 | loss scale: 1.0 | grad norm: 0.371 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1138/   25000 | consumed samples:      1165312 | elapsed time per iteration (ms): 39480.4 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.907205E+00 | loss scale: 1.0 | grad norm: 0.441 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1139/   25000 | consumed samples:      1166336 | elapsed time per iteration (ms): 39579.7 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.922644E+00 | loss scale: 1.0 | grad norm: 0.482 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1140/   25000 | consumed samples:      1167360 | elapsed time per iteration (ms): 39410.6 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.920569E+00 | loss scale: 1.0 | grad norm: 0.482 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1141/   25000 | consumed samples:      1168384 | elapsed time per iteration (ms): 39221.5 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.913797E+00 | loss scale: 1.0 | grad norm: 0.453 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1142/   25000 | consumed samples:      1169408 | elapsed time per iteration (ms): 39273.2 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.913805E+00 | loss scale: 1.0 | grad norm: 0.405 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1143/   25000 | consumed samples:      1170432 | elapsed time per iteration (ms): 39424.6 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.913939E+00 | loss scale: 1.0 | grad norm: 0.430 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1144/   25000 | consumed samples:      1171456 | elapsed time per iteration (ms): 39211.2 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.928578E+00 | loss scale: 1.0 | grad norm: 0.424 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1145/   25000 | consumed samples:      1172480 | elapsed time per iteration (ms): 39467.5 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.909700E+00 | loss scale: 1.0 | grad norm: 0.428 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1146/   25000 | consumed samples:      1173504 | elapsed time per iteration (ms): 39520.5 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.920448E+00 | loss scale: 1.0 | grad norm: 0.495 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1147/   25000 | consumed samples:      1174528 | elapsed time per iteration (ms): 39437.4 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.935002E+00 | loss scale: 1.0 | grad norm: 0.577 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1148/   25000 | consumed samples:      1175552 | elapsed time per iteration (ms): 39417.0 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.915095E+00 | loss scale: 1.0 | grad norm: 0.553 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1149/   25000 | consumed samples:      1176576 | elapsed time per iteration (ms): 39272.7 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.912322E+00 | loss scale: 1.0 | grad norm: 0.478 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1150/   25000 | consumed samples:      1177600 | elapsed time per iteration (ms): 39398.8 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.908991E+00 | loss scale: 1.0 | grad norm: 0.452 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1151/   25000 | consumed samples:      1178624 | elapsed time per iteration (ms): 39299.8 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.921530E+00 | loss scale: 1.0 | grad norm: 0.429 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1152/   25000 | consumed samples:      1179648 | elapsed time per iteration (ms): 39231.1 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.912961E+00 | loss scale: 1.0 | grad norm: 0.463 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1153/   25000 | consumed samples:      1180672 | elapsed time per iteration (ms): 39433.9 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.913386E+00 | loss scale: 1.0 | grad norm: 0.394 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1154/   25000 | consumed samples:      1181696 | elapsed time per iteration (ms): 39380.3 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.922477E+00 | loss scale: 1.0 | grad norm: 0.361 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1155/   25000 | consumed samples:      1182720 | elapsed time per iteration (ms): 39658.7 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.916598E+00 | loss scale: 1.0 | grad norm: 0.410 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1156/   25000 | consumed samples:      1183744 | elapsed time per iteration (ms): 39430.8 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.909715E+00 | loss scale: 1.0 | grad norm: 0.460 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1157/   25000 | consumed samples:      1184768 | elapsed time per iteration (ms): 39213.6 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.926596E+00 | loss scale: 1.0 | grad norm: 0.526 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1158/   25000 | consumed samples:      1185792 | elapsed time per iteration (ms): 39345.1 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.918422E+00 | loss scale: 1.0 | grad norm: 0.568 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1159/   25000 | consumed samples:      1186816 | elapsed time per iteration (ms): 39376.7 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.908806E+00 | loss scale: 1.0 | grad norm: 0.535 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1160/   25000 | consumed samples:      1187840 | elapsed time per iteration (ms): 39292.7 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.929371E+00 | loss scale: 1.0 | grad norm: 0.442 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1161/   25000 | consumed samples:      1188864 | elapsed time per iteration (ms): 39428.5 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.919050E+00 | loss scale: 1.0 | grad norm: 0.385 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1162/   25000 | consumed samples:      1189888 | elapsed time per iteration (ms): 39364.4 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.921027E+00 | loss scale: 1.0 | grad norm: 0.400 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1163/   25000 | consumed samples:      1190912 | elapsed time per iteration (ms): 39522.4 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.908517E+00 | loss scale: 1.0 | grad norm: 0.436 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1164/   25000 | consumed samples:      1191936 | elapsed time per iteration (ms): 39530.5 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.913581E+00 | loss scale: 1.0 | grad norm: 0.458 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1165/   25000 | consumed samples:      1192960 | elapsed time per iteration (ms): 39218.8 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.900706E+00 | loss scale: 1.0 | grad norm: 0.406 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1166/   25000 | consumed samples:      1193984 | elapsed time per iteration (ms): 39420.3 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.905169E+00 | loss scale: 1.0 | grad norm: 0.382 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1167/   25000 | consumed samples:      1195008 | elapsed time per iteration (ms): 39311.2 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.918923E+00 | loss scale: 1.0 | grad norm: 0.405 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1168/   25000 | consumed samples:      1196032 | elapsed time per iteration (ms): 39428.9 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.906283E+00 | loss scale: 1.0 | grad norm: 0.429 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1169/   25000 | consumed samples:      1197056 | elapsed time per iteration (ms): 39314.7 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.922030E+00 | loss scale: 1.0 | grad norm: 0.459 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1170/   25000 | consumed samples:      1198080 | elapsed time per iteration (ms): 39363.8 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.922143E+00 | loss scale: 1.0 | grad norm: 0.564 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1171/   25000 | consumed samples:      1199104 | elapsed time per iteration (ms): 39566.9 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.913448E+00 | loss scale: 1.0 | grad norm: 0.587 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1172/   25000 | consumed samples:      1200128 | elapsed time per iteration (ms): 39437.2 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.916916E+00 | loss scale: 1.0 | grad norm: 0.526 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1173/   25000 | consumed samples:      1201152 | elapsed time per iteration (ms): 39479.4 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.898472E+00 | loss scale: 1.0 | grad norm: 0.503 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1174/   25000 | consumed samples:      1202176 | elapsed time per iteration (ms): 39195.9 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.931310E+00 | loss scale: 1.0 | grad norm: 0.530 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1175/   25000 | consumed samples:      1203200 | elapsed time per iteration (ms): 39276.8 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.921238E+00 | loss scale: 1.0 | grad norm: 0.506 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1176/   25000 | consumed samples:      1204224 | elapsed time per iteration (ms): 39262.7 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.908584E+00 | loss scale: 1.0 | grad norm: 0.453 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1177/   25000 | consumed samples:      1205248 | elapsed time per iteration (ms): 39479.7 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.894979E+00 | loss scale: 1.0 | grad norm: 0.396 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1178/   25000 | consumed samples:      1206272 | elapsed time per iteration (ms): 39481.4 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.923935E+00 | loss scale: 1.0 | grad norm: 0.394 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1179/   25000 | consumed samples:      1207296 | elapsed time per iteration (ms): 39217.1 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.915321E+00 | loss scale: 1.0 | grad norm: 0.448 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1180/   25000 | consumed samples:      1208320 | elapsed time per iteration (ms): 39510.6 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.901501E+00 | loss scale: 1.0 | grad norm: 0.506 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1181/   25000 | consumed samples:      1209344 | elapsed time per iteration (ms): 39211.9 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.902390E+00 | loss scale: 1.0 | grad norm: 0.473 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1182/   25000 | consumed samples:      1210368 | elapsed time per iteration (ms): 39628.8 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.917853E+00 | loss scale: 1.0 | grad norm: 0.406 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1183/   25000 | consumed samples:      1211392 | elapsed time per iteration (ms): 39285.7 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.908396E+00 | loss scale: 1.0 | grad norm: 0.468 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1184/   25000 | consumed samples:      1212416 | elapsed time per iteration (ms): 39228.0 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.933766E+00 | loss scale: 1.0 | grad norm: 0.437 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1185/   25000 | consumed samples:      1213440 | elapsed time per iteration (ms): 39298.4 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.923179E+00 | loss scale: 1.0 | grad norm: 0.407 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1186/   25000 | consumed samples:      1214464 | elapsed time per iteration (ms): 39433.6 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.918405E+00 | loss scale: 1.0 | grad norm: 0.406 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1187/   25000 | consumed samples:      1215488 | elapsed time per iteration (ms): 39524.8 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.909862E+00 | loss scale: 1.0 | grad norm: 0.403 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1188/   25000 | consumed samples:      1216512 | elapsed time per iteration (ms): 39516.0 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.912861E+00 | loss scale: 1.0 | grad norm: 0.408 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1189/   25000 | consumed samples:      1217536 | elapsed time per iteration (ms): 39276.1 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.919226E+00 | loss scale: 1.0 | grad norm: 0.367 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1190/   25000 | consumed samples:      1218560 | elapsed time per iteration (ms): 39214.9 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.917974E+00 | loss scale: 1.0 | grad norm: 0.340 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1191/   25000 | consumed samples:      1219584 | elapsed time per iteration (ms): 39460.1 | learning rate: 9.999E-05 | global batch size:  1024 | lm loss: 1.901972E+00 | loss scale: 1.0 | grad norm: 0.364 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1192/   25000 | consumed samples:      1220608 | elapsed time per iteration (ms): 39419.7 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.912699E+00 | loss scale: 1.0 | grad norm: 0.385 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1193/   25000 | consumed samples:      1221632 | elapsed time per iteration (ms): 39425.5 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.897439E+00 | loss scale: 1.0 | grad norm: 0.373 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1194/   25000 | consumed samples:      1222656 | elapsed time per iteration (ms): 39292.7 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.895185E+00 | loss scale: 1.0 | grad norm: 0.372 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1195/   25000 | consumed samples:      1223680 | elapsed time per iteration (ms): 39364.1 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.912464E+00 | loss scale: 1.0 | grad norm: 0.383 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1196/   25000 | consumed samples:      1224704 | elapsed time per iteration (ms): 39502.4 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.900631E+00 | loss scale: 1.0 | grad norm: 0.337 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1197/   25000 | consumed samples:      1225728 | elapsed time per iteration (ms): 39347.3 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.926652E+00 | loss scale: 1.0 | grad norm: 0.369 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1198/   25000 | consumed samples:      1226752 | elapsed time per iteration (ms): 39409.8 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.906609E+00 | loss scale: 1.0 | grad norm: 0.433 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1199/   25000 | consumed samples:      1227776 | elapsed time per iteration (ms): 39296.7 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.893397E+00 | loss scale: 1.0 | grad norm: 0.487 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1200/   25000 | consumed samples:      1228800 | elapsed time per iteration (ms): 39377.7 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.913254E+00 | loss scale: 1.0 | grad norm: 0.501 | number of skipped iterations:   0 | number of nan iterations:   0 |
------------------------------------------------------------------------------------------------
 validation loss at iteration 1200 | lm loss value: 1.910998E+00 | lm loss PPL: 6.759830E+00 | 
------------------------------------------------------------------------------------------------
 iteration     1201/   25000 | consumed samples:      1229824 | elapsed time per iteration (ms): 174737.2 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.918046E+00 | loss scale: 1.0 | grad norm: 0.465 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1202/   25000 | consumed samples:      1230848 | elapsed time per iteration (ms): 39412.7 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.913658E+00 | loss scale: 1.0 | grad norm: 0.458 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1203/   25000 | consumed samples:      1231872 | elapsed time per iteration (ms): 40361.7 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.896962E+00 | loss scale: 1.0 | grad norm: 0.534 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1204/   25000 | consumed samples:      1232896 | elapsed time per iteration (ms): 39590.0 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.913889E+00 | loss scale: 1.0 | grad norm: 0.583 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1205/   25000 | consumed samples:      1233920 | elapsed time per iteration (ms): 39213.4 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.913026E+00 | loss scale: 1.0 | grad norm: 0.582 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1206/   25000 | consumed samples:      1234944 | elapsed time per iteration (ms): 39227.6 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.912412E+00 | loss scale: 1.0 | grad norm: 0.548 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1207/   25000 | consumed samples:      1235968 | elapsed time per iteration (ms): 39437.3 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.892256E+00 | loss scale: 1.0 | grad norm: 0.442 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1208/   25000 | consumed samples:      1236992 | elapsed time per iteration (ms): 39341.5 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.908067E+00 | loss scale: 1.0 | grad norm: 0.368 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1209/   25000 | consumed samples:      1238016 | elapsed time per iteration (ms): 39584.7 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.909915E+00 | loss scale: 1.0 | grad norm: 0.458 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1210/   25000 | consumed samples:      1239040 | elapsed time per iteration (ms): 39285.5 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.908177E+00 | loss scale: 1.0 | grad norm: 0.493 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1211/   25000 | consumed samples:      1240064 | elapsed time per iteration (ms): 39280.7 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.907380E+00 | loss scale: 1.0 | grad norm: 0.361 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1212/   25000 | consumed samples:      1241088 | elapsed time per iteration (ms): 39620.7 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.908712E+00 | loss scale: 1.0 | grad norm: 0.336 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1213/   25000 | consumed samples:      1242112 | elapsed time per iteration (ms): 39321.4 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.915773E+00 | loss scale: 1.0 | grad norm: 0.399 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1214/   25000 | consumed samples:      1243136 | elapsed time per iteration (ms): 39324.0 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.905201E+00 | loss scale: 1.0 | grad norm: 0.354 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1215/   25000 | consumed samples:      1244160 | elapsed time per iteration (ms): 39280.8 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.892134E+00 | loss scale: 1.0 | grad norm: 0.304 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1216/   25000 | consumed samples:      1245184 | elapsed time per iteration (ms): 39209.3 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.890527E+00 | loss scale: 1.0 | grad norm: 0.314 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1217/   25000 | consumed samples:      1246208 | elapsed time per iteration (ms): 39402.6 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.890026E+00 | loss scale: 1.0 | grad norm: 0.329 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1218/   25000 | consumed samples:      1247232 | elapsed time per iteration (ms): 39372.0 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.919927E+00 | loss scale: 1.0 | grad norm: 0.334 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1219/   25000 | consumed samples:      1248256 | elapsed time per iteration (ms): 39529.0 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.903126E+00 | loss scale: 1.0 | grad norm: 0.330 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1220/   25000 | consumed samples:      1249280 | elapsed time per iteration (ms): 39514.1 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.895852E+00 | loss scale: 1.0 | grad norm: 0.374 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1221/   25000 | consumed samples:      1250304 | elapsed time per iteration (ms): 39197.7 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.904806E+00 | loss scale: 1.0 | grad norm: 0.427 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1222/   25000 | consumed samples:      1251328 | elapsed time per iteration (ms): 39328.7 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.902280E+00 | loss scale: 1.0 | grad norm: 0.496 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1223/   25000 | consumed samples:      1252352 | elapsed time per iteration (ms): 39345.9 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.902346E+00 | loss scale: 1.0 | grad norm: 0.532 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1224/   25000 | consumed samples:      1253376 | elapsed time per iteration (ms): 39330.3 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.898691E+00 | loss scale: 1.0 | grad norm: 0.576 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1225/   25000 | consumed samples:      1254400 | elapsed time per iteration (ms): 39362.5 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.901216E+00 | loss scale: 1.0 | grad norm: 0.528 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1226/   25000 | consumed samples:      1255424 | elapsed time per iteration (ms): 39230.5 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.908582E+00 | loss scale: 1.0 | grad norm: 0.574 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1227/   25000 | consumed samples:      1256448 | elapsed time per iteration (ms): 39641.6 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.911341E+00 | loss scale: 1.0 | grad norm: 0.571 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1228/   25000 | consumed samples:      1257472 | elapsed time per iteration (ms): 39520.7 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.906550E+00 | loss scale: 1.0 | grad norm: 0.452 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1229/   25000 | consumed samples:      1258496 | elapsed time per iteration (ms): 39273.6 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.911787E+00 | loss scale: 1.0 | grad norm: 0.488 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1230/   25000 | consumed samples:      1259520 | elapsed time per iteration (ms): 39342.1 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.915335E+00 | loss scale: 1.0 | grad norm: 0.504 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1231/   25000 | consumed samples:      1260544 | elapsed time per iteration (ms): 39342.6 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.884343E+00 | loss scale: 1.0 | grad norm: 0.438 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1232/   25000 | consumed samples:      1261568 | elapsed time per iteration (ms): 39369.0 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.904260E+00 | loss scale: 1.0 | grad norm: 0.398 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1233/   25000 | consumed samples:      1262592 | elapsed time per iteration (ms): 39290.9 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.907390E+00 | loss scale: 1.0 | grad norm: 0.420 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1234/   25000 | consumed samples:      1263616 | elapsed time per iteration (ms): 39247.0 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.899678E+00 | loss scale: 1.0 | grad norm: 0.376 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1235/   25000 | consumed samples:      1264640 | elapsed time per iteration (ms): 39546.1 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.899205E+00 | loss scale: 1.0 | grad norm: 0.419 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1236/   25000 | consumed samples:      1265664 | elapsed time per iteration (ms): 39660.2 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.895437E+00 | loss scale: 1.0 | grad norm: 0.371 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1237/   25000 | consumed samples:      1266688 | elapsed time per iteration (ms): 39365.4 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.888644E+00 | loss scale: 1.0 | grad norm: 0.355 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1238/   25000 | consumed samples:      1267712 | elapsed time per iteration (ms): 39214.6 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.891746E+00 | loss scale: 1.0 | grad norm: 0.385 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1239/   25000 | consumed samples:      1268736 | elapsed time per iteration (ms): 39307.7 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.896383E+00 | loss scale: 1.0 | grad norm: 0.375 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1240/   25000 | consumed samples:      1269760 | elapsed time per iteration (ms): 39431.4 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.900694E+00 | loss scale: 1.0 | grad norm: 0.389 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1241/   25000 | consumed samples:      1270784 | elapsed time per iteration (ms): 39422.4 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.894437E+00 | loss scale: 1.0 | grad norm: 0.346 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1242/   25000 | consumed samples:      1271808 | elapsed time per iteration (ms): 39225.6 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.897942E+00 | loss scale: 1.0 | grad norm: 0.317 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1243/   25000 | consumed samples:      1272832 | elapsed time per iteration (ms): 39383.2 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.895849E+00 | loss scale: 1.0 | grad norm: 0.385 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1244/   25000 | consumed samples:      1273856 | elapsed time per iteration (ms): 39526.8 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.902078E+00 | loss scale: 1.0 | grad norm: 0.359 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1245/   25000 | consumed samples:      1274880 | elapsed time per iteration (ms): 39265.2 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.899879E+00 | loss scale: 1.0 | grad norm: 0.347 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1246/   25000 | consumed samples:      1275904 | elapsed time per iteration (ms): 39596.8 | learning rate: 9.998E-05 | global batch size:  1024 | lm loss: 1.890633E+00 | loss scale: 1.0 | grad norm: 0.403 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1247/   25000 | consumed samples:      1276928 | elapsed time per iteration (ms): 39258.9 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.901272E+00 | loss scale: 1.0 | grad norm: 0.438 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1248/   25000 | consumed samples:      1277952 | elapsed time per iteration (ms): 39185.0 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.891615E+00 | loss scale: 1.0 | grad norm: 0.485 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1249/   25000 | consumed samples:      1278976 | elapsed time per iteration (ms): 39247.7 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.872255E+00 | loss scale: 1.0 | grad norm: 0.487 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1250/   25000 | consumed samples:      1280000 | elapsed time per iteration (ms): 39257.0 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.913257E+00 | loss scale: 1.0 | grad norm: 0.519 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1251/   25000 | consumed samples:      1281024 | elapsed time per iteration (ms): 39664.9 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.891580E+00 | loss scale: 1.0 | grad norm: 0.482 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1252/   25000 | consumed samples:      1282048 | elapsed time per iteration (ms): 39344.4 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.910379E+00 | loss scale: 1.0 | grad norm: 0.444 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1253/   25000 | consumed samples:      1283072 | elapsed time per iteration (ms): 39307.7 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.884137E+00 | loss scale: 1.0 | grad norm: 0.387 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1254/   25000 | consumed samples:      1284096 | elapsed time per iteration (ms): 39177.6 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.917051E+00 | loss scale: 1.0 | grad norm: 0.335 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1255/   25000 | consumed samples:      1285120 | elapsed time per iteration (ms): 39408.4 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.887219E+00 | loss scale: 1.0 | grad norm: 0.360 | number of skipped iterations:   0 | number of nan iterations:   0 |
wandb: Network error (ReadTimeout), entering retry loop.
 iteration     1256/   25000 | consumed samples:      1286144 | elapsed time per iteration (ms): 39630.0 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.907374E+00 | loss scale: 1.0 | grad norm: 0.391 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1257/   25000 | consumed samples:      1287168 | elapsed time per iteration (ms): 39271.2 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.883591E+00 | loss scale: 1.0 | grad norm: 0.358 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1258/   25000 | consumed samples:      1288192 | elapsed time per iteration (ms): 39198.2 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.886891E+00 | loss scale: 1.0 | grad norm: 0.297 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1259/   25000 | consumed samples:      1289216 | elapsed time per iteration (ms): 39410.7 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.902404E+00 | loss scale: 1.0 | grad norm: 0.377 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1260/   25000 | consumed samples:      1290240 | elapsed time per iteration (ms): 39333.2 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.895636E+00 | loss scale: 1.0 | grad norm: 0.396 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1261/   25000 | consumed samples:      1291264 | elapsed time per iteration (ms): 39594.4 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.905482E+00 | loss scale: 1.0 | grad norm: 0.403 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1262/   25000 | consumed samples:      1292288 | elapsed time per iteration (ms): 39269.4 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.884863E+00 | loss scale: 1.0 | grad norm: 0.486 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1263/   25000 | consumed samples:      1293312 | elapsed time per iteration (ms): 39263.7 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.902281E+00 | loss scale: 1.0 | grad norm: 0.537 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1264/   25000 | consumed samples:      1294336 | elapsed time per iteration (ms): 39344.6 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.899641E+00 | loss scale: 1.0 | grad norm: 0.573 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1265/   25000 | consumed samples:      1295360 | elapsed time per iteration (ms): 39254.8 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.911594E+00 | loss scale: 1.0 | grad norm: 0.544 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1266/   25000 | consumed samples:      1296384 | elapsed time per iteration (ms): 39359.7 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.895551E+00 | loss scale: 1.0 | grad norm: 0.516 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1267/   25000 | consumed samples:      1297408 | elapsed time per iteration (ms): 39475.2 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.887239E+00 | loss scale: 1.0 | grad norm: 0.560 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1268/   25000 | consumed samples:      1298432 | elapsed time per iteration (ms): 39319.3 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.899998E+00 | loss scale: 1.0 | grad norm: 0.557 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1269/   25000 | consumed samples:      1299456 | elapsed time per iteration (ms): 39396.8 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.896773E+00 | loss scale: 1.0 | grad norm: 0.399 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1270/   25000 | consumed samples:      1300480 | elapsed time per iteration (ms): 39186.2 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.898232E+00 | loss scale: 1.0 | grad norm: 0.421 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1271/   25000 | consumed samples:      1301504 | elapsed time per iteration (ms): 39318.3 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.891489E+00 | loss scale: 1.0 | grad norm: 0.464 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1272/   25000 | consumed samples:      1302528 | elapsed time per iteration (ms): 39471.5 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.883583E+00 | loss scale: 1.0 | grad norm: 0.352 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1273/   25000 | consumed samples:      1303552 | elapsed time per iteration (ms): 39362.2 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.902779E+00 | loss scale: 1.0 | grad norm: 0.330 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1274/   25000 | consumed samples:      1304576 | elapsed time per iteration (ms): 39262.4 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.890811E+00 | loss scale: 1.0 | grad norm: 0.402 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1275/   25000 | consumed samples:      1305600 | elapsed time per iteration (ms): 39334.5 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.892403E+00 | loss scale: 1.0 | grad norm: 0.359 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1276/   25000 | consumed samples:      1306624 | elapsed time per iteration (ms): 39380.2 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.881594E+00 | loss scale: 1.0 | grad norm: 0.406 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1277/   25000 | consumed samples:      1307648 | elapsed time per iteration (ms): 39564.5 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.873495E+00 | loss scale: 1.0 | grad norm: 0.388 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1278/   25000 | consumed samples:      1308672 | elapsed time per iteration (ms): 39237.2 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.898320E+00 | loss scale: 1.0 | grad norm: 0.298 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1279/   25000 | consumed samples:      1309696 | elapsed time per iteration (ms): 39150.0 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.902573E+00 | loss scale: 1.0 | grad norm: 0.349 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1280/   25000 | consumed samples:      1310720 | elapsed time per iteration (ms): 39230.6 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.901830E+00 | loss scale: 1.0 | grad norm: 0.379 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1281/   25000 | consumed samples:      1311744 | elapsed time per iteration (ms): 39277.5 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.879421E+00 | loss scale: 1.0 | grad norm: 0.316 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1282/   25000 | consumed samples:      1312768 | elapsed time per iteration (ms): 39460.7 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.896661E+00 | loss scale: 1.0 | grad norm: 0.328 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1283/   25000 | consumed samples:      1313792 | elapsed time per iteration (ms): 39442.5 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.899404E+00 | loss scale: 1.0 | grad norm: 0.465 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1284/   25000 | consumed samples:      1314816 | elapsed time per iteration (ms): 39249.2 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.902557E+00 | loss scale: 1.0 | grad norm: 0.551 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1285/   25000 | consumed samples:      1315840 | elapsed time per iteration (ms): 39449.4 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.902871E+00 | loss scale: 1.0 | grad norm: 0.625 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1286/   25000 | consumed samples:      1316864 | elapsed time per iteration (ms): 39240.3 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.899111E+00 | loss scale: 1.0 | grad norm: 0.642 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1287/   25000 | consumed samples:      1317888 | elapsed time per iteration (ms): 39184.5 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.878115E+00 | loss scale: 1.0 | grad norm: 0.492 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1288/   25000 | consumed samples:      1318912 | elapsed time per iteration (ms): 39462.6 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.885091E+00 | loss scale: 1.0 | grad norm: 0.471 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1289/   25000 | consumed samples:      1319936 | elapsed time per iteration (ms): 39191.9 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.910337E+00 | loss scale: 1.0 | grad norm: 0.454 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1290/   25000 | consumed samples:      1320960 | elapsed time per iteration (ms): 39402.0 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.897191E+00 | loss scale: 1.0 | grad norm: 0.395 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1291/   25000 | consumed samples:      1321984 | elapsed time per iteration (ms): 39507.1 | learning rate: 9.997E-05 | global batch size:  1024 | lm loss: 1.874639E+00 | loss scale: 1.0 | grad norm: 0.378 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1292/   25000 | consumed samples:      1323008 | elapsed time per iteration (ms): 39241.8 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.897248E+00 | loss scale: 1.0 | grad norm: 0.390 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1293/   25000 | consumed samples:      1324032 | elapsed time per iteration (ms): 39881.9 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.876824E+00 | loss scale: 1.0 | grad norm: 0.326 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1294/   25000 | consumed samples:      1325056 | elapsed time per iteration (ms): 39206.0 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.882697E+00 | loss scale: 1.0 | grad norm: 0.296 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1295/   25000 | consumed samples:      1326080 | elapsed time per iteration (ms): 39328.2 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.891915E+00 | loss scale: 1.0 | grad norm: 0.312 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1296/   25000 | consumed samples:      1327104 | elapsed time per iteration (ms): 39351.4 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.888366E+00 | loss scale: 1.0 | grad norm: 0.290 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1297/   25000 | consumed samples:      1328128 | elapsed time per iteration (ms): 39206.7 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.881090E+00 | loss scale: 1.0 | grad norm: 0.313 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1298/   25000 | consumed samples:      1329152 | elapsed time per iteration (ms): 39323.3 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.876060E+00 | loss scale: 1.0 | grad norm: 0.321 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1299/   25000 | consumed samples:      1330176 | elapsed time per iteration (ms): 39495.7 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.872571E+00 | loss scale: 1.0 | grad norm: 0.341 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1300/   25000 | consumed samples:      1331200 | elapsed time per iteration (ms): 39579.1 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.894428E+00 | loss scale: 1.0 | grad norm: 0.341 | number of skipped iterations:   0 | number of nan iterations:   0 |
------------------------------------------------------------------------------------------------
 validation loss at iteration 1300 | lm loss value: 1.893294E+00 | lm loss PPL: 6.641210E+00 | 
------------------------------------------------------------------------------------------------
 iteration     1301/   25000 | consumed samples:      1332224 | elapsed time per iteration (ms): 174935.4 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.897450E+00 | loss scale: 1.0 | grad norm: 0.303 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1302/   25000 | consumed samples:      1333248 | elapsed time per iteration (ms): 39211.6 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.897998E+00 | loss scale: 1.0 | grad norm: 0.339 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1303/   25000 | consumed samples:      1334272 | elapsed time per iteration (ms): 39269.5 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.888332E+00 | loss scale: 1.0 | grad norm: 0.348 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1304/   25000 | consumed samples:      1335296 | elapsed time per iteration (ms): 39409.9 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.908586E+00 | loss scale: 1.0 | grad norm: 0.413 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1305/   25000 | consumed samples:      1336320 | elapsed time per iteration (ms): 39393.8 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.883211E+00 | loss scale: 1.0 | grad norm: 0.479 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1306/   25000 | consumed samples:      1337344 | elapsed time per iteration (ms): 39257.7 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.887161E+00 | loss scale: 1.0 | grad norm: 0.544 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1307/   25000 | consumed samples:      1338368 | elapsed time per iteration (ms): 39334.1 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.896995E+00 | loss scale: 1.0 | grad norm: 0.514 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1308/   25000 | consumed samples:      1339392 | elapsed time per iteration (ms): 39195.1 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.883410E+00 | loss scale: 1.0 | grad norm: 0.421 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1309/   25000 | consumed samples:      1340416 | elapsed time per iteration (ms): 39854.3 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.891067E+00 | loss scale: 1.0 | grad norm: 0.389 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1310/   25000 | consumed samples:      1341440 | elapsed time per iteration (ms): 39320.2 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.876220E+00 | loss scale: 1.0 | grad norm: 0.439 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1311/   25000 | consumed samples:      1342464 | elapsed time per iteration (ms): 39207.4 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.876039E+00 | loss scale: 1.0 | grad norm: 0.382 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1312/   25000 | consumed samples:      1343488 | elapsed time per iteration (ms): 39279.0 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.872451E+00 | loss scale: 1.0 | grad norm: 0.363 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1313/   25000 | consumed samples:      1344512 | elapsed time per iteration (ms): 39208.2 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.884817E+00 | loss scale: 1.0 | grad norm: 0.413 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1314/   25000 | consumed samples:      1345536 | elapsed time per iteration (ms): 39465.2 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.878350E+00 | loss scale: 1.0 | grad norm: 0.429 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1315/   25000 | consumed samples:      1346560 | elapsed time per iteration (ms): 39571.7 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.873736E+00 | loss scale: 1.0 | grad norm: 0.411 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1316/   25000 | consumed samples:      1347584 | elapsed time per iteration (ms): 39213.6 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.879487E+00 | loss scale: 1.0 | grad norm: 0.411 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1317/   25000 | consumed samples:      1348608 | elapsed time per iteration (ms): 39511.4 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.900724E+00 | loss scale: 1.0 | grad norm: 0.411 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1318/   25000 | consumed samples:      1349632 | elapsed time per iteration (ms): 39357.1 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.895673E+00 | loss scale: 1.0 | grad norm: 0.396 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1319/   25000 | consumed samples:      1350656 | elapsed time per iteration (ms): 39271.2 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.898073E+00 | loss scale: 1.0 | grad norm: 0.401 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1320/   25000 | consumed samples:      1351680 | elapsed time per iteration (ms): 39567.7 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.885766E+00 | loss scale: 1.0 | grad norm: 0.417 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1321/   25000 | consumed samples:      1352704 | elapsed time per iteration (ms): 39205.8 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.882991E+00 | loss scale: 1.0 | grad norm: 0.387 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1322/   25000 | consumed samples:      1353728 | elapsed time per iteration (ms): 39293.2 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.892719E+00 | loss scale: 1.0 | grad norm: 0.378 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1323/   25000 | consumed samples:      1354752 | elapsed time per iteration (ms): 39444.4 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.900176E+00 | loss scale: 1.0 | grad norm: 0.368 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1324/   25000 | consumed samples:      1355776 | elapsed time per iteration (ms): 39221.2 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.877914E+00 | loss scale: 1.0 | grad norm: 0.317 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1325/   25000 | consumed samples:      1356800 | elapsed time per iteration (ms): 39832.3 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.911770E+00 | loss scale: 1.0 | grad norm: 0.300 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1326/   25000 | consumed samples:      1357824 | elapsed time per iteration (ms): 39222.4 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.901542E+00 | loss scale: 1.0 | grad norm: 0.375 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1327/   25000 | consumed samples:      1358848 | elapsed time per iteration (ms): 39228.1 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.890475E+00 | loss scale: 1.0 | grad norm: 0.399 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1328/   25000 | consumed samples:      1359872 | elapsed time per iteration (ms): 39437.7 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.878916E+00 | loss scale: 1.0 | grad norm: 0.366 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1329/   25000 | consumed samples:      1360896 | elapsed time per iteration (ms): 39195.0 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.881507E+00 | loss scale: 1.0 | grad norm: 0.306 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1330/   25000 | consumed samples:      1361920 | elapsed time per iteration (ms): 39508.4 | learning rate: 9.996E-05 | global batch size:  1024 | lm loss: 1.895689E+00 | loss scale: 1.0 | grad norm: 0.317 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1331/   25000 | consumed samples:      1362944 | elapsed time per iteration (ms): 39365.6 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.888008E+00 | loss scale: 1.0 | grad norm: 0.378 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1332/   25000 | consumed samples:      1363968 | elapsed time per iteration (ms): 39349.4 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.896283E+00 | loss scale: 1.0 | grad norm: 0.420 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1333/   25000 | consumed samples:      1364992 | elapsed time per iteration (ms): 39406.8 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.893800E+00 | loss scale: 1.0 | grad norm: 0.463 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1334/   25000 | consumed samples:      1366016 | elapsed time per iteration (ms): 39181.7 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.888754E+00 | loss scale: 1.0 | grad norm: 0.518 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1335/   25000 | consumed samples:      1367040 | elapsed time per iteration (ms): 39381.8 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.899865E+00 | loss scale: 1.0 | grad norm: 0.495 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1336/   25000 | consumed samples:      1368064 | elapsed time per iteration (ms): 39423.9 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.903924E+00 | loss scale: 1.0 | grad norm: 0.400 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1337/   25000 | consumed samples:      1369088 | elapsed time per iteration (ms): 39372.1 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.880925E+00 | loss scale: 1.0 | grad norm: 0.393 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1338/   25000 | consumed samples:      1370112 | elapsed time per iteration (ms): 39313.1 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.878407E+00 | loss scale: 1.0 | grad norm: 0.441 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1339/   25000 | consumed samples:      1371136 | elapsed time per iteration (ms): 39291.4 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.884819E+00 | loss scale: 1.0 | grad norm: 0.355 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1340/   25000 | consumed samples:      1372160 | elapsed time per iteration (ms): 39415.3 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.872715E+00 | loss scale: 1.0 | grad norm: 0.311 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1341/   25000 | consumed samples:      1373184 | elapsed time per iteration (ms): 39620.2 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.888007E+00 | loss scale: 1.0 | grad norm: 0.406 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1342/   25000 | consumed samples:      1374208 | elapsed time per iteration (ms): 39345.4 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.888803E+00 | loss scale: 1.0 | grad norm: 0.412 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1343/   25000 | consumed samples:      1375232 | elapsed time per iteration (ms): 39219.7 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.879623E+00 | loss scale: 1.0 | grad norm: 0.335 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1344/   25000 | consumed samples:      1376256 | elapsed time per iteration (ms): 39433.6 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.881770E+00 | loss scale: 1.0 | grad norm: 0.292 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1345/   25000 | consumed samples:      1377280 | elapsed time per iteration (ms): 39201.6 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.890164E+00 | loss scale: 1.0 | grad norm: 0.332 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1346/   25000 | consumed samples:      1378304 | elapsed time per iteration (ms): 39580.8 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.872995E+00 | loss scale: 1.0 | grad norm: 0.380 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1347/   25000 | consumed samples:      1379328 | elapsed time per iteration (ms): 39314.5 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.891646E+00 | loss scale: 1.0 | grad norm: 0.381 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1348/   25000 | consumed samples:      1380352 | elapsed time per iteration (ms): 39371.5 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.878422E+00 | loss scale: 1.0 | grad norm: 0.389 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1349/   25000 | consumed samples:      1381376 | elapsed time per iteration (ms): 39494.3 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.864993E+00 | loss scale: 1.0 | grad norm: 0.473 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1350/   25000 | consumed samples:      1382400 | elapsed time per iteration (ms): 39432.9 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.898828E+00 | loss scale: 1.0 | grad norm: 0.510 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1351/   25000 | consumed samples:      1383424 | elapsed time per iteration (ms): 39324.8 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.885699E+00 | loss scale: 1.0 | grad norm: 0.487 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1352/   25000 | consumed samples:      1384448 | elapsed time per iteration (ms): 39388.5 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.873474E+00 | loss scale: 1.0 | grad norm: 0.439 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1353/   25000 | consumed samples:      1385472 | elapsed time per iteration (ms): 39232.5 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.894216E+00 | loss scale: 1.0 | grad norm: 0.461 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1354/   25000 | consumed samples:      1386496 | elapsed time per iteration (ms): 39411.1 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.890590E+00 | loss scale: 1.0 | grad norm: 0.399 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1355/   25000 | consumed samples:      1387520 | elapsed time per iteration (ms): 39357.3 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.881369E+00 | loss scale: 1.0 | grad norm: 0.361 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1356/   25000 | consumed samples:      1388544 | elapsed time per iteration (ms): 39350.7 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.883663E+00 | loss scale: 1.0 | grad norm: 0.365 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1357/   25000 | consumed samples:      1389568 | elapsed time per iteration (ms): 39601.0 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.880862E+00 | loss scale: 1.0 | grad norm: 0.378 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1358/   25000 | consumed samples:      1390592 | elapsed time per iteration (ms): 39265.2 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.883931E+00 | loss scale: 1.0 | grad norm: 0.364 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1359/   25000 | consumed samples:      1391616 | elapsed time per iteration (ms): 39381.4 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.883991E+00 | loss scale: 1.0 | grad norm: 0.340 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1360/   25000 | consumed samples:      1392640 | elapsed time per iteration (ms): 39257.6 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.879285E+00 | loss scale: 1.0 | grad norm: 0.368 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1361/   25000 | consumed samples:      1393664 | elapsed time per iteration (ms): 39187.9 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.886055E+00 | loss scale: 1.0 | grad norm: 0.346 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1362/   25000 | consumed samples:      1394688 | elapsed time per iteration (ms): 39483.7 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.879651E+00 | loss scale: 1.0 | grad norm: 0.324 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1363/   25000 | consumed samples:      1395712 | elapsed time per iteration (ms): 39209.7 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.900134E+00 | loss scale: 1.0 | grad norm: 0.419 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1364/   25000 | consumed samples:      1396736 | elapsed time per iteration (ms): 39645.9 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.901179E+00 | loss scale: 1.0 | grad norm: 0.463 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1365/   25000 | consumed samples:      1397760 | elapsed time per iteration (ms): 39268.1 | learning rate: 9.995E-05 | global batch size:  1024 | lm loss: 1.862229E+00 | loss scale: 1.0 | grad norm: 0.439 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1366/   25000 | consumed samples:      1398784 | elapsed time per iteration (ms): 39351.4 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.883919E+00 | loss scale: 1.0 | grad norm: 0.465 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1367/   25000 | consumed samples:      1399808 | elapsed time per iteration (ms): 39318.7 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.868263E+00 | loss scale: 1.0 | grad norm: 0.449 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1368/   25000 | consumed samples:      1400832 | elapsed time per iteration (ms): 39453.6 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.881391E+00 | loss scale: 1.0 | grad norm: 0.484 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1369/   25000 | consumed samples:      1401856 | elapsed time per iteration (ms): 39350.7 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.866728E+00 | loss scale: 1.0 | grad norm: 0.503 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1370/   25000 | consumed samples:      1402880 | elapsed time per iteration (ms): 39287.9 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.873331E+00 | loss scale: 1.0 | grad norm: 0.401 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1371/   25000 | consumed samples:      1403904 | elapsed time per iteration (ms): 39226.3 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.877588E+00 | loss scale: 1.0 | grad norm: 0.294 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1372/   25000 | consumed samples:      1404928 | elapsed time per iteration (ms): 39435.9 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.883446E+00 | loss scale: 1.0 | grad norm: 0.378 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1373/   25000 | consumed samples:      1405952 | elapsed time per iteration (ms): 39576.4 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.889989E+00 | loss scale: 1.0 | grad norm: 0.405 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1374/   25000 | consumed samples:      1406976 | elapsed time per iteration (ms): 39493.0 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.885381E+00 | loss scale: 1.0 | grad norm: 0.359 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1375/   25000 | consumed samples:      1408000 | elapsed time per iteration (ms): 39220.9 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.871424E+00 | loss scale: 1.0 | grad norm: 0.340 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1376/   25000 | consumed samples:      1409024 | elapsed time per iteration (ms): 39297.6 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.869596E+00 | loss scale: 1.0 | grad norm: 0.370 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1377/   25000 | consumed samples:      1410048 | elapsed time per iteration (ms): 39344.1 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.892074E+00 | loss scale: 1.0 | grad norm: 0.370 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1378/   25000 | consumed samples:      1411072 | elapsed time per iteration (ms): 39464.5 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.877125E+00 | loss scale: 1.0 | grad norm: 0.362 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1379/   25000 | consumed samples:      1412096 | elapsed time per iteration (ms): 39321.7 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.875232E+00 | loss scale: 1.0 | grad norm: 0.369 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1380/   25000 | consumed samples:      1413120 | elapsed time per iteration (ms): 39331.2 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.867568E+00 | loss scale: 1.0 | grad norm: 0.331 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1381/   25000 | consumed samples:      1414144 | elapsed time per iteration (ms): 39270.4 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.870227E+00 | loss scale: 1.0 | grad norm: 0.349 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1382/   25000 | consumed samples:      1415168 | elapsed time per iteration (ms): 39551.7 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.880818E+00 | loss scale: 1.0 | grad norm: 0.328 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1383/   25000 | consumed samples:      1416192 | elapsed time per iteration (ms): 39294.8 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.876505E+00 | loss scale: 1.0 | grad norm: 0.353 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1384/   25000 | consumed samples:      1417216 | elapsed time per iteration (ms): 39470.2 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.883979E+00 | loss scale: 1.0 | grad norm: 0.400 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1385/   25000 | consumed samples:      1418240 | elapsed time per iteration (ms): 39174.0 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.867313E+00 | loss scale: 1.0 | grad norm: 0.347 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1386/   25000 | consumed samples:      1419264 | elapsed time per iteration (ms): 39316.7 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.886180E+00 | loss scale: 1.0 | grad norm: 0.293 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1387/   25000 | consumed samples:      1420288 | elapsed time per iteration (ms): 39188.0 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.880580E+00 | loss scale: 1.0 | grad norm: 0.326 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1388/   25000 | consumed samples:      1421312 | elapsed time per iteration (ms): 39414.7 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.888561E+00 | loss scale: 1.0 | grad norm: 0.369 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1389/   25000 | consumed samples:      1422336 | elapsed time per iteration (ms): 39477.3 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.869562E+00 | loss scale: 1.0 | grad norm: 0.366 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1390/   25000 | consumed samples:      1423360 | elapsed time per iteration (ms): 39478.0 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.865025E+00 | loss scale: 1.0 | grad norm: 0.408 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1391/   25000 | consumed samples:      1424384 | elapsed time per iteration (ms): 39370.4 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.879978E+00 | loss scale: 1.0 | grad norm: 0.433 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1392/   25000 | consumed samples:      1425408 | elapsed time per iteration (ms): 39305.0 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.888384E+00 | loss scale: 1.0 | grad norm: 0.435 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1393/   25000 | consumed samples:      1426432 | elapsed time per iteration (ms): 39283.4 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.870496E+00 | loss scale: 1.0 | grad norm: 0.400 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1394/   25000 | consumed samples:      1427456 | elapsed time per iteration (ms): 39608.1 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.872935E+00 | loss scale: 1.0 | grad norm: 0.401 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1395/   25000 | consumed samples:      1428480 | elapsed time per iteration (ms): 39298.6 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.892227E+00 | loss scale: 1.0 | grad norm: 0.335 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1396/   25000 | consumed samples:      1429504 | elapsed time per iteration (ms): 39371.8 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.884679E+00 | loss scale: 1.0 | grad norm: 0.370 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1397/   25000 | consumed samples:      1430528 | elapsed time per iteration (ms): 39220.6 | learning rate: 9.994E-05 | global batch size:  1024 | lm loss: 1.884557E+00 | loss scale: 1.0 | grad norm: 0.386 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1398/   25000 | consumed samples:      1431552 | elapsed time per iteration (ms): 39628.7 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.862829E+00 | loss scale: 1.0 | grad norm: 0.338 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1399/   25000 | consumed samples:      1432576 | elapsed time per iteration (ms): 39419.3 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.888884E+00 | loss scale: 1.0 | grad norm: 0.354 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1400/   25000 | consumed samples:      1433600 | elapsed time per iteration (ms): 39385.2 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.851829E+00 | loss scale: 1.0 | grad norm: 0.393 | number of skipped iterations:   0 | number of nan iterations:   0 |
------------------------------------------------------------------------------------------------
 validation loss at iteration 1400 | lm loss value: 1.880426E+00 | lm loss PPL: 6.556299E+00 | 
------------------------------------------------------------------------------------------------
 iteration     1401/   25000 | consumed samples:      1434624 | elapsed time per iteration (ms): 174145.3 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.854756E+00 | loss scale: 1.0 | grad norm: 0.345 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1402/   25000 | consumed samples:      1435648 | elapsed time per iteration (ms): 39260.9 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.878852E+00 | loss scale: 1.0 | grad norm: 0.343 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1403/   25000 | consumed samples:      1436672 | elapsed time per iteration (ms): 39327.1 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.870796E+00 | loss scale: 1.0 | grad norm: 0.340 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1404/   25000 | consumed samples:      1437696 | elapsed time per iteration (ms): 39522.1 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.885202E+00 | loss scale: 1.0 | grad norm: 0.289 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1405/   25000 | consumed samples:      1438720 | elapsed time per iteration (ms): 39249.9 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.878006E+00 | loss scale: 1.0 | grad norm: 0.303 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1406/   25000 | consumed samples:      1439744 | elapsed time per iteration (ms): 39452.7 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.891639E+00 | loss scale: 1.0 | grad norm: 0.370 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1407/   25000 | consumed samples:      1440768 | elapsed time per iteration (ms): 39176.7 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.863386E+00 | loss scale: 1.0 | grad norm: 0.386 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1408/   25000 | consumed samples:      1441792 | elapsed time per iteration (ms): 39386.2 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.870223E+00 | loss scale: 1.0 | grad norm: 0.417 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1409/   25000 | consumed samples:      1442816 | elapsed time per iteration (ms): 39329.2 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.870377E+00 | loss scale: 1.0 | grad norm: 0.362 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1410/   25000 | consumed samples:      1443840 | elapsed time per iteration (ms): 39433.1 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.889677E+00 | loss scale: 1.0 | grad norm: 0.342 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1411/   25000 | consumed samples:      1444864 | elapsed time per iteration (ms): 39180.1 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.890646E+00 | loss scale: 1.0 | grad norm: 0.372 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1412/   25000 | consumed samples:      1445888 | elapsed time per iteration (ms): 39341.7 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.866951E+00 | loss scale: 1.0 | grad norm: 0.413 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1413/   25000 | consumed samples:      1446912 | elapsed time per iteration (ms): 39311.7 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.879943E+00 | loss scale: 1.0 | grad norm: 0.389 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1414/   25000 | consumed samples:      1447936 | elapsed time per iteration (ms): 39546.3 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.886019E+00 | loss scale: 1.0 | grad norm: 0.398 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1415/   25000 | consumed samples:      1448960 | elapsed time per iteration (ms): 39390.1 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.880483E+00 | loss scale: 1.0 | grad norm: 0.455 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1416/   25000 | consumed samples:      1449984 | elapsed time per iteration (ms): 39206.5 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.871672E+00 | loss scale: 1.0 | grad norm: 0.519 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1417/   25000 | consumed samples:      1451008 | elapsed time per iteration (ms): 39275.0 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.880270E+00 | loss scale: 1.0 | grad norm: 0.494 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1418/   25000 | consumed samples:      1452032 | elapsed time per iteration (ms): 39399.1 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.872117E+00 | loss scale: 1.0 | grad norm: 0.415 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1419/   25000 | consumed samples:      1453056 | elapsed time per iteration (ms): 39414.1 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.885085E+00 | loss scale: 1.0 | grad norm: 0.372 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1420/   25000 | consumed samples:      1454080 | elapsed time per iteration (ms): 39475.6 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.883972E+00 | loss scale: 1.0 | grad norm: 0.354 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1421/   25000 | consumed samples:      1455104 | elapsed time per iteration (ms): 39286.5 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.863643E+00 | loss scale: 1.0 | grad norm: 0.347 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1422/   25000 | consumed samples:      1456128 | elapsed time per iteration (ms): 39504.3 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.872203E+00 | loss scale: 1.0 | grad norm: 0.376 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1423/   25000 | consumed samples:      1457152 | elapsed time per iteration (ms): 39412.9 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.893435E+00 | loss scale: 1.0 | grad norm: 0.354 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1424/   25000 | consumed samples:      1458176 | elapsed time per iteration (ms): 39202.1 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.884284E+00 | loss scale: 1.0 | grad norm: 0.322 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1425/   25000 | consumed samples:      1459200 | elapsed time per iteration (ms): 39322.7 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.867565E+00 | loss scale: 1.0 | grad norm: 0.309 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1426/   25000 | consumed samples:      1460224 | elapsed time per iteration (ms): 39376.8 | learning rate: 9.993E-05 | global batch size:  1024 | lm loss: 1.869473E+00 | loss scale: 1.0 | grad norm: 0.321 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1427/   25000 | consumed samples:      1461248 | elapsed time per iteration (ms): 39177.3 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.868511E+00 | loss scale: 1.0 | grad norm: 0.344 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1428/   25000 | consumed samples:      1462272 | elapsed time per iteration (ms): 39626.9 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.890590E+00 | loss scale: 1.0 | grad norm: 0.313 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1429/   25000 | consumed samples:      1463296 | elapsed time per iteration (ms): 39166.4 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.863125E+00 | loss scale: 1.0 | grad norm: 0.310 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1430/   25000 | consumed samples:      1464320 | elapsed time per iteration (ms): 39385.7 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.875448E+00 | loss scale: 1.0 | grad norm: 0.299 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1431/   25000 | consumed samples:      1465344 | elapsed time per iteration (ms): 39426.8 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.861568E+00 | loss scale: 1.0 | grad norm: 0.289 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1432/   25000 | consumed samples:      1466368 | elapsed time per iteration (ms): 39241.9 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.877002E+00 | loss scale: 1.0 | grad norm: 0.297 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1433/   25000 | consumed samples:      1467392 | elapsed time per iteration (ms): 39310.4 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.868806E+00 | loss scale: 1.0 | grad norm: 0.324 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1434/   25000 | consumed samples:      1468416 | elapsed time per iteration (ms): 39248.8 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.873040E+00 | loss scale: 1.0 | grad norm: 0.333 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1435/   25000 | consumed samples:      1469440 | elapsed time per iteration (ms): 39173.6 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.855509E+00 | loss scale: 1.0 | grad norm: 0.376 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1436/   25000 | consumed samples:      1470464 | elapsed time per iteration (ms): 39466.7 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.865462E+00 | loss scale: 1.0 | grad norm: 0.397 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1437/   25000 | consumed samples:      1471488 | elapsed time per iteration (ms): 39424.6 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.871166E+00 | loss scale: 1.0 | grad norm: 0.412 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1438/   25000 | consumed samples:      1472512 | elapsed time per iteration (ms): 39538.7 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.879724E+00 | loss scale: 1.0 | grad norm: 0.443 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1439/   25000 | consumed samples:      1473536 | elapsed time per iteration (ms): 39289.5 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.877629E+00 | loss scale: 1.0 | grad norm: 0.518 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1440/   25000 | consumed samples:      1474560 | elapsed time per iteration (ms): 39208.4 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.862511E+00 | loss scale: 1.0 | grad norm: 0.565 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1441/   25000 | consumed samples:      1475584 | elapsed time per iteration (ms): 39396.8 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.850922E+00 | loss scale: 1.0 | grad norm: 0.493 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1442/   25000 | consumed samples:      1476608 | elapsed time per iteration (ms): 39412.8 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.874177E+00 | loss scale: 1.0 | grad norm: 0.348 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1443/   25000 | consumed samples:      1477632 | elapsed time per iteration (ms): 39268.2 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.860819E+00 | loss scale: 1.0 | grad norm: 0.327 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1444/   25000 | consumed samples:      1478656 | elapsed time per iteration (ms): 39369.4 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.866236E+00 | loss scale: 1.0 | grad norm: 0.431 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1445/   25000 | consumed samples:      1479680 | elapsed time per iteration (ms): 39195.3 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.871427E+00 | loss scale: 1.0 | grad norm: 0.394 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1446/   25000 | consumed samples:      1480704 | elapsed time per iteration (ms): 39571.7 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.882970E+00 | loss scale: 1.0 | grad norm: 0.353 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1447/   25000 | consumed samples:      1481728 | elapsed time per iteration (ms): 39574.5 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.861797E+00 | loss scale: 1.0 | grad norm: 0.330 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1448/   25000 | consumed samples:      1482752 | elapsed time per iteration (ms): 39227.8 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.876439E+00 | loss scale: 1.0 | grad norm: 0.332 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1449/   25000 | consumed samples:      1483776 | elapsed time per iteration (ms): 39240.1 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.869168E+00 | loss scale: 1.0 | grad norm: 0.345 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1450/   25000 | consumed samples:      1484800 | elapsed time per iteration (ms): 39313.8 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.863014E+00 | loss scale: 1.0 | grad norm: 0.287 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1451/   25000 | consumed samples:      1485824 | elapsed time per iteration (ms): 39165.2 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.886982E+00 | loss scale: 1.0 | grad norm: 0.316 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1452/   25000 | consumed samples:      1486848 | elapsed time per iteration (ms): 39543.1 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.856853E+00 | loss scale: 1.0 | grad norm: 0.346 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1453/   25000 | consumed samples:      1487872 | elapsed time per iteration (ms): 39318.3 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.862741E+00 | loss scale: 1.0 | grad norm: 0.297 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1454/   25000 | consumed samples:      1488896 | elapsed time per iteration (ms): 39295.6 | learning rate: 9.992E-05 | global batch size:  1024 | lm loss: 1.844434E+00 | loss scale: 1.0 | grad norm: 0.293 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1455/   25000 | consumed samples:      1489920 | elapsed time per iteration (ms): 39457.0 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.865891E+00 | loss scale: 1.0 | grad norm: 0.331 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1456/   25000 | consumed samples:      1490944 | elapsed time per iteration (ms): 39172.0 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.882024E+00 | loss scale: 1.0 | grad norm: 0.335 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1457/   25000 | consumed samples:      1491968 | elapsed time per iteration (ms): 39479.8 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.866032E+00 | loss scale: 1.0 | grad norm: 0.323 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1458/   25000 | consumed samples:      1492992 | elapsed time per iteration (ms): 39232.0 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.883485E+00 | loss scale: 1.0 | grad norm: 0.371 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1459/   25000 | consumed samples:      1494016 | elapsed time per iteration (ms): 39299.7 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.867104E+00 | loss scale: 1.0 | grad norm: 0.390 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1460/   25000 | consumed samples:      1495040 | elapsed time per iteration (ms): 39304.8 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.865345E+00 | loss scale: 1.0 | grad norm: 0.431 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1461/   25000 | consumed samples:      1496064 | elapsed time per iteration (ms): 39264.4 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.869978E+00 | loss scale: 1.0 | grad norm: 0.497 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1462/   25000 | consumed samples:      1497088 | elapsed time per iteration (ms): 39517.6 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.876620E+00 | loss scale: 1.0 | grad norm: 0.479 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1463/   25000 | consumed samples:      1498112 | elapsed time per iteration (ms): 39455.5 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.871682E+00 | loss scale: 1.0 | grad norm: 0.495 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1464/   25000 | consumed samples:      1499136 | elapsed time per iteration (ms): 39349.5 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.885639E+00 | loss scale: 1.0 | grad norm: 0.481 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1465/   25000 | consumed samples:      1500160 | elapsed time per iteration (ms): 39257.3 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.872226E+00 | loss scale: 1.0 | grad norm: 0.388 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1466/   25000 | consumed samples:      1501184 | elapsed time per iteration (ms): 39186.5 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.881456E+00 | loss scale: 1.0 | grad norm: 0.355 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1467/   25000 | consumed samples:      1502208 | elapsed time per iteration (ms): 39400.6 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.859925E+00 | loss scale: 1.0 | grad norm: 0.455 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1468/   25000 | consumed samples:      1503232 | elapsed time per iteration (ms): 39467.3 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.866316E+00 | loss scale: 1.0 | grad norm: 0.439 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1469/   25000 | consumed samples:      1504256 | elapsed time per iteration (ms): 39340.1 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.870847E+00 | loss scale: 1.0 | grad norm: 0.330 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1470/   25000 | consumed samples:      1505280 | elapsed time per iteration (ms): 39318.9 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.861120E+00 | loss scale: 1.0 | grad norm: 0.389 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1471/   25000 | consumed samples:      1506304 | elapsed time per iteration (ms): 39322.1 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.860788E+00 | loss scale: 1.0 | grad norm: 0.349 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1472/   25000 | consumed samples:      1507328 | elapsed time per iteration (ms): 39303.5 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.865746E+00 | loss scale: 1.0 | grad norm: 0.268 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1473/   25000 | consumed samples:      1508352 | elapsed time per iteration (ms): 39523.3 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.849990E+00 | loss scale: 1.0 | grad norm: 0.294 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1474/   25000 | consumed samples:      1509376 | elapsed time per iteration (ms): 39243.5 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.854054E+00 | loss scale: 1.0 | grad norm: 0.267 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1475/   25000 | consumed samples:      1510400 | elapsed time per iteration (ms): 39232.0 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.853117E+00 | loss scale: 1.0 | grad norm: 0.267 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1476/   25000 | consumed samples:      1511424 | elapsed time per iteration (ms): 39178.5 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.872874E+00 | loss scale: 1.0 | grad norm: 0.334 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1477/   25000 | consumed samples:      1512448 | elapsed time per iteration (ms): 39505.9 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.863207E+00 | loss scale: 1.0 | grad norm: 0.362 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1478/   25000 | consumed samples:      1513472 | elapsed time per iteration (ms): 39293.3 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.860775E+00 | loss scale: 1.0 | grad norm: 0.398 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1479/   25000 | consumed samples:      1514496 | elapsed time per iteration (ms): 39519.7 | learning rate: 9.991E-05 | global batch size:  1024 | lm loss: 1.864409E+00 | loss scale: 1.0 | grad norm: 0.406 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1480/   25000 | consumed samples:      1515520 | elapsed time per iteration (ms): 39166.5 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.861861E+00 | loss scale: 1.0 | grad norm: 0.407 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1481/   25000 | consumed samples:      1516544 | elapsed time per iteration (ms): 39234.0 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.874445E+00 | loss scale: 1.0 | grad norm: 0.376 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1482/   25000 | consumed samples:      1517568 | elapsed time per iteration (ms): 39441.0 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.867777E+00 | loss scale: 1.0 | grad norm: 0.369 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1483/   25000 | consumed samples:      1518592 | elapsed time per iteration (ms): 39241.2 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.868540E+00 | loss scale: 1.0 | grad norm: 0.416 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1484/   25000 | consumed samples:      1519616 | elapsed time per iteration (ms): 39355.2 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.853899E+00 | loss scale: 1.0 | grad norm: 0.462 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1485/   25000 | consumed samples:      1520640 | elapsed time per iteration (ms): 39310.1 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.846759E+00 | loss scale: 1.0 | grad norm: 0.443 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1486/   25000 | consumed samples:      1521664 | elapsed time per iteration (ms): 39296.5 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.860614E+00 | loss scale: 1.0 | grad norm: 0.307 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1487/   25000 | consumed samples:      1522688 | elapsed time per iteration (ms): 40498.4 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.873386E+00 | loss scale: 1.0 | grad norm: 0.340 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1488/   25000 | consumed samples:      1523712 | elapsed time per iteration (ms): 39108.6 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.873640E+00 | loss scale: 1.0 | grad norm: 0.412 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1489/   25000 | consumed samples:      1524736 | elapsed time per iteration (ms): 39297.4 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.867371E+00 | loss scale: 1.0 | grad norm: 0.344 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1490/   25000 | consumed samples:      1525760 | elapsed time per iteration (ms): 39189.7 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.872459E+00 | loss scale: 1.0 | grad norm: 0.272 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1491/   25000 | consumed samples:      1526784 | elapsed time per iteration (ms): 39431.6 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.861726E+00 | loss scale: 1.0 | grad norm: 0.352 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1492/   25000 | consumed samples:      1527808 | elapsed time per iteration (ms): 39200.0 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.851338E+00 | loss scale: 1.0 | grad norm: 0.381 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1493/   25000 | consumed samples:      1528832 | elapsed time per iteration (ms): 39272.8 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.887844E+00 | loss scale: 1.0 | grad norm: 0.365 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1494/   25000 | consumed samples:      1529856 | elapsed time per iteration (ms): 39195.9 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.877477E+00 | loss scale: 1.0 | grad norm: 0.400 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1495/   25000 | consumed samples:      1530880 | elapsed time per iteration (ms): 39587.7 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.860311E+00 | loss scale: 1.0 | grad norm: 0.392 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1496/   25000 | consumed samples:      1531904 | elapsed time per iteration (ms): 39228.7 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.868891E+00 | loss scale: 1.0 | grad norm: 0.378 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1497/   25000 | consumed samples:      1532928 | elapsed time per iteration (ms): 39220.6 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.906458E+00 | loss scale: 1.0 | grad norm: 0.344 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1498/   25000 | consumed samples:      1533952 | elapsed time per iteration (ms): 39156.0 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.869870E+00 | loss scale: 1.0 | grad norm: 0.357 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1499/   25000 | consumed samples:      1534976 | elapsed time per iteration (ms): 39304.5 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.874036E+00 | loss scale: 1.0 | grad norm: 0.334 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1500/   25000 | consumed samples:      1536000 | elapsed time per iteration (ms): 39554.2 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.860226E+00 | loss scale: 1.0 | grad norm: 0.337 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration    1500 to /bb/llm/gaf51275/llama/checkpoints/Llama-2-7b-base-extended/okazaki_lab_cc-en-updated/tp2-pp2
------------------------------------------------------------------------------------------------
 validation loss at iteration 1500 | lm loss value: 1.869817E+00 | lm loss PPL: 6.487108E+00 | 
------------------------------------------------------------------------------------------------
  successfully saved checkpoint at iteration    1500 to /bb/llm/gaf51275/llama/checkpoints/Llama-2-7b-base-extended/okazaki_lab_cc-en-updated/tp2-pp2
(min, max) time across ranks (ms):
    save-checkpoint ................................: (45054.64, 45056.32)
 iteration     1501/   25000 | consumed samples:      1537024 | elapsed time per iteration (ms): 219863.3 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.867102E+00 | loss scale: 1.0 | grad norm: 0.376 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1502/   25000 | consumed samples:      1538048 | elapsed time per iteration (ms): 39108.1 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.871591E+00 | loss scale: 1.0 | grad norm: 0.382 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1503/   25000 | consumed samples:      1539072 | elapsed time per iteration (ms): 39619.2 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.863912E+00 | loss scale: 1.0 | grad norm: 0.306 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1504/   25000 | consumed samples:      1540096 | elapsed time per iteration (ms): 39133.9 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.879005E+00 | loss scale: 1.0 | grad norm: 0.295 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1505/   25000 | consumed samples:      1541120 | elapsed time per iteration (ms): 39469.7 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.862330E+00 | loss scale: 1.0 | grad norm: 0.336 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1506/   25000 | consumed samples:      1542144 | elapsed time per iteration (ms): 39246.0 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.855567E+00 | loss scale: 1.0 | grad norm: 0.326 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1507/   25000 | consumed samples:      1543168 | elapsed time per iteration (ms): 39200.6 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.850049E+00 | loss scale: 1.0 | grad norm: 0.324 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1508/   25000 | consumed samples:      1544192 | elapsed time per iteration (ms): 39126.1 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.872263E+00 | loss scale: 1.0 | grad norm: 0.335 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1509/   25000 | consumed samples:      1545216 | elapsed time per iteration (ms): 39362.2 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.840030E+00 | loss scale: 1.0 | grad norm: 0.322 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1510/   25000 | consumed samples:      1546240 | elapsed time per iteration (ms): 39302.3 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.873090E+00 | loss scale: 1.0 | grad norm: 0.319 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1511/   25000 | consumed samples:      1547264 | elapsed time per iteration (ms): 39692.5 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.852559E+00 | loss scale: 1.0 | grad norm: 0.290 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1512/   25000 | consumed samples:      1548288 | elapsed time per iteration (ms): 39147.1 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.851319E+00 | loss scale: 1.0 | grad norm: 0.298 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1513/   25000 | consumed samples:      1549312 | elapsed time per iteration (ms): 39218.6 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.859675E+00 | loss scale: 1.0 | grad norm: 0.286 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1514/   25000 | consumed samples:      1550336 | elapsed time per iteration (ms): 39217.9 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.849010E+00 | loss scale: 1.0 | grad norm: 0.302 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1515/   25000 | consumed samples:      1551360 | elapsed time per iteration (ms): 39384.1 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.863720E+00 | loss scale: 1.0 | grad norm: 0.372 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1516/   25000 | consumed samples:      1552384 | elapsed time per iteration (ms): 39462.1 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.871728E+00 | loss scale: 1.0 | grad norm: 0.422 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1517/   25000 | consumed samples:      1553408 | elapsed time per iteration (ms): 39284.3 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.859392E+00 | loss scale: 1.0 | grad norm: 0.551 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1518/   25000 | consumed samples:      1554432 | elapsed time per iteration (ms): 39137.6 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.854549E+00 | loss scale: 1.0 | grad norm: 0.696 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1519/   25000 | consumed samples:      1555456 | elapsed time per iteration (ms): 39613.7 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.874004E+00 | loss scale: 1.0 | grad norm: 0.718 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1520/   25000 | consumed samples:      1556480 | elapsed time per iteration (ms): 39156.2 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.880682E+00 | loss scale: 1.0 | grad norm: 0.569 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1521/   25000 | consumed samples:      1557504 | elapsed time per iteration (ms): 39561.0 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.863048E+00 | loss scale: 1.0 | grad norm: 0.388 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1522/   25000 | consumed samples:      1558528 | elapsed time per iteration (ms): 39175.1 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.859094E+00 | loss scale: 1.0 | grad norm: 0.468 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1523/   25000 | consumed samples:      1559552 | elapsed time per iteration (ms): 39309.5 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.878507E+00 | loss scale: 1.0 | grad norm: 0.481 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1524/   25000 | consumed samples:      1560576 | elapsed time per iteration (ms): 39194.0 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.845266E+00 | loss scale: 1.0 | grad norm: 0.315 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1525/   25000 | consumed samples:      1561600 | elapsed time per iteration (ms): 39330.7 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.854998E+00 | loss scale: 1.0 | grad norm: 0.384 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1526/   25000 | consumed samples:      1562624 | elapsed time per iteration (ms): 39426.5 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.868027E+00 | loss scale: 1.0 | grad norm: 0.388 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1527/   25000 | consumed samples:      1563648 | elapsed time per iteration (ms): 39478.7 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.861790E+00 | loss scale: 1.0 | grad norm: 0.273 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1528/   25000 | consumed samples:      1564672 | elapsed time per iteration (ms): 39423.5 | learning rate: 9.989E-05 | global batch size:  1024 | lm loss: 1.870814E+00 | loss scale: 1.0 | grad norm: 0.303 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1529/   25000 | consumed samples:      1565696 | elapsed time per iteration (ms): 39243.9 | learning rate: 9.988E-05 | global batch size:  1024 | lm loss: 1.859799E+00 | loss scale: 1.0 | grad norm: 0.337 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1530/   25000 | consumed samples:      1566720 | elapsed time per iteration (ms): 39160.1 | learning rate: 9.988E-05 | global batch size:  1024 | lm loss: 1.866710E+00 | loss scale: 1.0 | grad norm: 0.273 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1531/   25000 | consumed samples:      1567744 | elapsed time per iteration (ms): 39426.9 | learning rate: 9.988E-05 | global batch size:  1024 | lm loss: 1.847695E+00 | loss scale: 1.0 | grad norm: 0.287 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1532/   25000 | consumed samples:      1568768 | elapsed time per iteration (ms): 39363.3 | learning rate: 9.988E-05 | global batch size:  1024 | lm loss: 1.858956E+00 | loss scale: 1.0 | grad norm: 0.320 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1533/   25000 | consumed samples:      1569792 | elapsed time per iteration (ms): 39303.4 | learning rate: 9.988E-05 | global batch size:  1024 | lm loss: 1.867799E+00 | loss scale: 1.0 | grad norm: 0.314 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1534/   25000 | consumed samples:      1570816 | elapsed time per iteration (ms): 39156.6 | learning rate: 9.988E-05 | global batch size:  1024 | lm loss: 1.862383E+00 | loss scale: 1.0 | grad norm: 0.277 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1535/   25000 | consumed samples:      1571840 | elapsed time per iteration (ms): 39373.4 | learning rate: 9.988E-05 | global batch size:  1024 | lm loss: 1.878700E+00 | loss scale: 1.0 | grad norm: 0.329 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1536/   25000 | consumed samples:      1572864 | elapsed time per iteration (ms): 39361.2 | learning rate: 9.988E-05 | global batch size:  1024 | lm loss: 1.861283E+00 | loss scale: 1.0 | grad norm: 0.296 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1537/   25000 | consumed samples:      1573888 | elapsed time per iteration (ms): 39527.3 | learning rate: 9.988E-05 | global batch size:  1024 | lm loss: 1.868007E+00 | loss scale: 1.0 | grad norm: 0.298 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1538/   25000 | consumed samples:      1574912 | elapsed time per iteration (ms): 39227.1 | learning rate: 9.988E-05 | global batch size:  1024 | lm loss: 1.861514E+00 | loss scale: 1.0 | grad norm: 0.338 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1539/   25000 | consumed samples:      1575936 | elapsed time per iteration (ms): 39222.0 | learning rate: 9.988E-05 | global batch size:  1024 | lm loss: 1.850616E+00 | loss scale: 1.0 | grad norm: 0.385 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1540/   25000 | consumed samples:      1576960 | elapsed time per iteration (ms): 39147.8 | learning rate: 9.988E-05 | global batch size:  1024 | lm loss: 1.860755E+00 | loss scale: 1.0 | grad norm: 0.383 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1541/   25000 | consumed samples:      1577984 | elapsed time per iteration (ms): 39505.8 | learning rate: 9.988E-05 | global batch size:  1024 | lm loss: 1.864036E+00 | loss scale: 1.0 | grad norm: 0.380 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1542/   25000 | consumed samples:      1579008 | elapsed time per iteration (ms): 39268.0 | learning rate: 9.988E-05 | global batch size:  1024 | lm loss: 1.864242E+00 | loss scale: 1.0 | grad norm: 0.300 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1543/   25000 | consumed samples:      1580032 | elapsed time per iteration (ms): 39364.8 | learning rate: 9.988E-05 | global batch size:  1024 | lm loss: 1.848116E+00 | loss scale: 1.0 | grad norm: 0.368 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1544/   25000 | consumed samples:      1581056 | elapsed time per iteration (ms): 39220.1 | learning rate: 9.988E-05 | global batch size:  1024 | lm loss: 1.858969E+00 | loss scale: 1.0 | grad norm: 0.359 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1545/   25000 | consumed samples:      1582080 | elapsed time per iteration (ms): 39153.1 | learning rate: 9.988E-05 | global batch size:  1024 | lm loss: 1.849112E+00 | loss scale: 1.0 | grad norm: 0.317 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1546/   25000 | consumed samples:      1583104 | elapsed time per iteration (ms): 39543.2 | learning rate: 9.988E-05 | global batch size:  1024 | lm loss: 1.866557E+00 | loss scale: 1.0 | grad norm: 0.337 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1547/   25000 | consumed samples:      1584128 | elapsed time per iteration (ms): 39284.2 | learning rate: 9.988E-05 | global batch size:  1024 | lm loss: 1.875285E+00 | loss scale: 1.0 | grad norm: 0.349 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1548/   25000 | consumed samples:      1585152 | elapsed time per iteration (ms): 39271.5 | learning rate: 9.988E-05 | global batch size:  1024 | lm loss: 1.853793E+00 | loss scale: 1.0 | grad norm: 0.335 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1549/   25000 | consumed samples:      1586176 | elapsed time per iteration (ms): 39303.5 | learning rate: 9.988E-05 | global batch size:  1024 | lm loss: 1.852656E+00 | loss scale: 1.0 | grad norm: 0.315 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1550/   25000 | consumed samples:      1587200 | elapsed time per iteration (ms): 39332.1 | learning rate: 9.988E-05 | global batch size:  1024 | lm loss: 1.847679E+00 | loss scale: 1.0 | grad norm: 0.307 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1551/   25000 | consumed samples:      1588224 | elapsed time per iteration (ms): 39389.9 | learning rate: 9.987E-05 | global batch size:  1024 | lm loss: 1.824866E+00 | loss scale: 1.0 | grad norm: 0.302 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1552/   25000 | consumed samples:      1589248 | elapsed time per iteration (ms): 39226.5 | learning rate: 9.987E-05 | global batch size:  1024 | lm loss: 1.868037E+00 | loss scale: 1.0 | grad norm: 0.355 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1553/   25000 | consumed samples:      1590272 | elapsed time per iteration (ms): 39334.3 | learning rate: 9.987E-05 | global batch size:  1024 | lm loss: 1.853102E+00 | loss scale: 1.0 | grad norm: 0.392 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1554/   25000 | consumed samples:      1591296 | elapsed time per iteration (ms): 39215.2 | learning rate: 9.987E-05 | global batch size:  1024 | lm loss: 1.864942E+00 | loss scale: 1.0 | grad norm: 0.336 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1555/   25000 | consumed samples:      1592320 | elapsed time per iteration (ms): 39535.0 | learning rate: 9.987E-05 | global batch size:  1024 | lm loss: 1.838627E+00 | loss scale: 1.0 | grad norm: 0.300 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1556/   25000 | consumed samples:      1593344 | elapsed time per iteration (ms): 39194.8 | learning rate: 9.987E-05 | global batch size:  1024 | lm loss: 1.861367E+00 | loss scale: 1.0 | grad norm: 0.306 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1557/   25000 | consumed samples:      1594368 | elapsed time per iteration (ms): 39352.2 | learning rate: 9.987E-05 | global batch size:  1024 | lm loss: 1.865160E+00 | loss scale: 1.0 | grad norm: 0.338 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1558/   25000 | consumed samples:      1595392 | elapsed time per iteration (ms): 39359.0 | learning rate: 9.987E-05 | global batch size:  1024 | lm loss: 1.850586E+00 | loss scale: 1.0 | grad norm: 0.317 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1559/   25000 | consumed samples:      1596416 | elapsed time per iteration (ms): 39467.3 | learning rate: 9.987E-05 | global batch size:  1024 | lm loss: 1.860434E+00 | loss scale: 1.0 | grad norm: 0.339 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1560/   25000 | consumed samples:      1597440 | elapsed time per iteration (ms): 39377.6 | learning rate: 9.987E-05 | global batch size:  1024 | lm loss: 1.878293E+00 | loss scale: 1.0 | grad norm: 0.463 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1561/   25000 | consumed samples:      1598464 | elapsed time per iteration (ms): 39179.7 | learning rate: 9.987E-05 | global batch size:  1024 | lm loss: 1.856623E+00 | loss scale: 1.0 | grad norm: 0.521 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1562/   25000 | consumed samples:      1599488 | elapsed time per iteration (ms): 39248.7 | learning rate: 9.987E-05 | global batch size:  1024 | lm loss: 1.867261E+00 | loss scale: 1.0 | grad norm: 0.369 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1563/   25000 | consumed samples:      1600512 | elapsed time per iteration (ms): 39317.3 | learning rate: 9.987E-05 | global batch size:  1024 | lm loss: 1.842635E+00 | loss scale: 1.0 | grad norm: 0.393 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1564/   25000 | consumed samples:      1601536 | elapsed time per iteration (ms): 39541.6 | learning rate: 9.987E-05 | global batch size:  1024 | lm loss: 1.852088E+00 | loss scale: 1.0 | grad norm: 0.444 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1565/   25000 | consumed samples:      1602560 | elapsed time per iteration (ms): 39397.8 | learning rate: 9.987E-05 | global batch size:  1024 | lm loss: 1.832783E+00 | loss scale: 1.0 | grad norm: 0.313 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1566/   25000 | consumed samples:      1603584 | elapsed time per iteration (ms): 39266.2 | learning rate: 9.987E-05 | global batch size:  1024 | lm loss: 1.855679E+00 | loss scale: 1.0 | grad norm: 0.419 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1567/   25000 | consumed samples:      1604608 | elapsed time per iteration (ms): 39342.9 | learning rate: 9.987E-05 | global batch size:  1024 | lm loss: 1.871335E+00 | loss scale: 1.0 | grad norm: 0.424 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1568/   25000 | consumed samples:      1605632 | elapsed time per iteration (ms): 39399.9 | learning rate: 9.987E-05 | global batch size:  1024 | lm loss: 1.844400E+00 | loss scale: 1.0 | grad norm: 0.299 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1569/   25000 | consumed samples:      1606656 | elapsed time per iteration (ms): 39362.6 | learning rate: 9.987E-05 | global batch size:  1024 | lm loss: 1.849164E+00 | loss scale: 1.0 | grad norm: 0.372 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1570/   25000 | consumed samples:      1607680 | elapsed time per iteration (ms): 39382.3 | learning rate: 9.987E-05 | global batch size:  1024 | lm loss: 1.852702E+00 | loss scale: 1.0 | grad norm: 0.354 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1571/   25000 | consumed samples:      1608704 | elapsed time per iteration (ms): 39253.5 | learning rate: 9.987E-05 | global batch size:  1024 | lm loss: 1.858959E+00 | loss scale: 1.0 | grad norm: 0.274 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1572/   25000 | consumed samples:      1609728 | elapsed time per iteration (ms): 39188.9 | learning rate: 9.987E-05 | global batch size:  1024 | lm loss: 1.851661E+00 | loss scale: 1.0 | grad norm: 0.364 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1573/   25000 | consumed samples:      1610752 | elapsed time per iteration (ms): 39351.1 | learning rate: 9.986E-05 | global batch size:  1024 | lm loss: 1.867569E+00 | loss scale: 1.0 | grad norm: 0.395 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1574/   25000 | consumed samples:      1611776 | elapsed time per iteration (ms): 39495.3 | learning rate: 9.986E-05 | global batch size:  1024 | lm loss: 1.866780E+00 | loss scale: 1.0 | grad norm: 0.389 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1575/   25000 | consumed samples:      1612800 | elapsed time per iteration (ms): 39375.8 | learning rate: 9.986E-05 | global batch size:  1024 | lm loss: 1.864725E+00 | loss scale: 1.0 | grad norm: 0.432 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1576/   25000 | consumed samples:      1613824 | elapsed time per iteration (ms): 39404.6 | learning rate: 9.986E-05 | global batch size:  1024 | lm loss: 1.856754E+00 | loss scale: 1.0 | grad norm: 0.438 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1577/   25000 | consumed samples:      1614848 | elapsed time per iteration (ms): 39243.8 | learning rate: 9.986E-05 | global batch size:  1024 | lm loss: 1.842362E+00 | loss scale: 1.0 | grad norm: 0.429 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1578/   25000 | consumed samples:      1615872 | elapsed time per iteration (ms): 39249.4 | learning rate: 9.986E-05 | global batch size:  1024 | lm loss: 1.860323E+00 | loss scale: 1.0 | grad norm: 0.458 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1579/   25000 | consumed samples:      1616896 | elapsed time per iteration (ms): 39344.2 | learning rate: 9.986E-05 | global batch size:  1024 | lm loss: 1.865757E+00 | loss scale: 1.0 | grad norm: 0.410 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1580/   25000 | consumed samples:      1617920 | elapsed time per iteration (ms): 39400.0 | learning rate: 9.986E-05 | global batch size:  1024 | lm loss: 1.851685E+00 | loss scale: 1.0 | grad norm: 0.310 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1581/   25000 | consumed samples:      1618944 | elapsed time per iteration (ms): 39179.1 | learning rate: 9.986E-05 | global batch size:  1024 | lm loss: 1.853387E+00 | loss scale: 1.0 | grad norm: 0.321 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1582/   25000 | consumed samples:      1619968 | elapsed time per iteration (ms): 39494.1 | learning rate: 9.986E-05 | global batch size:  1024 | lm loss: 1.861405E+00 | loss scale: 1.0 | grad norm: 0.333 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1583/   25000 | consumed samples:      1620992 | elapsed time per iteration (ms): 39188.9 | learning rate: 9.986E-05 | global batch size:  1024 | lm loss: 1.857533E+00 | loss scale: 1.0 | grad norm: 0.301 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1584/   25000 | consumed samples:      1622016 | elapsed time per iteration (ms): 39535.6 | learning rate: 9.986E-05 | global batch size:  1024 | lm loss: 1.859685E+00 | loss scale: 1.0 | grad norm: 0.303 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1585/   25000 | consumed samples:      1623040 | elapsed time per iteration (ms): 39389.7 | learning rate: 9.986E-05 | global batch size:  1024 | lm loss: 1.865038E+00 | loss scale: 1.0 | grad norm: 0.281 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1586/   25000 | consumed samples:      1624064 | elapsed time per iteration (ms): 39307.9 | learning rate: 9.986E-05 | global batch size:  1024 | lm loss: 1.860012E+00 | loss scale: 1.0 | grad norm: 0.263 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1587/   25000 | consumed samples:      1625088 | elapsed time per iteration (ms): 39256.8 | learning rate: 9.986E-05 | global batch size:  1024 | lm loss: 1.839122E+00 | loss scale: 1.0 | grad norm: 0.302 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1588/   25000 | consumed samples:      1626112 | elapsed time per iteration (ms): 39172.3 | learning rate: 9.986E-05 | global batch size:  1024 | lm loss: 1.852494E+00 | loss scale: 1.0 | grad norm: 0.297 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1589/   25000 | consumed samples:      1627136 | elapsed time per iteration (ms): 39173.4 | learning rate: 9.986E-05 | global batch size:  1024 | lm loss: 1.865506E+00 | loss scale: 1.0 | grad norm: 0.273 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1590/   25000 | consumed samples:      1628160 | elapsed time per iteration (ms): 39554.7 | learning rate: 9.986E-05 | global batch size:  1024 | lm loss: 1.867689E+00 | loss scale: 1.0 | grad norm: 0.302 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1591/   25000 | consumed samples:      1629184 | elapsed time per iteration (ms): 39332.0 | learning rate: 9.986E-05 | global batch size:  1024 | lm loss: 1.865303E+00 | loss scale: 1.0 | grad norm: 0.335 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1592/   25000 | consumed samples:      1630208 | elapsed time per iteration (ms): 39461.3 | learning rate: 9.986E-05 | global batch size:  1024 | lm loss: 1.852643E+00 | loss scale: 1.0 | grad norm: 0.407 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1593/   25000 | consumed samples:      1631232 | elapsed time per iteration (ms): 39167.1 | learning rate: 9.985E-05 | global batch size:  1024 | lm loss: 1.848248E+00 | loss scale: 1.0 | grad norm: 0.489 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1594/   25000 | consumed samples:      1632256 | elapsed time per iteration (ms): 39244.3 | learning rate: 9.985E-05 | global batch size:  1024 | lm loss: 1.856806E+00 | loss scale: 1.0 | grad norm: 0.525 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1595/   25000 | consumed samples:      1633280 | elapsed time per iteration (ms): 39606.5 | learning rate: 9.985E-05 | global batch size:  1024 | lm loss: 1.857367E+00 | loss scale: 1.0 | grad norm: 0.440 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1596/   25000 | consumed samples:      1634304 | elapsed time per iteration (ms): 39262.1 | learning rate: 9.985E-05 | global batch size:  1024 | lm loss: 1.857490E+00 | loss scale: 1.0 | grad norm: 0.310 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1597/   25000 | consumed samples:      1635328 | elapsed time per iteration (ms): 39172.9 | learning rate: 9.985E-05 | global batch size:  1024 | lm loss: 1.852244E+00 | loss scale: 1.0 | grad norm: 0.346 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1598/   25000 | consumed samples:      1636352 | elapsed time per iteration (ms): 39329.2 | learning rate: 9.985E-05 | global batch size:  1024 | lm loss: 1.857476E+00 | loss scale: 1.0 | grad norm: 0.376 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1599/   25000 | consumed samples:      1637376 | elapsed time per iteration (ms): 39305.4 | learning rate: 9.985E-05 | global batch size:  1024 | lm loss: 1.859588E+00 | loss scale: 1.0 | grad norm: 0.401 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1600/   25000 | consumed samples:      1638400 | elapsed time per iteration (ms): 39530.4 | learning rate: 9.985E-05 | global batch size:  1024 | lm loss: 1.852015E+00 | loss scale: 1.0 | grad norm: 0.387 | number of skipped iterations:   0 | number of nan iterations:   0 |
------------------------------------------------------------------------------------------------
 validation loss at iteration 1600 | lm loss value: 1.862551E+00 | lm loss PPL: 6.440143E+00 | 
------------------------------------------------------------------------------------------------
 iteration     1601/   25000 | consumed samples:      1639424 | elapsed time per iteration (ms): 174424.6 | learning rate: 9.985E-05 | global batch size:  1024 | lm loss: 1.851999E+00 | loss scale: 1.0 | grad norm: 0.310 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1602/   25000 | consumed samples:      1640448 | elapsed time per iteration (ms): 39254.2 | learning rate: 9.985E-05 | global batch size:  1024 | lm loss: 1.854825E+00 | loss scale: 1.0 | grad norm: 0.295 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1603/   25000 | consumed samples:      1641472 | elapsed time per iteration (ms): 39168.8 | learning rate: 9.985E-05 | global batch size:  1024 | lm loss: 1.850605E+00 | loss scale: 1.0 | grad norm: 0.312 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1604/   25000 | consumed samples:      1642496 | elapsed time per iteration (ms): 39378.1 | learning rate: 9.985E-05 | global batch size:  1024 | lm loss: 1.853196E+00 | loss scale: 1.0 | grad norm: 0.331 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1605/   25000 | consumed samples:      1643520 | elapsed time per iteration (ms): 39301.0 | learning rate: 9.985E-05 | global batch size:  1024 | lm loss: 1.849859E+00 | loss scale: 1.0 | grad norm: 0.324 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1606/   25000 | consumed samples:      1644544 | elapsed time per iteration (ms): 39586.6 | learning rate: 9.985E-05 | global batch size:  1024 | lm loss: 1.869472E+00 | loss scale: 1.0 | grad norm: 0.291 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1607/   25000 | consumed samples:      1645568 | elapsed time per iteration (ms): 39176.4 | learning rate: 9.985E-05 | global batch size:  1024 | lm loss: 1.852150E+00 | loss scale: 1.0 | grad norm: 0.289 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1608/   25000 | consumed samples:      1646592 | elapsed time per iteration (ms): 39468.9 | learning rate: 9.985E-05 | global batch size:  1024 | lm loss: 1.835876E+00 | loss scale: 1.0 | grad norm: 0.327 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1609/   25000 | consumed samples:      1647616 | elapsed time per iteration (ms): 39385.5 | learning rate: 9.985E-05 | global batch size:  1024 | lm loss: 1.854914E+00 | loss scale: 1.0 | grad norm: 0.300 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1610/   25000 | consumed samples:      1648640 | elapsed time per iteration (ms): 39337.3 | learning rate: 9.985E-05 | global batch size:  1024 | lm loss: 1.851080E+00 | loss scale: 1.0 | grad norm: 0.279 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1611/   25000 | consumed samples:      1649664 | elapsed time per iteration (ms): 39365.9 | learning rate: 9.985E-05 | global batch size:  1024 | lm loss: 1.845644E+00 | loss scale: 1.0 | grad norm: 0.313 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1612/   25000 | consumed samples:      1650688 | elapsed time per iteration (ms): 39252.7 | learning rate: 9.985E-05 | global batch size:  1024 | lm loss: 1.832101E+00 | loss scale: 1.0 | grad norm: 0.335 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1613/   25000 | consumed samples:      1651712 | elapsed time per iteration (ms): 39184.4 | learning rate: 9.984E-05 | global batch size:  1024 | lm loss: 1.848764E+00 | loss scale: 1.0 | grad norm: 0.363 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1614/   25000 | consumed samples:      1652736 | elapsed time per iteration (ms): 39535.5 | learning rate: 9.984E-05 | global batch size:  1024 | lm loss: 1.852092E+00 | loss scale: 1.0 | grad norm: 0.391 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1615/   25000 | consumed samples:      1653760 | elapsed time per iteration (ms): 39184.2 | learning rate: 9.984E-05 | global batch size:  1024 | lm loss: 1.854389E+00 | loss scale: 1.0 | grad norm: 0.421 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1616/   25000 | consumed samples:      1654784 | elapsed time per iteration (ms): 39666.6 | learning rate: 9.984E-05 | global batch size:  1024 | lm loss: 1.851836E+00 | loss scale: 1.0 | grad norm: 0.407 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1617/   25000 | consumed samples:      1655808 | elapsed time per iteration (ms): 39263.2 | learning rate: 9.984E-05 | global batch size:  1024 | lm loss: 1.836964E+00 | loss scale: 1.0 | grad norm: 0.306 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1618/   25000 | consumed samples:      1656832 | elapsed time per iteration (ms): 39242.9 | learning rate: 9.984E-05 | global batch size:  1024 | lm loss: 1.853380E+00 | loss scale: 1.0 | grad norm: 0.354 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1619/   25000 | consumed samples:      1657856 | elapsed time per iteration (ms): 39467.2 | learning rate: 9.984E-05 | global batch size:  1024 | lm loss: 1.844847E+00 | loss scale: 1.0 | grad norm: 0.444 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1620/   25000 | consumed samples:      1658880 | elapsed time per iteration (ms): 39274.6 | learning rate: 9.984E-05 | global batch size:  1024 | lm loss: 1.856246E+00 | loss scale: 1.0 | grad norm: 0.383 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1621/   25000 | consumed samples:      1659904 | elapsed time per iteration (ms): 39238.4 | learning rate: 9.984E-05 | global batch size:  1024 | lm loss: 1.861724E+00 | loss scale: 1.0 | grad norm: 0.313 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1622/   25000 | consumed samples:      1660928 | elapsed time per iteration (ms): 39450.0 | learning rate: 9.984E-05 | global batch size:  1024 | lm loss: 1.852525E+00 | loss scale: 1.0 | grad norm: 0.333 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1623/   25000 | consumed samples:      1661952 | elapsed time per iteration (ms): 39249.0 | learning rate: 9.984E-05 | global batch size:  1024 | lm loss: 1.860855E+00 | loss scale: 1.0 | grad norm: 0.326 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1624/   25000 | consumed samples:      1662976 | elapsed time per iteration (ms): 39605.1 | learning rate: 9.984E-05 | global batch size:  1024 | lm loss: 1.849743E+00 | loss scale: 1.0 | grad norm: 0.304 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1625/   25000 | consumed samples:      1664000 | elapsed time per iteration (ms): 39185.6 | learning rate: 9.984E-05 | global batch size:  1024 | lm loss: 1.841770E+00 | loss scale: 1.0 | grad norm: 0.327 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1626/   25000 | consumed samples:      1665024 | elapsed time per iteration (ms): 39265.9 | learning rate: 9.984E-05 | global batch size:  1024 | lm loss: 1.848254E+00 | loss scale: 1.0 | grad norm: 0.303 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1627/   25000 | consumed samples:      1666048 | elapsed time per iteration (ms): 39387.2 | learning rate: 9.984E-05 | global batch size:  1024 | lm loss: 1.855700E+00 | loss scale: 1.0 | grad norm: 0.286 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1628/   25000 | consumed samples:      1667072 | elapsed time per iteration (ms): 39433.5 | learning rate: 9.984E-05 | global batch size:  1024 | lm loss: 1.842616E+00 | loss scale: 1.0 | grad norm: 0.297 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1629/   25000 | consumed samples:      1668096 | elapsed time per iteration (ms): 39318.5 | learning rate: 9.984E-05 | global batch size:  1024 | lm loss: 1.851077E+00 | loss scale: 1.0 | grad norm: 0.286 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1630/   25000 | consumed samples:      1669120 | elapsed time per iteration (ms): 39344.2 | learning rate: 9.984E-05 | global batch size:  1024 | lm loss: 1.850489E+00 | loss scale: 1.0 | grad norm: 0.329 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1631/   25000 | consumed samples:      1670144 | elapsed time per iteration (ms): 39185.2 | learning rate: 9.984E-05 | global batch size:  1024 | lm loss: 1.862183E+00 | loss scale: 1.0 | grad norm: 0.380 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1632/   25000 | consumed samples:      1671168 | elapsed time per iteration (ms): 39596.3 | learning rate: 9.984E-05 | global batch size:  1024 | lm loss: 1.845429E+00 | loss scale: 1.0 | grad norm: 0.401 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1633/   25000 | consumed samples:      1672192 | elapsed time per iteration (ms): 39265.0 | learning rate: 9.983E-05 | global batch size:  1024 | lm loss: 1.845870E+00 | loss scale: 1.0 | grad norm: 0.377 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1634/   25000 | consumed samples:      1673216 | elapsed time per iteration (ms): 39392.5 | learning rate: 9.983E-05 | global batch size:  1024 | lm loss: 1.868548E+00 | loss scale: 1.0 | grad norm: 0.389 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1635/   25000 | consumed samples:      1674240 | elapsed time per iteration (ms): 39190.0 | learning rate: 9.983E-05 | global batch size:  1024 | lm loss: 1.854797E+00 | loss scale: 1.0 | grad norm: 0.404 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1636/   25000 | consumed samples:      1675264 | elapsed time per iteration (ms): 39263.4 | learning rate: 9.983E-05 | global batch size:  1024 | lm loss: 1.844353E+00 | loss scale: 1.0 | grad norm: 0.377 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1637/   25000 | consumed samples:      1676288 | elapsed time per iteration (ms): 39400.1 | learning rate: 9.983E-05 | global batch size:  1024 | lm loss: 1.846736E+00 | loss scale: 1.0 | grad norm: 0.329 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1638/   25000 | consumed samples:      1677312 | elapsed time per iteration (ms): 39455.8 | learning rate: 9.983E-05 | global batch size:  1024 | lm loss: 1.851906E+00 | loss scale: 1.0 | grad norm: 0.274 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1639/   25000 | consumed samples:      1678336 | elapsed time per iteration (ms): 39309.0 | learning rate: 9.983E-05 | global batch size:  1024 | lm loss: 1.861245E+00 | loss scale: 1.0 | grad norm: 0.288 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1640/   25000 | consumed samples:      1679360 | elapsed time per iteration (ms): 39401.6 | learning rate: 9.983E-05 | global batch size:  1024 | lm loss: 1.842569E+00 | loss scale: 1.0 | grad norm: 0.317 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1641/   25000 | consumed samples:      1680384 | elapsed time per iteration (ms): 39319.0 | learning rate: 9.983E-05 | global batch size:  1024 | lm loss: 1.838929E+00 | loss scale: 1.0 | grad norm: 0.352 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1642/   25000 | consumed samples:      1681408 | elapsed time per iteration (ms): 39266.7 | learning rate: 9.983E-05 | global batch size:  1024 | lm loss: 1.850894E+00 | loss scale: 1.0 | grad norm: 0.412 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1643/   25000 | consumed samples:      1682432 | elapsed time per iteration (ms): 39355.3 | learning rate: 9.983E-05 | global batch size:  1024 | lm loss: 1.846065E+00 | loss scale: 1.0 | grad norm: 0.370 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1644/   25000 | consumed samples:      1683456 | elapsed time per iteration (ms): 39381.2 | learning rate: 9.983E-05 | global batch size:  1024 | lm loss: 1.844385E+00 | loss scale: 1.0 | grad norm: 0.333 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1645/   25000 | consumed samples:      1684480 | elapsed time per iteration (ms): 39191.8 | learning rate: 9.983E-05 | global batch size:  1024 | lm loss: 1.851665E+00 | loss scale: 1.0 | grad norm: 0.335 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1646/   25000 | consumed samples:      1685504 | elapsed time per iteration (ms): 39496.4 | learning rate: 9.983E-05 | global batch size:  1024 | lm loss: 1.849313E+00 | loss scale: 1.0 | grad norm: 0.323 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1647/   25000 | consumed samples:      1686528 | elapsed time per iteration (ms): 39299.1 | learning rate: 9.983E-05 | global batch size:  1024 | lm loss: 1.847084E+00 | loss scale: 1.0 | grad norm: 0.304 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1648/   25000 | consumed samples:      1687552 | elapsed time per iteration (ms): 39736.3 | learning rate: 9.983E-05 | global batch size:  1024 | lm loss: 1.838446E+00 | loss scale: 1.0 | grad norm: 0.285 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1649/   25000 | consumed samples:      1688576 | elapsed time per iteration (ms): 39266.5 | learning rate: 9.983E-05 | global batch size:  1024 | lm loss: 1.844437E+00 | loss scale: 1.0 | grad norm: 0.274 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1650/   25000 | consumed samples:      1689600 | elapsed time per iteration (ms): 39344.9 | learning rate: 9.983E-05 | global batch size:  1024 | lm loss: 1.828973E+00 | loss scale: 1.0 | grad norm: 0.268 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1651/   25000 | consumed samples:      1690624 | elapsed time per iteration (ms): 39188.5 | learning rate: 9.983E-05 | global batch size:  1024 | lm loss: 1.840775E+00 | loss scale: 1.0 | grad norm: 0.270 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1652/   25000 | consumed samples:      1691648 | elapsed time per iteration (ms): 39279.0 | learning rate: 9.982E-05 | global batch size:  1024 | lm loss: 1.839402E+00 | loss scale: 1.0 | grad norm: 0.290 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1653/   25000 | consumed samples:      1692672 | elapsed time per iteration (ms): 39390.5 | learning rate: 9.982E-05 | global batch size:  1024 | lm loss: 1.852041E+00 | loss scale: 1.0 | grad norm: 0.315 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1654/   25000 | consumed samples:      1693696 | elapsed time per iteration (ms): 39480.0 | learning rate: 9.982E-05 | global batch size:  1024 | lm loss: 1.865461E+00 | loss scale: 1.0 | grad norm: 0.342 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1655/   25000 | consumed samples:      1694720 | elapsed time per iteration (ms): 39360.5 | learning rate: 9.982E-05 | global batch size:  1024 | lm loss: 1.841481E+00 | loss scale: 1.0 | grad norm: 0.347 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1656/   25000 | consumed samples:      1695744 | elapsed time per iteration (ms): 39425.8 | learning rate: 9.982E-05 | global batch size:  1024 | lm loss: 1.856416E+00 | loss scale: 1.0 | grad norm: 0.325 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1657/   25000 | consumed samples:      1696768 | elapsed time per iteration (ms): 39286.8 | learning rate: 9.982E-05 | global batch size:  1024 | lm loss: 1.842405E+00 | loss scale: 1.0 | grad norm: 0.295 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1658/   25000 | consumed samples:      1697792 | elapsed time per iteration (ms): 39433.8 | learning rate: 9.982E-05 | global batch size:  1024 | lm loss: 1.857670E+00 | loss scale: 1.0 | grad norm: 0.309 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1659/   25000 | consumed samples:      1698816 | elapsed time per iteration (ms): 39456.7 | learning rate: 9.982E-05 | global batch size:  1024 | lm loss: 1.855213E+00 | loss scale: 1.0 | grad norm: 0.353 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1660/   25000 | consumed samples:      1699840 | elapsed time per iteration (ms): 39280.5 | learning rate: 9.982E-05 | global batch size:  1024 | lm loss: 1.846993E+00 | loss scale: 1.0 | grad norm: 0.389 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1661/   25000 | consumed samples:      1700864 | elapsed time per iteration (ms): 39217.4 | learning rate: 9.982E-05 | global batch size:  1024 | lm loss: 1.852987E+00 | loss scale: 1.0 | grad norm: 0.429 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1662/   25000 | consumed samples:      1701888 | elapsed time per iteration (ms): 39373.7 | learning rate: 9.982E-05 | global batch size:  1024 | lm loss: 1.852563E+00 | loss scale: 1.0 | grad norm: 0.450 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1663/   25000 | consumed samples:      1702912 | elapsed time per iteration (ms): 39336.8 | learning rate: 9.982E-05 | global batch size:  1024 | lm loss: 1.831592E+00 | loss scale: 1.0 | grad norm: 0.394 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1664/   25000 | consumed samples:      1703936 | elapsed time per iteration (ms): 39764.1 | learning rate: 9.982E-05 | global batch size:  1024 | lm loss: 1.846549E+00 | loss scale: 1.0 | grad norm: 0.350 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1665/   25000 | consumed samples:      1704960 | elapsed time per iteration (ms): 39277.9 | learning rate: 9.982E-05 | global batch size:  1024 | lm loss: 1.833959E+00 | loss scale: 1.0 | grad norm: 0.416 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1666/   25000 | consumed samples:      1705984 | elapsed time per iteration (ms): 39212.3 | learning rate: 9.982E-05 | global batch size:  1024 | lm loss: 1.841963E+00 | loss scale: 1.0 | grad norm: 0.417 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1667/   25000 | consumed samples:      1707008 | elapsed time per iteration (ms): 39291.2 | learning rate: 9.982E-05 | global batch size:  1024 | lm loss: 1.836328E+00 | loss scale: 1.0 | grad norm: 0.276 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1668/   25000 | consumed samples:      1708032 | elapsed time per iteration (ms): 39473.1 | learning rate: 9.982E-05 | global batch size:  1024 | lm loss: 1.836179E+00 | loss scale: 1.0 | grad norm: 0.266 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1669/   25000 | consumed samples:      1709056 | elapsed time per iteration (ms): 39324.6 | learning rate: 9.982E-05 | global batch size:  1024 | lm loss: 1.841561E+00 | loss scale: 1.0 | grad norm: 0.338 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1670/   25000 | consumed samples:      1710080 | elapsed time per iteration (ms): 39357.3 | learning rate: 9.981E-05 | global batch size:  1024 | lm loss: 1.862200E+00 | loss scale: 1.0 | grad norm: 0.291 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1671/   25000 | consumed samples:      1711104 | elapsed time per iteration (ms): 39272.7 | learning rate: 9.981E-05 | global batch size:  1024 | lm loss: 1.854403E+00 | loss scale: 1.0 | grad norm: 0.293 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1672/   25000 | consumed samples:      1712128 | elapsed time per iteration (ms): 39348.1 | learning rate: 9.981E-05 | global batch size:  1024 | lm loss: 1.832850E+00 | loss scale: 1.0 | grad norm: 0.319 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1673/   25000 | consumed samples:      1713152 | elapsed time per iteration (ms): 39661.0 | learning rate: 9.981E-05 | global batch size:  1024 | lm loss: 1.840804E+00 | loss scale: 1.0 | grad norm: 0.333 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1674/   25000 | consumed samples:      1714176 | elapsed time per iteration (ms): 39265.2 | learning rate: 9.981E-05 | global batch size:  1024 | lm loss: 1.864011E+00 | loss scale: 1.0 | grad norm: 0.310 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1675/   25000 | consumed samples:      1715200 | elapsed time per iteration (ms): 39403.1 | learning rate: 9.981E-05 | global batch size:  1024 | lm loss: 1.855932E+00 | loss scale: 1.0 | grad norm: 0.296 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1676/   25000 | consumed samples:      1716224 | elapsed time per iteration (ms): 39279.5 | learning rate: 9.981E-05 | global batch size:  1024 | lm loss: 1.835203E+00 | loss scale: 1.0 | grad norm: 0.321 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1677/   25000 | consumed samples:      1717248 | elapsed time per iteration (ms): 39262.1 | learning rate: 9.981E-05 | global batch size:  1024 | lm loss: 1.844488E+00 | loss scale: 1.0 | grad norm: 0.330 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1678/   25000 | consumed samples:      1718272 | elapsed time per iteration (ms): 39387.2 | learning rate: 9.981E-05 | global batch size:  1024 | lm loss: 1.836887E+00 | loss scale: 1.0 | grad norm: 0.356 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1679/   25000 | consumed samples:      1719296 | elapsed time per iteration (ms): 39256.0 | learning rate: 9.981E-05 | global batch size:  1024 | lm loss: 1.840857E+00 | loss scale: 1.0 | grad norm: 0.333 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1680/   25000 | consumed samples:      1720320 | elapsed time per iteration (ms): 39445.5 | learning rate: 9.981E-05 | global batch size:  1024 | lm loss: 1.841971E+00 | loss scale: 1.0 | grad norm: 0.296 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1681/   25000 | consumed samples:      1721344 | elapsed time per iteration (ms): 39427.9 | learning rate: 9.981E-05 | global batch size:  1024 | lm loss: 1.841918E+00 | loss scale: 1.0 | grad norm: 0.326 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1682/   25000 | consumed samples:      1722368 | elapsed time per iteration (ms): 39366.1 | learning rate: 9.981E-05 | global batch size:  1024 | lm loss: 1.850477E+00 | loss scale: 1.0 | grad norm: 0.372 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1683/   25000 | consumed samples:      1723392 | elapsed time per iteration (ms): 39389.6 | learning rate: 9.981E-05 | global batch size:  1024 | lm loss: 1.858834E+00 | loss scale: 1.0 | grad norm: 0.408 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1684/   25000 | consumed samples:      1724416 | elapsed time per iteration (ms): 39275.2 | learning rate: 9.981E-05 | global batch size:  1024 | lm loss: 1.848965E+00 | loss scale: 1.0 | grad norm: 0.376 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1685/   25000 | consumed samples:      1725440 | elapsed time per iteration (ms): 39378.5 | learning rate: 9.981E-05 | global batch size:  1024 | lm loss: 1.856242E+00 | loss scale: 1.0 | grad norm: 0.323 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1686/   25000 | consumed samples:      1726464 | elapsed time per iteration (ms): 39338.7 | learning rate: 9.981E-05 | global batch size:  1024 | lm loss: 1.839195E+00 | loss scale: 1.0 | grad norm: 0.316 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1687/   25000 | consumed samples:      1727488 | elapsed time per iteration (ms): 39281.6 | learning rate: 9.981E-05 | global batch size:  1024 | lm loss: 1.846316E+00 | loss scale: 1.0 | grad norm: 0.282 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1688/   25000 | consumed samples:      1728512 | elapsed time per iteration (ms): 39322.7 | learning rate: 9.980E-05 | global batch size:  1024 | lm loss: 1.840105E+00 | loss scale: 1.0 | grad norm: 0.292 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1689/   25000 | consumed samples:      1729536 | elapsed time per iteration (ms): 39511.9 | learning rate: 9.980E-05 | global batch size:  1024 | lm loss: 1.841222E+00 | loss scale: 1.0 | grad norm: 0.287 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1690/   25000 | consumed samples:      1730560 | elapsed time per iteration (ms): 39315.5 | learning rate: 9.980E-05 | global batch size:  1024 | lm loss: 1.844031E+00 | loss scale: 1.0 | grad norm: 0.288 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1691/   25000 | consumed samples:      1731584 | elapsed time per iteration (ms): 39487.3 | learning rate: 9.980E-05 | global batch size:  1024 | lm loss: 1.846634E+00 | loss scale: 1.0 | grad norm: 0.363 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1692/   25000 | consumed samples:      1732608 | elapsed time per iteration (ms): 39366.6 | learning rate: 9.980E-05 | global batch size:  1024 | lm loss: 1.825104E+00 | loss scale: 1.0 | grad norm: 0.435 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1693/   25000 | consumed samples:      1733632 | elapsed time per iteration (ms): 39314.2 | learning rate: 9.980E-05 | global batch size:  1024 | lm loss: 1.842621E+00 | loss scale: 1.0 | grad norm: 0.473 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1694/   25000 | consumed samples:      1734656 | elapsed time per iteration (ms): 39274.1 | learning rate: 9.980E-05 | global batch size:  1024 | lm loss: 1.857767E+00 | loss scale: 1.0 | grad norm: 0.475 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1695/   25000 | consumed samples:      1735680 | elapsed time per iteration (ms): 39343.6 | learning rate: 9.980E-05 | global batch size:  1024 | lm loss: 1.843676E+00 | loss scale: 1.0 | grad norm: 0.420 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1696/   25000 | consumed samples:      1736704 | elapsed time per iteration (ms): 39390.6 | learning rate: 9.980E-05 | global batch size:  1024 | lm loss: 1.841326E+00 | loss scale: 1.0 | grad norm: 0.455 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1697/   25000 | consumed samples:      1737728 | elapsed time per iteration (ms): 39499.5 | learning rate: 9.980E-05 | global batch size:  1024 | lm loss: 1.846847E+00 | loss scale: 1.0 | grad norm: 0.376 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1698/   25000 | consumed samples:      1738752 | elapsed time per iteration (ms): 39370.8 | learning rate: 9.980E-05 | global batch size:  1024 | lm loss: 1.822695E+00 | loss scale: 1.0 | grad norm: 0.352 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1699/   25000 | consumed samples:      1739776 | elapsed time per iteration (ms): 39283.5 | learning rate: 9.980E-05 | global batch size:  1024 | lm loss: 1.846676E+00 | loss scale: 1.0 | grad norm: 0.354 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1700/   25000 | consumed samples:      1740800 | elapsed time per iteration (ms): 39271.8 | learning rate: 9.980E-05 | global batch size:  1024 | lm loss: 1.826609E+00 | loss scale: 1.0 | grad norm: 0.311 | number of skipped iterations:   0 | number of nan iterations:   0 |
------------------------------------------------------------------------------------------------
 validation loss at iteration 1700 | lm loss value: 1.851438E+00 | lm loss PPL: 6.368969E+00 | 
------------------------------------------------------------------------------------------------
 iteration     1701/   25000 | consumed samples:      1741824 | elapsed time per iteration (ms): 175224.6 | learning rate: 9.980E-05 | global batch size:  1024 | lm loss: 1.835501E+00 | loss scale: 1.0 | grad norm: 0.310 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1702/   25000 | consumed samples:      1742848 | elapsed time per iteration (ms): 39408.9 | learning rate: 9.980E-05 | global batch size:  1024 | lm loss: 1.845702E+00 | loss scale: 1.0 | grad norm: 0.296 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1703/   25000 | consumed samples:      1743872 | elapsed time per iteration (ms): 39293.1 | learning rate: 9.980E-05 | global batch size:  1024 | lm loss: 1.853801E+00 | loss scale: 1.0 | grad norm: 0.280 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1704/   25000 | consumed samples:      1744896 | elapsed time per iteration (ms): 39209.0 | learning rate: 9.980E-05 | global batch size:  1024 | lm loss: 1.839666E+00 | loss scale: 1.0 | grad norm: 0.306 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1705/   25000 | consumed samples:      1745920 | elapsed time per iteration (ms): 39638.0 | learning rate: 9.979E-05 | global batch size:  1024 | lm loss: 1.832049E+00 | loss scale: 1.0 | grad norm: 0.295 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1706/   25000 | consumed samples:      1746944 | elapsed time per iteration (ms): 39337.3 | learning rate: 9.979E-05 | global batch size:  1024 | lm loss: 1.859361E+00 | loss scale: 1.0 | grad norm: 0.277 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1707/   25000 | consumed samples:      1747968 | elapsed time per iteration (ms): 39474.7 | learning rate: 9.979E-05 | global batch size:  1024 | lm loss: 1.828277E+00 | loss scale: 1.0 | grad norm: 0.279 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1708/   25000 | consumed samples:      1748992 | elapsed time per iteration (ms): 39272.6 | learning rate: 9.979E-05 | global batch size:  1024 | lm loss: 1.843124E+00 | loss scale: 1.0 | grad norm: 0.269 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1709/   25000 | consumed samples:      1750016 | elapsed time per iteration (ms): 39219.8 | learning rate: 9.979E-05 | global batch size:  1024 | lm loss: 1.862341E+00 | loss scale: 1.0 | grad norm: 0.261 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1710/   25000 | consumed samples:      1751040 | elapsed time per iteration (ms): 39470.8 | learning rate: 9.979E-05 | global batch size:  1024 | lm loss: 1.840314E+00 | loss scale: 1.0 | grad norm: 0.248 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1711/   25000 | consumed samples:      1752064 | elapsed time per iteration (ms): 39295.0 | learning rate: 9.979E-05 | global batch size:  1024 | lm loss: 1.841180E+00 | loss scale: 1.0 | grad norm: 0.307 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1712/   25000 | consumed samples:      1753088 | elapsed time per iteration (ms): 39538.9 | learning rate: 9.979E-05 | global batch size:  1024 | lm loss: 1.836917E+00 | loss scale: 1.0 | grad norm: 0.331 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1713/   25000 | consumed samples:      1754112 | elapsed time per iteration (ms): 39511.9 | learning rate: 9.979E-05 | global batch size:  1024 | lm loss: 1.842616E+00 | loss scale: 1.0 | grad norm: 0.274 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1714/   25000 | consumed samples:      1755136 | elapsed time per iteration (ms): 39295.9 | learning rate: 9.979E-05 | global batch size:  1024 | lm loss: 1.848971E+00 | loss scale: 1.0 | grad norm: 0.316 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1715/   25000 | consumed samples:      1756160 | elapsed time per iteration (ms): 39320.3 | learning rate: 9.979E-05 | global batch size:  1024 | lm loss: 1.832700E+00 | loss scale: 1.0 | grad norm: 0.353 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1716/   25000 | consumed samples:      1757184 | elapsed time per iteration (ms): 39223.7 | learning rate: 9.979E-05 | global batch size:  1024 | lm loss: 1.832194E+00 | loss scale: 1.0 | grad norm: 0.309 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1717/   25000 | consumed samples:      1758208 | elapsed time per iteration (ms): 39615.2 | learning rate: 9.979E-05 | global batch size:  1024 | lm loss: 1.833180E+00 | loss scale: 1.0 | grad norm: 0.292 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1718/   25000 | consumed samples:      1759232 | elapsed time per iteration (ms): 39360.2 | learning rate: 9.979E-05 | global batch size:  1024 | lm loss: 1.829986E+00 | loss scale: 1.0 | grad norm: 0.338 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1719/   25000 | consumed samples:      1760256 | elapsed time per iteration (ms): 39463.9 | learning rate: 9.979E-05 | global batch size:  1024 | lm loss: 1.826584E+00 | loss scale: 1.0 | grad norm: 0.372 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1720/   25000 | consumed samples:      1761280 | elapsed time per iteration (ms): 39228.0 | learning rate: 9.979E-05 | global batch size:  1024 | lm loss: 1.833775E+00 | loss scale: 1.0 | grad norm: 0.391 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1721/   25000 | consumed samples:      1762304 | elapsed time per iteration (ms): 39507.5 | learning rate: 9.979E-05 | global batch size:  1024 | lm loss: 1.838716E+00 | loss scale: 1.0 | grad norm: 0.391 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1722/   25000 | consumed samples:      1763328 | elapsed time per iteration (ms): 39467.0 | learning rate: 9.978E-05 | global batch size:  1024 | lm loss: 1.826401E+00 | loss scale: 1.0 | grad norm: 0.376 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1723/   25000 | consumed samples:      1764352 | elapsed time per iteration (ms): 39443.7 | learning rate: 9.978E-05 | global batch size:  1024 | lm loss: 1.848158E+00 | loss scale: 1.0 | grad norm: 0.439 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1724/   25000 | consumed samples:      1765376 | elapsed time per iteration (ms): 39241.9 | learning rate: 9.978E-05 | global batch size:  1024 | lm loss: 1.834326E+00 | loss scale: 1.0 | grad norm: 0.473 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1725/   25000 | consumed samples:      1766400 | elapsed time per iteration (ms): 39285.1 | learning rate: 9.978E-05 | global batch size:  1024 | lm loss: 1.837487E+00 | loss scale: 1.0 | grad norm: 0.401 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1726/   25000 | consumed samples:      1767424 | elapsed time per iteration (ms): 39237.2 | learning rate: 9.978E-05 | global batch size:  1024 | lm loss: 1.844689E+00 | loss scale: 1.0 | grad norm: 0.392 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1727/   25000 | consumed samples:      1768448 | elapsed time per iteration (ms): 39537.7 | learning rate: 9.978E-05 | global batch size:  1024 | lm loss: 1.840672E+00 | loss scale: 1.0 | grad norm: 0.327 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1728/   25000 | consumed samples:      1769472 | elapsed time per iteration (ms): 39597.6 | learning rate: 9.978E-05 | global batch size:  1024 | lm loss: 1.840856E+00 | loss scale: 1.0 | grad norm: 0.331 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1729/   25000 | consumed samples:      1770496 | elapsed time per iteration (ms): 39507.3 | learning rate: 9.978E-05 | global batch size:  1024 | lm loss: 1.833911E+00 | loss scale: 1.0 | grad norm: 0.378 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1730/   25000 | consumed samples:      1771520 | elapsed time per iteration (ms): 39236.5 | learning rate: 9.978E-05 | global batch size:  1024 | lm loss: 1.839801E+00 | loss scale: 1.0 | grad norm: 0.371 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1731/   25000 | consumed samples:      1772544 | elapsed time per iteration (ms): 39299.3 | learning rate: 9.978E-05 | global batch size:  1024 | lm loss: 1.840553E+00 | loss scale: 1.0 | grad norm: 0.287 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1732/   25000 | consumed samples:      1773568 | elapsed time per iteration (ms): 39417.8 | learning rate: 9.978E-05 | global batch size:  1024 | lm loss: 1.848193E+00 | loss scale: 1.0 | grad norm: 0.280 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1733/   25000 | consumed samples:      1774592 | elapsed time per iteration (ms): 39490.2 | learning rate: 9.978E-05 | global batch size:  1024 | lm loss: 1.854052E+00 | loss scale: 1.0 | grad norm: 0.342 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1734/   25000 | consumed samples:      1775616 | elapsed time per iteration (ms): 39235.7 | learning rate: 9.978E-05 | global batch size:  1024 | lm loss: 1.847513E+00 | loss scale: 1.0 | grad norm: 0.352 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1735/   25000 | consumed samples:      1776640 | elapsed time per iteration (ms): 39385.0 | learning rate: 9.978E-05 | global batch size:  1024 | lm loss: 1.812231E+00 | loss scale: 1.0 | grad norm: 0.297 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1736/   25000 | consumed samples:      1777664 | elapsed time per iteration (ms): 39228.5 | learning rate: 9.978E-05 | global batch size:  1024 | lm loss: 1.850872E+00 | loss scale: 1.0 | grad norm: 0.270 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1737/   25000 | consumed samples:      1778688 | elapsed time per iteration (ms): 39669.3 | learning rate: 9.978E-05 | global batch size:  1024 | lm loss: 1.830844E+00 | loss scale: 1.0 | grad norm: 0.276 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1738/   25000 | consumed samples:      1779712 | elapsed time per iteration (ms): 39409.7 | learning rate: 9.978E-05 | global batch size:  1024 | lm loss: 1.833273E+00 | loss scale: 1.0 | grad norm: 0.296 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1739/   25000 | consumed samples:      1780736 | elapsed time per iteration (ms): 39375.8 | learning rate: 9.977E-05 | global batch size:  1024 | lm loss: 1.837003E+00 | loss scale: 1.0 | grad norm: 0.320 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1740/   25000 | consumed samples:      1781760 | elapsed time per iteration (ms): 39219.7 | learning rate: 9.977E-05 | global batch size:  1024 | lm loss: 1.842304E+00 | loss scale: 1.0 | grad norm: 0.313 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1741/   25000 | consumed samples:      1782784 | elapsed time per iteration (ms): 39361.6 | learning rate: 9.977E-05 | global batch size:  1024 | lm loss: 1.836156E+00 | loss scale: 1.0 | grad norm: 0.290 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1742/   25000 | consumed samples:      1783808 | elapsed time per iteration (ms): 39350.7 | learning rate: 9.977E-05 | global batch size:  1024 | lm loss: 1.832928E+00 | loss scale: 1.0 | grad norm: 0.294 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1743/   25000 | consumed samples:      1784832 | elapsed time per iteration (ms): 39422.1 | learning rate: 9.977E-05 | global batch size:  1024 | lm loss: 1.842018E+00 | loss scale: 1.0 | grad norm: 0.270 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1744/   25000 | consumed samples:      1785856 | elapsed time per iteration (ms): 39317.0 | learning rate: 9.977E-05 | global batch size:  1024 | lm loss: 1.831653E+00 | loss scale: 1.0 | grad norm: 0.263 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1745/   25000 | consumed samples:      1786880 | elapsed time per iteration (ms): 39381.1 | learning rate: 9.977E-05 | global batch size:  1024 | lm loss: 1.841848E+00 | loss scale: 1.0 | grad norm: 0.305 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1746/   25000 | consumed samples:      1787904 | elapsed time per iteration (ms): 39418.6 | learning rate: 9.977E-05 | global batch size:  1024 | lm loss: 1.825031E+00 | loss scale: 1.0 | grad norm: 0.387 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1747/   25000 | consumed samples:      1788928 | elapsed time per iteration (ms): 39432.9 | learning rate: 9.977E-05 | global batch size:  1024 | lm loss: 1.827802E+00 | loss scale: 1.0 | grad norm: 0.394 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1748/   25000 | consumed samples:      1789952 | elapsed time per iteration (ms): 39178.6 | learning rate: 9.977E-05 | global batch size:  1024 | lm loss: 1.821362E+00 | loss scale: 1.0 | grad norm: 0.395 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1749/   25000 | consumed samples:      1790976 | elapsed time per iteration (ms): 39363.3 | learning rate: 9.977E-05 | global batch size:  1024 | lm loss: 1.823997E+00 | loss scale: 1.0 | grad norm: 0.373 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1750/   25000 | consumed samples:      1792000 | elapsed time per iteration (ms): 39244.7 | learning rate: 9.977E-05 | global batch size:  1024 | lm loss: 1.844779E+00 | loss scale: 1.0 | grad norm: 0.308 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1751/   25000 | consumed samples:      1793024 | elapsed time per iteration (ms): 39319.1 | learning rate: 9.977E-05 | global batch size:  1024 | lm loss: 1.840477E+00 | loss scale: 1.0 | grad norm: 0.354 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1752/   25000 | consumed samples:      1794048 | elapsed time per iteration (ms): 39304.1 | learning rate: 9.977E-05 | global batch size:  1024 | lm loss: 1.835572E+00 | loss scale: 1.0 | grad norm: 0.405 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1753/   25000 | consumed samples:      1795072 | elapsed time per iteration (ms): 39383.5 | learning rate: 9.977E-05 | global batch size:  1024 | lm loss: 1.845674E+00 | loss scale: 1.0 | grad norm: 0.356 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1754/   25000 | consumed samples:      1796096 | elapsed time per iteration (ms): 39424.5 | learning rate: 9.977E-05 | global batch size:  1024 | lm loss: 1.829703E+00 | loss scale: 1.0 | grad norm: 0.293 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1755/   25000 | consumed samples:      1797120 | elapsed time per iteration (ms): 39401.9 | learning rate: 9.976E-05 | global batch size:  1024 | lm loss: 1.828116E+00 | loss scale: 1.0 | grad norm: 0.287 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1756/   25000 | consumed samples:      1798144 | elapsed time per iteration (ms): 39293.9 | learning rate: 9.976E-05 | global batch size:  1024 | lm loss: 1.832134E+00 | loss scale: 1.0 | grad norm: 0.309 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1757/   25000 | consumed samples:      1799168 | elapsed time per iteration (ms): 39230.9 | learning rate: 9.976E-05 | global batch size:  1024 | lm loss: 1.833198E+00 | loss scale: 1.0 | grad norm: 0.272 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1758/   25000 | consumed samples:      1800192 | elapsed time per iteration (ms): 39163.8 | learning rate: 9.976E-05 | global batch size:  1024 | lm loss: 1.835551E+00 | loss scale: 1.0 | grad norm: 0.262 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1759/   25000 | consumed samples:      1801216 | elapsed time per iteration (ms): 39529.8 | learning rate: 9.976E-05 | global batch size:  1024 | lm loss: 1.831385E+00 | loss scale: 1.0 | grad norm: 0.278 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1760/   25000 | consumed samples:      1802240 | elapsed time per iteration (ms): 39232.5 | learning rate: 9.976E-05 | global batch size:  1024 | lm loss: 1.827721E+00 | loss scale: 1.0 | grad norm: 0.271 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1761/   25000 | consumed samples:      1803264 | elapsed time per iteration (ms): 39494.3 | learning rate: 9.976E-05 | global batch size:  1024 | lm loss: 1.838024E+00 | loss scale: 1.0 | grad norm: 0.290 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1762/   25000 | consumed samples:      1804288 | elapsed time per iteration (ms): 39223.0 | learning rate: 9.976E-05 | global batch size:  1024 | lm loss: 1.835670E+00 | loss scale: 1.0 | grad norm: 0.295 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1763/   25000 | consumed samples:      1805312 | elapsed time per iteration (ms): 39232.3 | learning rate: 9.976E-05 | global batch size:  1024 | lm loss: 1.826074E+00 | loss scale: 1.0 | grad norm: 0.297 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1764/   25000 | consumed samples:      1806336 | elapsed time per iteration (ms): 39325.4 | learning rate: 9.976E-05 | global batch size:  1024 | lm loss: 1.853670E+00 | loss scale: 1.0 | grad norm: 0.273 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1765/   25000 | consumed samples:      1807360 | elapsed time per iteration (ms): 39415.7 | learning rate: 9.976E-05 | global batch size:  1024 | lm loss: 1.832027E+00 | loss scale: 1.0 | grad norm: 0.344 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1766/   25000 | consumed samples:      1808384 | elapsed time per iteration (ms): 39372.2 | learning rate: 9.976E-05 | global batch size:  1024 | lm loss: 1.831536E+00 | loss scale: 1.0 | grad norm: 0.470 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1767/   25000 | consumed samples:      1809408 | elapsed time per iteration (ms): 39329.4 | learning rate: 9.976E-05 | global batch size:  1024 | lm loss: 1.838763E+00 | loss scale: 1.0 | grad norm: 0.536 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1768/   25000 | consumed samples:      1810432 | elapsed time per iteration (ms): 39245.4 | learning rate: 9.976E-05 | global batch size:  1024 | lm loss: 1.847033E+00 | loss scale: 1.0 | grad norm: 0.469 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1769/   25000 | consumed samples:      1811456 | elapsed time per iteration (ms): 39306.7 | learning rate: 9.976E-05 | global batch size:  1024 | lm loss: 1.852209E+00 | loss scale: 1.0 | grad norm: 0.311 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1770/   25000 | consumed samples:      1812480 | elapsed time per iteration (ms): 39507.3 | learning rate: 9.976E-05 | global batch size:  1024 | lm loss: 1.842395E+00 | loss scale: 1.0 | grad norm: 0.317 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1771/   25000 | consumed samples:      1813504 | elapsed time per iteration (ms): 39355.6 | learning rate: 9.975E-05 | global batch size:  1024 | lm loss: 1.829349E+00 | loss scale: 1.0 | grad norm: 0.347 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1772/   25000 | consumed samples:      1814528 | elapsed time per iteration (ms): 39164.1 | learning rate: 9.975E-05 | global batch size:  1024 | lm loss: 1.837782E+00 | loss scale: 1.0 | grad norm: 0.337 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1773/   25000 | consumed samples:      1815552 | elapsed time per iteration (ms): 39424.1 | learning rate: 9.975E-05 | global batch size:  1024 | lm loss: 1.829600E+00 | loss scale: 1.0 | grad norm: 0.410 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1774/   25000 | consumed samples:      1816576 | elapsed time per iteration (ms): 39158.6 | learning rate: 9.975E-05 | global batch size:  1024 | lm loss: 1.838161E+00 | loss scale: 1.0 | grad norm: 0.369 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1775/   25000 | consumed samples:      1817600 | elapsed time per iteration (ms): 39424.9 | learning rate: 9.975E-05 | global batch size:  1024 | lm loss: 1.826891E+00 | loss scale: 1.0 | grad norm: 0.276 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1776/   25000 | consumed samples:      1818624 | elapsed time per iteration (ms): 39370.6 | learning rate: 9.975E-05 | global batch size:  1024 | lm loss: 1.817621E+00 | loss scale: 1.0 | grad norm: 0.325 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1777/   25000 | consumed samples:      1819648 | elapsed time per iteration (ms): 39379.9 | learning rate: 9.975E-05 | global batch size:  1024 | lm loss: 1.831606E+00 | loss scale: 1.0 | grad norm: 0.325 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1778/   25000 | consumed samples:      1820672 | elapsed time per iteration (ms): 39318.1 | learning rate: 9.975E-05 | global batch size:  1024 | lm loss: 1.829410E+00 | loss scale: 1.0 | grad norm: 0.312 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1779/   25000 | consumed samples:      1821696 | elapsed time per iteration (ms): 39249.4 | learning rate: 9.975E-05 | global batch size:  1024 | lm loss: 1.847185E+00 | loss scale: 1.0 | grad norm: 0.335 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1780/   25000 | consumed samples:      1822720 | elapsed time per iteration (ms): 39163.4 | learning rate: 9.975E-05 | global batch size:  1024 | lm loss: 1.840438E+00 | loss scale: 1.0 | grad norm: 0.299 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1781/   25000 | consumed samples:      1823744 | elapsed time per iteration (ms): 39510.2 | learning rate: 9.975E-05 | global batch size:  1024 | lm loss: 1.827201E+00 | loss scale: 1.0 | grad norm: 0.280 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1782/   25000 | consumed samples:      1824768 | elapsed time per iteration (ms): 39245.8 | learning rate: 9.975E-05 | global batch size:  1024 | lm loss: 1.812818E+00 | loss scale: 1.0 | grad norm: 0.236 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1783/   25000 | consumed samples:      1825792 | elapsed time per iteration (ms): 39424.7 | learning rate: 9.975E-05 | global batch size:  1024 | lm loss: 1.834505E+00 | loss scale: 1.0 | grad norm: 0.281 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1784/   25000 | consumed samples:      1826816 | elapsed time per iteration (ms): 39185.2 | learning rate: 9.975E-05 | global batch size:  1024 | lm loss: 1.840707E+00 | loss scale: 1.0 | grad norm: 0.353 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1785/   25000 | consumed samples:      1827840 | elapsed time per iteration (ms): 39240.9 | learning rate: 9.975E-05 | global batch size:  1024 | lm loss: 1.833898E+00 | loss scale: 1.0 | grad norm: 0.364 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1786/   25000 | consumed samples:      1828864 | elapsed time per iteration (ms): 39783.4 | learning rate: 9.974E-05 | global batch size:  1024 | lm loss: 1.834678E+00 | loss scale: 1.0 | grad norm: 0.354 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1787/   25000 | consumed samples:      1829888 | elapsed time per iteration (ms): 39172.5 | learning rate: 9.974E-05 | global batch size:  1024 | lm loss: 1.859319E+00 | loss scale: 1.0 | grad norm: 0.326 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1788/   25000 | consumed samples:      1830912 | elapsed time per iteration (ms): 39251.7 | learning rate: 9.974E-05 | global batch size:  1024 | lm loss: 1.833569E+00 | loss scale: 1.0 | grad norm: 0.301 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1789/   25000 | consumed samples:      1831936 | elapsed time per iteration (ms): 39247.8 | learning rate: 9.974E-05 | global batch size:  1024 | lm loss: 1.841094E+00 | loss scale: 1.0 | grad norm: 0.298 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1790/   25000 | consumed samples:      1832960 | elapsed time per iteration (ms): 39176.1 | learning rate: 9.974E-05 | global batch size:  1024 | lm loss: 1.843953E+00 | loss scale: 1.0 | grad norm: 0.330 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1791/   25000 | consumed samples:      1833984 | elapsed time per iteration (ms): 39653.1 | learning rate: 9.974E-05 | global batch size:  1024 | lm loss: 1.832384E+00 | loss scale: 1.0 | grad norm: 0.342 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1792/   25000 | consumed samples:      1835008 | elapsed time per iteration (ms): 39336.4 | learning rate: 9.974E-05 | global batch size:  1024 | lm loss: 1.832567E+00 | loss scale: 1.0 | grad norm: 0.341 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1793/   25000 | consumed samples:      1836032 | elapsed time per iteration (ms): 39262.0 | learning rate: 9.974E-05 | global batch size:  1024 | lm loss: 1.838499E+00 | loss scale: 1.0 | grad norm: 0.345 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1794/   25000 | consumed samples:      1837056 | elapsed time per iteration (ms): 39390.9 | learning rate: 9.974E-05 | global batch size:  1024 | lm loss: 1.825275E+00 | loss scale: 1.0 | grad norm: 0.332 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1795/   25000 | consumed samples:      1838080 | elapsed time per iteration (ms): 39260.2 | learning rate: 9.974E-05 | global batch size:  1024 | lm loss: 1.844480E+00 | loss scale: 1.0 | grad norm: 0.295 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1796/   25000 | consumed samples:      1839104 | elapsed time per iteration (ms): 39444.2 | learning rate: 9.974E-05 | global batch size:  1024 | lm loss: 1.836967E+00 | loss scale: 1.0 | grad norm: 0.341 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1797/   25000 | consumed samples:      1840128 | elapsed time per iteration (ms): 39378.7 | learning rate: 9.974E-05 | global batch size:  1024 | lm loss: 1.832287E+00 | loss scale: 1.0 | grad norm: 0.344 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1798/   25000 | consumed samples:      1841152 | elapsed time per iteration (ms): 39182.7 | learning rate: 9.974E-05 | global batch size:  1024 | lm loss: 1.849537E+00 | loss scale: 1.0 | grad norm: 0.349 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1799/   25000 | consumed samples:      1842176 | elapsed time per iteration (ms): 39253.7 | learning rate: 9.974E-05 | global batch size:  1024 | lm loss: 1.814871E+00 | loss scale: 1.0 | grad norm: 0.351 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1800/   25000 | consumed samples:      1843200 | elapsed time per iteration (ms): 39264.5 | learning rate: 9.974E-05 | global batch size:  1024 | lm loss: 1.824970E+00 | loss scale: 1.0 | grad norm: 0.303 | number of skipped iterations:   0 | number of nan iterations:   0 |
------------------------------------------------------------------------------------------------
 validation loss at iteration 1800 | lm loss value: 1.841645E+00 | lm loss PPL: 6.306905E+00 | 
------------------------------------------------------------------------------------------------
 iteration     1801/   25000 | consumed samples:      1844224 | elapsed time per iteration (ms): 175176.9 | learning rate: 9.974E-05 | global batch size:  1024 | lm loss: 1.814960E+00 | loss scale: 1.0 | grad norm: 0.263 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1802/   25000 | consumed samples:      1845248 | elapsed time per iteration (ms): 39636.7 | learning rate: 9.973E-05 | global batch size:  1024 | lm loss: 1.851463E+00 | loss scale: 1.0 | grad norm: 0.271 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1803/   25000 | consumed samples:      1846272 | elapsed time per iteration (ms): 39245.1 | learning rate: 9.973E-05 | global batch size:  1024 | lm loss: 1.820757E+00 | loss scale: 1.0 | grad norm: 0.304 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1804/   25000 | consumed samples:      1847296 | elapsed time per iteration (ms): 39309.3 | learning rate: 9.973E-05 | global batch size:  1024 | lm loss: 1.824455E+00 | loss scale: 1.0 | grad norm: 0.300 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1805/   25000 | consumed samples:      1848320 | elapsed time per iteration (ms): 39544.1 | learning rate: 9.973E-05 | global batch size:  1024 | lm loss: 1.824966E+00 | loss scale: 1.0 | grad norm: 0.285 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1806/   25000 | consumed samples:      1849344 | elapsed time per iteration (ms): 39242.0 | learning rate: 9.973E-05 | global batch size:  1024 | lm loss: 1.828403E+00 | loss scale: 1.0 | grad norm: 0.258 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1807/   25000 | consumed samples:      1850368 | elapsed time per iteration (ms): 39578.0 | learning rate: 9.973E-05 | global batch size:  1024 | lm loss: 1.850096E+00 | loss scale: 1.0 | grad norm: 0.236 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1808/   25000 | consumed samples:      1851392 | elapsed time per iteration (ms): 39241.7 | learning rate: 9.973E-05 | global batch size:  1024 | lm loss: 1.840294E+00 | loss scale: 1.0 | grad norm: 0.289 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1809/   25000 | consumed samples:      1852416 | elapsed time per iteration (ms): 39234.2 | learning rate: 9.973E-05 | global batch size:  1024 | lm loss: 1.830631E+00 | loss scale: 1.0 | grad norm: 0.377 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1810/   25000 | consumed samples:      1853440 | elapsed time per iteration (ms): 39817.1 | learning rate: 9.973E-05 | global batch size:  1024 | lm loss: 1.836069E+00 | loss scale: 1.0 | grad norm: 0.425 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1811/   25000 | consumed samples:      1854464 | elapsed time per iteration (ms): 39239.3 | learning rate: 9.973E-05 | global batch size:  1024 | lm loss: 1.826717E+00 | loss scale: 1.0 | grad norm: 0.431 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1812/   25000 | consumed samples:      1855488 | elapsed time per iteration (ms): 39381.7 | learning rate: 9.973E-05 | global batch size:  1024 | lm loss: 1.822255E+00 | loss scale: 1.0 | grad norm: 0.373 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1813/   25000 | consumed samples:      1856512 | elapsed time per iteration (ms): 39425.5 | learning rate: 9.973E-05 | global batch size:  1024 | lm loss: 1.838076E+00 | loss scale: 1.0 | grad norm: 0.276 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1814/   25000 | consumed samples:      1857536 | elapsed time per iteration (ms): 39312.7 | learning rate: 9.973E-05 | global batch size:  1024 | lm loss: 1.830484E+00 | loss scale: 1.0 | grad norm: 0.306 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1815/   25000 | consumed samples:      1858560 | elapsed time per iteration (ms): 39506.9 | learning rate: 9.973E-05 | global batch size:  1024 | lm loss: 1.844601E+00 | loss scale: 1.0 | grad norm: 0.355 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1816/   25000 | consumed samples:      1859584 | elapsed time per iteration (ms): 39236.5 | learning rate: 9.973E-05 | global batch size:  1024 | lm loss: 1.831630E+00 | loss scale: 1.0 | grad norm: 0.322 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1817/   25000 | consumed samples:      1860608 | elapsed time per iteration (ms): 39226.1 | learning rate: 9.972E-05 | global batch size:  1024 | lm loss: 1.822827E+00 | loss scale: 1.0 | grad norm: 0.284 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1818/   25000 | consumed samples:      1861632 | elapsed time per iteration (ms): 39694.4 | learning rate: 9.972E-05 | global batch size:  1024 | lm loss: 1.817269E+00 | loss scale: 1.0 | grad norm: 0.307 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1819/   25000 | consumed samples:      1862656 | elapsed time per iteration (ms): 39395.4 | learning rate: 9.972E-05 | global batch size:  1024 | lm loss: 1.837400E+00 | loss scale: 1.0 | grad norm: 0.306 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1820/   25000 | consumed samples:      1863680 | elapsed time per iteration (ms): 39459.8 | learning rate: 9.972E-05 | global batch size:  1024 | lm loss: 1.845625E+00 | loss scale: 1.0 | grad norm: 0.304 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1821/   25000 | consumed samples:      1864704 | elapsed time per iteration (ms): 39302.5 | learning rate: 9.972E-05 | global batch size:  1024 | lm loss: 1.834840E+00 | loss scale: 1.0 | grad norm: 0.282 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1822/   25000 | consumed samples:      1865728 | elapsed time per iteration (ms): 39238.7 | learning rate: 9.972E-05 | global batch size:  1024 | lm loss: 1.836274E+00 | loss scale: 1.0 | grad norm: 0.300 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1823/   25000 | consumed samples:      1866752 | elapsed time per iteration (ms): 39588.7 | learning rate: 9.972E-05 | global batch size:  1024 | lm loss: 1.826988E+00 | loss scale: 1.0 | grad norm: 0.319 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1824/   25000 | consumed samples:      1867776 | elapsed time per iteration (ms): 39313.7 | learning rate: 9.972E-05 | global batch size:  1024 | lm loss: 1.820658E+00 | loss scale: 1.0 | grad norm: 0.252 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1825/   25000 | consumed samples:      1868800 | elapsed time per iteration (ms): 39367.6 | learning rate: 9.972E-05 | global batch size:  1024 | lm loss: 1.831179E+00 | loss scale: 1.0 | grad norm: 0.291 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1826/   25000 | consumed samples:      1869824 | elapsed time per iteration (ms): 39500.1 | learning rate: 9.972E-05 | global batch size:  1024 | lm loss: 1.831779E+00 | loss scale: 1.0 | grad norm: 0.327 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1827/   25000 | consumed samples:      1870848 | elapsed time per iteration (ms): 39175.5 | learning rate: 9.972E-05 | global batch size:  1024 | lm loss: 1.830965E+00 | loss scale: 1.0 | grad norm: 0.342 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1828/   25000 | consumed samples:      1871872 | elapsed time per iteration (ms): 39520.5 | learning rate: 9.972E-05 | global batch size:  1024 | lm loss: 1.830741E+00 | loss scale: 1.0 | grad norm: 0.341 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1829/   25000 | consumed samples:      1872896 | elapsed time per iteration (ms): 39279.7 | learning rate: 9.972E-05 | global batch size:  1024 | lm loss: 1.845275E+00 | loss scale: 1.0 | grad norm: 0.329 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1830/   25000 | consumed samples:      1873920 | elapsed time per iteration (ms): 39437.1 | learning rate: 9.972E-05 | global batch size:  1024 | lm loss: 1.821181E+00 | loss scale: 1.0 | grad norm: 0.317 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1831/   25000 | consumed samples:      1874944 | elapsed time per iteration (ms): 39298.7 | learning rate: 9.971E-05 | global batch size:  1024 | lm loss: 1.817001E+00 | loss scale: 1.0 | grad norm: 0.288 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1832/   25000 | consumed samples:      1875968 | elapsed time per iteration (ms): 39369.2 | learning rate: 9.971E-05 | global batch size:  1024 | lm loss: 1.815131E+00 | loss scale: 1.0 | grad norm: 0.286 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1833/   25000 | consumed samples:      1876992 | elapsed time per iteration (ms): 39227.5 | learning rate: 9.971E-05 | global batch size:  1024 | lm loss: 1.833212E+00 | loss scale: 1.0 | grad norm: 0.290 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1834/   25000 | consumed samples:      1878016 | elapsed time per iteration (ms): 39695.1 | learning rate: 9.971E-05 | global batch size:  1024 | lm loss: 1.833331E+00 | loss scale: 1.0 | grad norm: 0.312 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1835/   25000 | consumed samples:      1879040 | elapsed time per iteration (ms): 39358.2 | learning rate: 9.971E-05 | global batch size:  1024 | lm loss: 1.826540E+00 | loss scale: 1.0 | grad norm: 0.323 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1836/   25000 | consumed samples:      1880064 | elapsed time per iteration (ms): 39306.6 | learning rate: 9.971E-05 | global batch size:  1024 | lm loss: 1.836418E+00 | loss scale: 1.0 | grad norm: 0.258 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1837/   25000 | consumed samples:      1881088 | elapsed time per iteration (ms): 39393.4 | learning rate: 9.971E-05 | global batch size:  1024 | lm loss: 1.843393E+00 | loss scale: 1.0 | grad norm: 0.292 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1838/   25000 | consumed samples:      1882112 | elapsed time per iteration (ms): 39299.4 | learning rate: 9.971E-05 | global batch size:  1024 | lm loss: 1.834266E+00 | loss scale: 1.0 | grad norm: 0.309 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1839/   25000 | consumed samples:      1883136 | elapsed time per iteration (ms): 39408.4 | learning rate: 9.971E-05 | global batch size:  1024 | lm loss: 1.841584E+00 | loss scale: 1.0 | grad norm: 0.340 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1840/   25000 | consumed samples:      1884160 | elapsed time per iteration (ms): 39508.4 | learning rate: 9.971E-05 | global batch size:  1024 | lm loss: 1.828435E+00 | loss scale: 1.0 | grad norm: 0.371 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1841/   25000 | consumed samples:      1885184 | elapsed time per iteration (ms): 39314.8 | learning rate: 9.971E-05 | global batch size:  1024 | lm loss: 1.819891E+00 | loss scale: 1.0 | grad norm: 0.333 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1842/   25000 | consumed samples:      1886208 | elapsed time per iteration (ms): 39455.9 | learning rate: 9.971E-05 | global batch size:  1024 | lm loss: 1.826747E+00 | loss scale: 1.0 | grad norm: 0.298 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1843/   25000 | consumed samples:      1887232 | elapsed time per iteration (ms): 39317.9 | learning rate: 9.971E-05 | global batch size:  1024 | lm loss: 1.826663E+00 | loss scale: 1.0 | grad norm: 0.313 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1844/   25000 | consumed samples:      1888256 | elapsed time per iteration (ms): 39433.0 | learning rate: 9.971E-05 | global batch size:  1024 | lm loss: 1.831419E+00 | loss scale: 1.0 | grad norm: 0.317 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1845/   25000 | consumed samples:      1889280 | elapsed time per iteration (ms): 39459.6 | learning rate: 9.971E-05 | global batch size:  1024 | lm loss: 1.832934E+00 | loss scale: 1.0 | grad norm: 0.337 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1846/   25000 | consumed samples:      1890304 | elapsed time per iteration (ms): 39474.9 | learning rate: 9.970E-05 | global batch size:  1024 | lm loss: 1.839477E+00 | loss scale: 1.0 | grad norm: 0.376 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1847/   25000 | consumed samples:      1891328 | elapsed time per iteration (ms): 39236.6 | learning rate: 9.970E-05 | global batch size:  1024 | lm loss: 1.844776E+00 | loss scale: 1.0 | grad norm: 0.415 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1848/   25000 | consumed samples:      1892352 | elapsed time per iteration (ms): 39388.3 | learning rate: 9.970E-05 | global batch size:  1024 | lm loss: 1.820417E+00 | loss scale: 1.0 | grad norm: 0.434 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1849/   25000 | consumed samples:      1893376 | elapsed time per iteration (ms): 39296.0 | learning rate: 9.970E-05 | global batch size:  1024 | lm loss: 1.815112E+00 | loss scale: 1.0 | grad norm: 0.413 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1850/   25000 | consumed samples:      1894400 | elapsed time per iteration (ms): 39703.3 | learning rate: 9.970E-05 | global batch size:  1024 | lm loss: 1.840240E+00 | loss scale: 1.0 | grad norm: 0.338 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1851/   25000 | consumed samples:      1895424 | elapsed time per iteration (ms): 39315.1 | learning rate: 9.970E-05 | global batch size:  1024 | lm loss: 1.828029E+00 | loss scale: 1.0 | grad norm: 0.289 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1852/   25000 | consumed samples:      1896448 | elapsed time per iteration (ms): 39328.0 | learning rate: 9.970E-05 | global batch size:  1024 | lm loss: 1.827444E+00 | loss scale: 1.0 | grad norm: 0.276 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1853/   25000 | consumed samples:      1897472 | elapsed time per iteration (ms): 39225.7 | learning rate: 9.970E-05 | global batch size:  1024 | lm loss: 1.814944E+00 | loss scale: 1.0 | grad norm: 0.316 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1854/   25000 | consumed samples:      1898496 | elapsed time per iteration (ms): 39462.0 | learning rate: 9.970E-05 | global batch size:  1024 | lm loss: 1.836678E+00 | loss scale: 1.0 | grad norm: 0.321 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1855/   25000 | consumed samples:      1899520 | elapsed time per iteration (ms): 39582.6 | learning rate: 9.970E-05 | global batch size:  1024 | lm loss: 1.835183E+00 | loss scale: 1.0 | grad norm: 0.315 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1856/   25000 | consumed samples:      1900544 | elapsed time per iteration (ms): 39387.7 | learning rate: 9.970E-05 | global batch size:  1024 | lm loss: 1.803467E+00 | loss scale: 1.0 | grad norm: 0.296 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1857/   25000 | consumed samples:      1901568 | elapsed time per iteration (ms): 39235.0 | learning rate: 9.970E-05 | global batch size:  1024 | lm loss: 1.823664E+00 | loss scale: 1.0 | grad norm: 0.287 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1858/   25000 | consumed samples:      1902592 | elapsed time per iteration (ms): 39377.7 | learning rate: 9.970E-05 | global batch size:  1024 | lm loss: 1.829499E+00 | loss scale: 1.0 | grad norm: 0.301 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1859/   25000 | consumed samples:      1903616 | elapsed time per iteration (ms): 39599.9 | learning rate: 9.970E-05 | global batch size:  1024 | lm loss: 1.836897E+00 | loss scale: 1.0 | grad norm: 0.271 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1860/   25000 | consumed samples:      1904640 | elapsed time per iteration (ms): 39499.2 | learning rate: 9.969E-05 | global batch size:  1024 | lm loss: 1.843152E+00 | loss scale: 1.0 | grad norm: 0.285 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1861/   25000 | consumed samples:      1905664 | elapsed time per iteration (ms): 39245.8 | learning rate: 9.969E-05 | global batch size:  1024 | lm loss: 1.827716E+00 | loss scale: 1.0 | grad norm: 0.311 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1862/   25000 | consumed samples:      1906688 | elapsed time per iteration (ms): 39318.3 | learning rate: 9.969E-05 | global batch size:  1024 | lm loss: 1.815594E+00 | loss scale: 1.0 | grad norm: 0.339 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1863/   25000 | consumed samples:      1907712 | elapsed time per iteration (ms): 39247.9 | learning rate: 9.969E-05 | global batch size:  1024 | lm loss: 1.843910E+00 | loss scale: 1.0 | grad norm: 0.378 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1864/   25000 | consumed samples:      1908736 | elapsed time per iteration (ms): 39668.3 | learning rate: 9.969E-05 | global batch size:  1024 | lm loss: 1.832139E+00 | loss scale: 1.0 | grad norm: 0.457 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1865/   25000 | consumed samples:      1909760 | elapsed time per iteration (ms): 39342.0 | learning rate: 9.969E-05 | global batch size:  1024 | lm loss: 1.833695E+00 | loss scale: 1.0 | grad norm: 0.476 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1866/   25000 | consumed samples:      1910784 | elapsed time per iteration (ms): 39460.0 | learning rate: 9.969E-05 | global batch size:  1024 | lm loss: 1.804199E+00 | loss scale: 1.0 | grad norm: 0.421 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1867/   25000 | consumed samples:      1911808 | elapsed time per iteration (ms): 39381.4 | learning rate: 9.969E-05 | global batch size:  1024 | lm loss: 1.832930E+00 | loss scale: 1.0 | grad norm: 0.348 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1868/   25000 | consumed samples:      1912832 | elapsed time per iteration (ms): 39400.7 | learning rate: 9.969E-05 | global batch size:  1024 | lm loss: 1.821150E+00 | loss scale: 1.0 | grad norm: 0.399 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1869/   25000 | consumed samples:      1913856 | elapsed time per iteration (ms): 39373.0 | learning rate: 9.969E-05 | global batch size:  1024 | lm loss: 1.837407E+00 | loss scale: 1.0 | grad norm: 0.367 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1870/   25000 | consumed samples:      1914880 | elapsed time per iteration (ms): 39307.5 | learning rate: 9.969E-05 | global batch size:  1024 | lm loss: 1.817314E+00 | loss scale: 1.0 | grad norm: 0.308 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1871/   25000 | consumed samples:      1915904 | elapsed time per iteration (ms): 39425.3 | learning rate: 9.969E-05 | global batch size:  1024 | lm loss: 1.843531E+00 | loss scale: 1.0 | grad norm: 0.353 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1872/   25000 | consumed samples:      1916928 | elapsed time per iteration (ms): 39391.3 | learning rate: 9.969E-05 | global batch size:  1024 | lm loss: 1.814525E+00 | loss scale: 1.0 | grad norm: 0.399 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1873/   25000 | consumed samples:      1917952 | elapsed time per iteration (ms): 39309.0 | learning rate: 9.969E-05 | global batch size:  1024 | lm loss: 1.822379E+00 | loss scale: 1.0 | grad norm: 0.294 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1874/   25000 | consumed samples:      1918976 | elapsed time per iteration (ms): 39525.3 | learning rate: 9.968E-05 | global batch size:  1024 | lm loss: 1.806820E+00 | loss scale: 1.0 | grad norm: 0.299 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1875/   25000 | consumed samples:      1920000 | elapsed time per iteration (ms): 39379.7 | learning rate: 9.968E-05 | global batch size:  1024 | lm loss: 1.829791E+00 | loss scale: 1.0 | grad norm: 0.350 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1876/   25000 | consumed samples:      1921024 | elapsed time per iteration (ms): 39449.2 | learning rate: 9.968E-05 | global batch size:  1024 | lm loss: 1.825227E+00 | loss scale: 1.0 | grad norm: 0.298 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1877/   25000 | consumed samples:      1922048 | elapsed time per iteration (ms): 39330.7 | learning rate: 9.968E-05 | global batch size:  1024 | lm loss: 1.816015E+00 | loss scale: 1.0 | grad norm: 0.286 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1878/   25000 | consumed samples:      1923072 | elapsed time per iteration (ms): 39331.1 | learning rate: 9.968E-05 | global batch size:  1024 | lm loss: 1.797696E+00 | loss scale: 1.0 | grad norm: 0.310 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1879/   25000 | consumed samples:      1924096 | elapsed time per iteration (ms): 39375.2 | learning rate: 9.968E-05 | global batch size:  1024 | lm loss: 1.807053E+00 | loss scale: 1.0 | grad norm: 0.281 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1880/   25000 | consumed samples:      1925120 | elapsed time per iteration (ms): 39386.9 | learning rate: 9.968E-05 | global batch size:  1024 | lm loss: 1.815162E+00 | loss scale: 1.0 | grad norm: 0.269 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1881/   25000 | consumed samples:      1926144 | elapsed time per iteration (ms): 39349.4 | learning rate: 9.968E-05 | global batch size:  1024 | lm loss: 1.814171E+00 | loss scale: 1.0 | grad norm: 0.262 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1882/   25000 | consumed samples:      1927168 | elapsed time per iteration (ms): 39527.9 | learning rate: 9.968E-05 | global batch size:  1024 | lm loss: 1.825482E+00 | loss scale: 1.0 | grad norm: 0.266 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1883/   25000 | consumed samples:      1928192 | elapsed time per iteration (ms): 39534.7 | learning rate: 9.968E-05 | global batch size:  1024 | lm loss: 1.830958E+00 | loss scale: 1.0 | grad norm: 0.303 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1884/   25000 | consumed samples:      1929216 | elapsed time per iteration (ms): 39440.9 | learning rate: 9.968E-05 | global batch size:  1024 | lm loss: 1.828496E+00 | loss scale: 1.0 | grad norm: 0.280 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1885/   25000 | consumed samples:      1930240 | elapsed time per iteration (ms): 39236.4 | learning rate: 9.968E-05 | global batch size:  1024 | lm loss: 1.821128E+00 | loss scale: 1.0 | grad norm: 0.252 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1886/   25000 | consumed samples:      1931264 | elapsed time per iteration (ms): 39437.4 | learning rate: 9.968E-05 | global batch size:  1024 | lm loss: 1.824860E+00 | loss scale: 1.0 | grad norm: 0.259 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1887/   25000 | consumed samples:      1932288 | elapsed time per iteration (ms): 39348.7 | learning rate: 9.968E-05 | global batch size:  1024 | lm loss: 1.829860E+00 | loss scale: 1.0 | grad norm: 0.254 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1888/   25000 | consumed samples:      1933312 | elapsed time per iteration (ms): 39376.3 | learning rate: 9.967E-05 | global batch size:  1024 | lm loss: 1.806108E+00 | loss scale: 1.0 | grad norm: 0.270 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1889/   25000 | consumed samples:      1934336 | elapsed time per iteration (ms): 39361.2 | learning rate: 9.967E-05 | global batch size:  1024 | lm loss: 1.829984E+00 | loss scale: 1.0 | grad norm: 0.282 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1890/   25000 | consumed samples:      1935360 | elapsed time per iteration (ms): 39311.6 | learning rate: 9.967E-05 | global batch size:  1024 | lm loss: 1.830209E+00 | loss scale: 1.0 | grad norm: 0.267 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1891/   25000 | consumed samples:      1936384 | elapsed time per iteration (ms): 39449.1 | learning rate: 9.967E-05 | global batch size:  1024 | lm loss: 1.817393E+00 | loss scale: 1.0 | grad norm: 0.290 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1892/   25000 | consumed samples:      1937408 | elapsed time per iteration (ms): 39652.8 | learning rate: 9.967E-05 | global batch size:  1024 | lm loss: 1.828341E+00 | loss scale: 1.0 | grad norm: 0.269 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1893/   25000 | consumed samples:      1938432 | elapsed time per iteration (ms): 39234.4 | learning rate: 9.967E-05 | global batch size:  1024 | lm loss: 1.830283E+00 | loss scale: 1.0 | grad norm: 0.357 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1894/   25000 | consumed samples:      1939456 | elapsed time per iteration (ms): 39438.7 | learning rate: 9.967E-05 | global batch size:  1024 | lm loss: 1.829858E+00 | loss scale: 1.0 | grad norm: 0.429 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1895/   25000 | consumed samples:      1940480 | elapsed time per iteration (ms): 39310.7 | learning rate: 9.967E-05 | global batch size:  1024 | lm loss: 1.828999E+00 | loss scale: 1.0 | grad norm: 0.485 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1896/   25000 | consumed samples:      1941504 | elapsed time per iteration (ms): 39388.4 | learning rate: 9.967E-05 | global batch size:  1024 | lm loss: 1.827945E+00 | loss scale: 1.0 | grad norm: 0.452 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1897/   25000 | consumed samples:      1942528 | elapsed time per iteration (ms): 39336.8 | learning rate: 9.967E-05 | global batch size:  1024 | lm loss: 1.805484E+00 | loss scale: 1.0 | grad norm: 0.395 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1898/   25000 | consumed samples:      1943552 | elapsed time per iteration (ms): 39368.1 | learning rate: 9.967E-05 | global batch size:  1024 | lm loss: 1.836423E+00 | loss scale: 1.0 | grad norm: 0.358 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1899/   25000 | consumed samples:      1944576 | elapsed time per iteration (ms): 39577.5 | learning rate: 9.967E-05 | global batch size:  1024 | lm loss: 1.822093E+00 | loss scale: 1.0 | grad norm: 0.344 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1900/   25000 | consumed samples:      1945600 | elapsed time per iteration (ms): 39336.8 | learning rate: 9.967E-05 | global batch size:  1024 | lm loss: 1.828168E+00 | loss scale: 1.0 | grad norm: 0.353 | number of skipped iterations:   0 | number of nan iterations:   0 |
