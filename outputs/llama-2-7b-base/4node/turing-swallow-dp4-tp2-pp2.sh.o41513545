MASTER_ADDR=10.0.59.5
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
using world size: 32, data-parallel-size: 8, tensor-model-parallel size: 2, pipeline-model-parallel size: 2 
WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:Llama2Tokenizer
accumulate and all-reduce gradients in fp32 for bfloat16 data type.
using torch.bfloat16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. True
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. False
  add_position_embedding .......................... False
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  async_tensor_model_parallel_allreduce ........... False
  attention_dropout ............................... 0.0
  attention_softmax_in_fp32 ....................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ True
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ False
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  check_for_nan_in_loss_and_grad .................. True
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  data_cache_path ................................. None
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 8
  data_path ....................................... ['9344955862', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/merged_0_text_document', '9387405706', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/merged_1_text_document', '10614722501', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/merged_2_text_document', '10774826633', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/merged_3_text_document', '10525668913', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/merged_4_text_document', '9502019045', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/merged_5_text_document', '8784459147', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/merged_6_text_document', '9826112028', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/merged_7_text_document', '9152375731', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/merged_8_text_document', '9891239743', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/merged_9_text_document', '9341639254', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/merged_10_text_document', '9702056537', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/merged_11_text_document', '9047625381', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/merged_12_text_document', '9059299870', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/merged_13_text_document', '8623585025', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/merged_14_text_document', '11360430162', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/merged_15_text_document', '10562828472', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/merged_16_text_document', '9116094403', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/merged_17_text_document', '9932843686', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/merged_18_text_document', '11097404819', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/merged_19_text_document', '9224853685', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/merged_20_text_document', '1672543873', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/ja_wiki_merged_train_text_document', '11474721693', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/lumi_en_arxiv_merged_text_document', '11474721693', '/bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/lumi_en_falcon_merged_threadripper-3960x_8_text_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  delay_grad_reduce ............................... True
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 10
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  encoder_num_layers .............................. 32
  encoder_seq_length .............................. 1024
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... False
  eval_interval ................................... 100
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_parallel ................................. False
  ffn_hidden_size ................................. 11008
  finetune ........................................ False
  fp16 ............................................ False
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 1024
  gradient_accumulation_fusion .................... True
  group_query_attention ........................... True
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.0
  hidden_size ..................................... 4096
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  iteration ....................................... 25000
  kv_channels ..................................... 128
  lazy_mpu_init ................................... None
  load ............................................ /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 2e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 1000
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 1024
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... None
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6.6e-07
  mmap_warmup ..................................... False
  model_spec ...................................... None
  no_load_optim ................................... True
  no_load_rng ..................................... True
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... RMSNorm
  num_attention_heads ............................. 32
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... None
  num_layers ...................................... 32
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 32
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  override_opt_param_scheduler .................... False
  padded_vocab_size ............................... 43176
  params_dtype .................................... torch.bfloat16
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 2
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... rope
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... selective
  recompute_method ................................ None
  recompute_num_layers ............................ None
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_return_doc_ids ............................ False
  retro_workdir ................................... None
  rotary_percent .................................. 1.0
  rotary_seq_len_interpolation_factor ............. None
  sample_rate ..................................... 1.0
  save ............................................ /groups/gaf51275/llama/checkpoints/Llama-2-7b-seq-1024/okazaki_lab_cc/tp2-pp2
  save_interval ................................... 500
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sequence_parallel ............................... True
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  skip_train_iteration_range ...................... None
  split ........................................... 949,50,1
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.1
  swiglu .......................................... True
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 2
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. /bb/llm/gaf51275/jalm/jalm-tokenizer-private/tokenizer/jalm_llama_okazaki_lab_cc_nfkc_16k_aligned_8/merged_tokenizer_sp/jalm_llama.model
  tokenizer_type .................................. Llama2Tokenizer
  train_data_path ................................. None
  train_iters ..................................... 25000
  train_samples ................................... None
  transformer_impl ................................ local
  transformer_pipeline_model_parallel_size ........ 2
  untie_embeddings_and_output_weights ............. True
  use_checkpoint_args ............................. True
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_distributed_optimizer ....................... True
  use_flash_attn .................................. True
  use_mpi ......................................... True
  use_one_sent_docs ............................... False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. True
  use_z_loss ...................................... False
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... None
  vocab_size ...................................... None
  wandb_entity .................................... brain-team
  wandb_id ........................................ None
  wandb_name ...................................... llama-2-7b-base-extended-okazaki-lab-cc-a100-4node-32gpu-1024s-DP=8-TP=2-PP=2-BS=1024-LR=2e-5-MINLR=6.6e-7-WARMUP=1000-WD=0.1-GC=1
  wandb_project ................................... Swallow-7B
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  world_size ...................................... 32
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 128
> building Llama2Tokenizer tokenizer ...
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
> initializing torch distributed ...
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
checkpoint path: /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2, iteration: 25000
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
Setting num_query_groups to 32 from checkpoint
Setting group_query_attention to True from checkpoint
Setting kv_channels to 128 from checkpoint
Setting position_embedding_type to rope from checkpoint
Setting add_position_embedding to False from checkpoint
Setting use_rotary_position_embeddings to True from checkpoint
Setting rotary_percent to 1.0 from checkpoint
Setting add_bias_linear to False from checkpoint
Setting swiglu to True from checkpoint
Setting untie_embeddings_and_output_weights to True from checkpoint
Setting apply_layernorm_1p to False from checkpoint
Setting normalization to RMSNorm from checkpoint
Setting padded_vocab_size to 43176 from checkpoint
Setting tensor_model_parallel_size to 2 from checkpoint
Setting pipeline_model_parallel_size to 2 from checkpoint
Checkpoint did not provide arguments virtual_pipeline_model_parallel_size
Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage
wandb: Currently logged in as: okoge (brain-team). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /bb/1/llm/gaf51275/llama/Megatron-LM/wandb/run-20240129_080200-pftb2ofs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llama-2-7b-base-extended-okazaki-lab-cc-a100-4node-32gpu-1024s-DP=8-TP=2-PP=2-BS=1024-LR=2e-5-MINLR=6.6e-7-WARMUP=1000-WD=0.1-GC=1-2024-01-29-08-01-58
wandb: ⭐️ View project at https://wandb.ai/brain-team/Swallow-7B
wandb: 🚀 View run at https://wandb.ai/brain-team/Swallow-7B/runs/pftb2ofs
> wandb ...
> initialized tensor model parallel with size 2
> initialized pipeline model parallel with size 2
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory '/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/data'
>>> done with dataset index builder. Compilation time: 0.066 seconds
WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
> compiling and loading fused kernels ...
>>> done with compiling and loading fused kernels. Compilation time: 5.078 seconds
time to initialize megatron (seconds): 17.733
[after megatron is initialized] datetime: 2024-01-29 08:02:09 
building GPT model ...
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1707556864
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1707556864
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 1707560960
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1707560960
> buckets for gradient all-reduce / reduce-scatter:
    params for bucket 1
      module.language_model.encoder.layers.13.self_attention.query_key_value.weight
      module.language_model.encoder.layers.7.self_attention.query_key_value.weight
      module.language_model.encoder.layers.3.input_norm.weight
      module.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.11.input_norm.weight
      module.language_model.encoder.layers.1.self_attention.dense.weight
      module.language_model.encoder.layers.4.self_attention.query_key_value.weight
      module.language_model.encoder.layers.15.post_attention_norm.weight
      module.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.2.post_attention_norm.weight
      module.language_model.encoder.layers.3.post_attention_norm.weight
      module.language_model.encoder.layers.12.self_attention.dense.weight
      module.language_model.encoder.layers.7.input_norm.weight
      module.language_model.encoder.layers.12.self_attention.query_key_value.weight
      module.language_model.encoder.layers.6.post_attention_norm.weight
      module.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.10.input_norm.weight
      module.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.0.self_attention.dense.weight
      module.language_model.encoder.layers.14.post_attention_norm.weight
      module.language_model.encoder.layers.9.self_attention.query_key_value.weight
      module.language_model.encoder.layers.2.input_norm.weight
      module.language_model.encoder.layers.0.post_attention_norm.weight
      module.language_model.embedding.word_embeddings.weight
      module.language_model.encoder.layers.11.self_attention.dense.weight
      module.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.2.self_attention.query_key_value.weight
      module.language_model.encoder.layers.11.self_attention.query_key_value.weight
      module.language_model.encoder.layers.3.self_attention.query_key_value.weight
      module.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.8.self_attention.dense.weight
      module.language_model.encoder.layers.5.post_attention_norm.weight
      module.language_model.encoder.layers.13.post_attention_norm.weight
      module.language_model.encoder.layers.8.self_attention.query_key_value.weight
      module.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.10.self_attention.dense.weight
      module.language_model.encoder.layers.10.self_attention.query_key_value.weight
      module.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.4.self_attention.dense.weight
      module.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.1.input_norm.weight
      module.language_model.encoder.layers.12.post_attention_norm.weight
      module.language_model.encoder.layers.6.self_attention.dense.weight
      module.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.9.self_attention.dense.weight
      module.language_model.encoder.layers.1.post_attention_norm.weight
      module.language_model.encoder.layers.15.input_norm.weight
      module.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.1.self_attention.query_key_value.weight
      module.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.5.input_norm.weight
      module.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.4.input_norm.weight
      module.language_model.encoder.layers.11.post_attention_norm.weight
      module.language_model.encoder.layers.8.input_norm.weight
      module.language_model.encoder.layers.5.self_attention.dense.weight
      module.language_model.encoder.layers.5.self_attention.query_key_value.weight
      module.language_model.encoder.layers.2.self_attention.dense.weight
      module.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.14.input_norm.weight
      module.language_model.encoder.layers.8.post_attention_norm.weight
      module.language_model.encoder.layers.3.self_attention.dense.weight
      module.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.7.post_attention_norm.weight
      module.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.15.self_attention.dense.weight
      module.language_model.encoder.layers.10.post_attention_norm.weight
      module.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.6.input_norm.weight
      module.language_model.encoder.layers.13.input_norm.weight
      module.language_model.encoder.layers.6.self_attention.query_key_value.weight
      module.language_model.encoder.layers.0.input_norm.weight
      module.language_model.encoder.layers.15.self_attention.query_key_value.weight
      module.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight
      module.language_model.encoder.layers.4.post_attention_norm.weight
      module.language_model.encoder.layers.14.self_attention.dense.weight
      module.language_model.encoder.layers.9.post_attention_norm.weight
      module.language_model.encoder.layers.0.self_attention.query_key_value.weight
      module.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.12.input_norm.weight
      module.language_model.encoder.layers.7.self_attention.dense.weight
      module.language_model.encoder.layers.14.self_attention.query_key_value.weight
      module.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight
      module.language_model.encoder.layers.13.self_attention.dense.weight
      module.language_model.encoder.layers.9.input_norm.weight
      module.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight
     total number of elements: 1707556864
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
> learning rate decay style: cosine
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py:427: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = bucket.data.storage()._untyped()
 loading checkpoint from /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2 at iteration 25000
 checkpoint version 3.0
  successfully loaded checkpoint from /bb/llm/gaf51275/llama/checkpoints/mdx-Llama-2-7b-base-extended/okazaki_lab_cc/tp2-pp2 at iteration 25000
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/bb/1/llm/gaf51275/llama/Megatron-LM/.env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2562: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
(min, max) time across ranks (ms):
    load-checkpoint ................................: (3755.15, 3755.25)
[after model, optimizer, and learning rate scheduler are built] datetime: 2024-01-29 08:02:13 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      25600000
    validation: 2570240
    test:       10240
> building train, validation, and test datasets for GPT ...
Single data path provided for train, valid & test
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.003280 seconds
    number of documents: 7871521
 > dataset split:
    train:
     document indices in [0, 7470073) total of 7470073 documents
    validation:
     document indices in [7470073, 7863649) total of 393576 documents
    test:
     document indices in [7863649, 7871521) total of 7872 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.287441
    using:
     number of documents:       7470073
     number of epochs:          1
     sequence length:           1024
     total number of samples:   8667065
 > elasped time to build and save sample-idx mapping (seconds): 0.163163
 > building shuffle index with split [0, 8667065) and [8667065, 8667065) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.212684
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/a49d1fb3e1fa832c83cb1f478b652eda_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/a49d1fb3e1fa832c83cb1f478b652eda_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/a49d1fb3e1fa832c83cb1f478b652eda_shuffle_idx.npy
    loaded indexed file in 0.009 seconds
    total number of samples: 8667066
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.017340
    using:
     number of documents:       393576
     number of epochs:          1
     sequence length:           1024
     total number of samples:   449287
 > elasped time to build and save sample-idx mapping (seconds): 0.038873
 > building shuffle index with split [0, 449287) and [449287, 449287) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.178063
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/23aa50c3ec08bdc6748ef50672b2af9b_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/23aa50c3ec08bdc6748ef50672b2af9b_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/23aa50c3ec08bdc6748ef50672b2af9b_shuffle_idx.npy
    loaded indexed file in 0.005 seconds
    total number of samples: 449288
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.006286
    using:
     number of documents:       7872
     number of epochs:          1
     sequence length:           1024
     total number of samples:   9580
 > elasped time to build and save sample-idx mapping (seconds): 0.007669
 > building shuffle index with split [0, 9580) and [9580, 9580) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.006722
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/fe4f8ce65718d0ac9807852a13935ae1_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/fe4f8ce65718d0ac9807852a13935ae1_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/fe4f8ce65718d0ac9807852a13935ae1_shuffle_idx.npy
    loaded indexed file in 0.006 seconds
    total number of samples: 9581
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.002164 seconds
    number of documents: 7980407
 > dataset split:
    train:
     document indices in [0, 7573407) total of 7573407 documents
    validation:
     document indices in [7573407, 7972427) total of 399020 documents
    test:
     document indices in [7972427, 7980407) total of 7980 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.323366
    using:
     number of documents:       7573407
     number of epochs:          1
     sequence length:           1024
     total number of samples:   8695411
 > elasped time to build and save sample-idx mapping (seconds): 0.175330
 > building shuffle index with split [0, 8695411) and [8695411, 8695411) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.225701
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/f30c389c8aad28c150e9e228492566ee_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/f30c389c8aad28c150e9e228492566ee_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/f30c389c8aad28c150e9e228492566ee_shuffle_idx.npy
    loaded indexed file in 0.009 seconds
    total number of samples: 8695412
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.019245
    using:
     number of documents:       399020
     number of epochs:          1
     sequence length:           1024
     total number of samples:   462261
 > elasped time to build and save sample-idx mapping (seconds): 0.011499
 > building shuffle index with split [0, 462261) and [462261, 462261) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.015195
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/7b992790f121d91bd7c2285a28fb5cc7_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/7b992790f121d91bd7c2285a28fb5cc7_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/7b992790f121d91bd7c2285a28fb5cc7_shuffle_idx.npy
    loaded indexed file in 0.005 seconds
    total number of samples: 462262
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.360017
    using:
     number of documents:       7980
     number of epochs:          1
     sequence length:           1024
     total number of samples:   9715
 > elasped time to build and save sample-idx mapping (seconds): 0.005913
 > building shuffle index with split [0, 9715) and [9715, 9715) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.004415
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/a854e420527c808e71daaef40a1d098d_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/a854e420527c808e71daaef40a1d098d_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/a854e420527c808e71daaef40a1d098d_shuffle_idx.npy
    loaded indexed file in 0.001 seconds
    total number of samples: 9716
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.005771 seconds
    number of documents: 8959506
 > dataset split:
    train:
     document indices in [0, 8502571) total of 8502571 documents
    validation:
     document indices in [8502571, 8950546) total of 447975 documents
    test:
     document indices in [8950546, 8959506) total of 8960 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.315977
    using:
     number of documents:       8502571
     number of epochs:          1
     sequence length:           1024
     total number of samples:   9839032
 > elasped time to build and save sample-idx mapping (seconds): 0.193388
 > building shuffle index with split [0, 9839032) and [9839032, 9839032) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.244740
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/60d626a610dc13ec12885ba1bc51432d_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/60d626a610dc13ec12885ba1bc51432d_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/60d626a610dc13ec12885ba1bc51432d_shuffle_idx.npy
    loaded indexed file in 0.008 seconds
    total number of samples: 9839033
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.013540
    using:
     number of documents:       447975
     number of epochs:          1
     sequence length:           1024
     total number of samples:   516468
 > elasped time to build and save sample-idx mapping (seconds): 0.008642
 > building shuffle index with split [0, 516468) and [516468, 516468) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.011323
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e46855081d9a7bcb51dc1cbf2e2c3783_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e46855081d9a7bcb51dc1cbf2e2c3783_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e46855081d9a7bcb51dc1cbf2e2c3783_shuffle_idx.npy
    loaded indexed file in 0.005 seconds
    total number of samples: 516469
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001443
    using:
     number of documents:       8960
     number of epochs:          1
     sequence length:           1024
     total number of samples:   10438
 > elasped time to build and save sample-idx mapping (seconds): 0.001012
 > building shuffle index with split [0, 10438) and [10438, 10438) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001058
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/f8e51fbce5a51d817c7ebbc376c4a204_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/f8e51fbce5a51d817c7ebbc376c4a204_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/f8e51fbce5a51d817c7ebbc376c4a204_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of samples: 10439
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.005338 seconds
    number of documents: 9101179
 > dataset split:
    train:
     document indices in [0, 8637019) total of 8637019 documents
    validation:
     document indices in [8637019, 9092078) total of 455059 documents
    test:
     document indices in [9092078, 9101179) total of 9101 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.310579
    using:
     number of documents:       8637019
     number of epochs:          1
     sequence length:           1024
     total number of samples:   9975411
 > elasped time to build and save sample-idx mapping (seconds): 0.187314
 > building shuffle index with split [0, 9975411) and [9975411, 9975411) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.243312
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e45e3bc62f798084e44822cdac43a743_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e45e3bc62f798084e44822cdac43a743_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e45e3bc62f798084e44822cdac43a743_shuffle_idx.npy
    loaded indexed file in 0.035 seconds
    total number of samples: 9975412
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.013391
    using:
     number of documents:       455059
     number of epochs:          1
     sequence length:           1024
     total number of samples:   536199
 > elasped time to build and save sample-idx mapping (seconds): 0.008458
 > building shuffle index with split [0, 536199) and [536199, 536199) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.012133
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/c39ea8eebb8b00993368585fda2eb00d_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/c39ea8eebb8b00993368585fda2eb00d_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/c39ea8eebb8b00993368585fda2eb00d_shuffle_idx.npy
    loaded indexed file in 0.005 seconds
    total number of samples: 536200
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001095
    using:
     number of documents:       9101
     number of epochs:          1
     sequence length:           1024
     total number of samples:   10679
 > elasped time to build and save sample-idx mapping (seconds): 0.001078
 > building shuffle index with split [0, 10679) and [10679, 10679) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001606
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e48ca445761cea7f5b13a9a48fee35bf_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e48ca445761cea7f5b13a9a48fee35bf_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e48ca445761cea7f5b13a9a48fee35bf_shuffle_idx.npy
    loaded indexed file in 0.003 seconds
    total number of samples: 10680
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.005353 seconds
    number of documents: 8922899
 > dataset split:
    train:
     document indices in [0, 8467831) total of 8467831 documents
    validation:
     document indices in [8467831, 8913976) total of 446145 documents
    test:
     document indices in [8913976, 8922899) total of 8923 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.305193
    using:
     number of documents:       8467831
     number of epochs:          1
     sequence length:           1024
     total number of samples:   9752509
 > elasped time to build and save sample-idx mapping (seconds): 0.181806
 > building shuffle index with split [0, 9752509) and [9752509, 9752509) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.238666
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/69584969de5db681a72a3185d1ce559c_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/69584969de5db681a72a3185d1ce559c_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/69584969de5db681a72a3185d1ce559c_shuffle_idx.npy
    loaded indexed file in 0.008 seconds
    total number of samples: 9752510
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.013743
    using:
     number of documents:       446145
     number of epochs:          1
     sequence length:           1024
     total number of samples:   516100
 > elasped time to build and save sample-idx mapping (seconds): 0.008031
 > building shuffle index with split [0, 516100) and [516100, 516100) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.011265
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e7128f00d6250ee2e1934beafb9f40e9_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e7128f00d6250ee2e1934beafb9f40e9_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e7128f00d6250ee2e1934beafb9f40e9_shuffle_idx.npy
    loaded indexed file in 0.005 seconds
    total number of samples: 516101
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001074
    using:
     number of documents:       8923
     number of epochs:          1
     sequence length:           1024
     total number of samples:   10363
 > elasped time to build and save sample-idx mapping (seconds): 0.001015
 > building shuffle index with split [0, 10363) and [10363, 10363) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001277
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/ebdef8fc069fc5552c74cc690c11f865_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/ebdef8fc069fc5552c74cc690c11f865_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/ebdef8fc069fc5552c74cc690c11f865_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of samples: 10364
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.005182 seconds
    number of documents: 8030942
 > dataset split:
    train:
     document indices in [0, 7621364) total of 7621364 documents
    validation:
     document indices in [7621364, 8022911) total of 401547 documents
    test:
     document indices in [8022911, 8030942) total of 8031 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.281013
    using:
     number of documents:       7621364
     number of epochs:          1
     sequence length:           1024
     total number of samples:   8796119
 > elasped time to build and save sample-idx mapping (seconds): 0.157251
 > building shuffle index with split [0, 8796119) and [8796119, 8796119) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.208835
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/d7a89070a168534e8e2884491a00ff92_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/d7a89070a168534e8e2884491a00ff92_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/d7a89070a168534e8e2884491a00ff92_shuffle_idx.npy
    loaded indexed file in 0.008 seconds
    total number of samples: 8796120
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.012275
    using:
     number of documents:       401547
     number of epochs:          1
     sequence length:           1024
     total number of samples:   473788
 > elasped time to build and save sample-idx mapping (seconds): 0.008276
 > building shuffle index with split [0, 473788) and [473788, 473788) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.011704
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/d5810bf6436372425d598e60108fce19_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/d5810bf6436372425d598e60108fce19_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/d5810bf6436372425d598e60108fce19_shuffle_idx.npy
    loaded indexed file in 0.005 seconds
    total number of samples: 473789
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001161
    using:
     number of documents:       8031
     number of epochs:          1
     sequence length:           1024
     total number of samples:   9407
 > elasped time to build and save sample-idx mapping (seconds): 0.001085
 > building shuffle index with split [0, 9407) and [9407, 9407) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001222
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/6ac5ecaa56d45d7d55cd6bb3c9697b0a_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/6ac5ecaa56d45d7d55cd6bb3c9697b0a_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/6ac5ecaa56d45d7d55cd6bb3c9697b0a_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of samples: 9408
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.005730 seconds
    number of documents: 7417917
 > dataset split:
    train:
     document indices in [0, 7039603) total of 7039603 documents
    validation:
     document indices in [7039603, 7410499) total of 370896 documents
    test:
     document indices in [7410499, 7417917) total of 7418 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.260417
    using:
     number of documents:       7039603
     number of epochs:          1
     sequence length:           1024
     total number of samples:   8144096
 > elasped time to build and save sample-idx mapping (seconds): 0.143851
 > building shuffle index with split [0, 8144096) and [8144096, 8144096) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.191620
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/4790cd4206157bf8b8391c63c8db0d4d_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/4790cd4206157bf8b8391c63c8db0d4d_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/4790cd4206157bf8b8391c63c8db0d4d_shuffle_idx.npy
    loaded indexed file in 0.008 seconds
    total number of samples: 8144097
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.012118
    using:
     number of documents:       370896
     number of epochs:          1
     sequence length:           1024
     total number of samples:   425900
 > elasped time to build and save sample-idx mapping (seconds): 0.007686
 > building shuffle index with split [0, 425900) and [425900, 425900) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.010084
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/13322953cf3d9c3115de7ae09315609b_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/13322953cf3d9c3115de7ae09315609b_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/13322953cf3d9c3115de7ae09315609b_shuffle_idx.npy
    loaded indexed file in 0.005 seconds
    total number of samples: 425901
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001037
    using:
     number of documents:       7418
     number of epochs:          1
     sequence length:           1024
     total number of samples:   8576
 > elasped time to build and save sample-idx mapping (seconds): 0.001011
 > building shuffle index with split [0, 8576) and [8576, 8576) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001122
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/ea55eccc9c5b89ea02137d863634eff2_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/ea55eccc9c5b89ea02137d863634eff2_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/ea55eccc9c5b89ea02137d863634eff2_shuffle_idx.npy
    loaded indexed file in 0.007 seconds
    total number of samples: 8577
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.005307 seconds
    number of documents: 8312582
 > dataset split:
    train:
     document indices in [0, 7888640) total of 7888640 documents
    validation:
     document indices in [7888640, 8304269) total of 415629 documents
    test:
     document indices in [8304269, 8312582) total of 8313 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.292533
    using:
     number of documents:       7888640
     number of epochs:          1
     sequence length:           1024
     total number of samples:   9107581
 > elasped time to build and save sample-idx mapping (seconds): 0.169514
 > building shuffle index with split [0, 9107581) and [9107581, 9107581) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.213465
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/2b4cbf589b0cde25567aa820edcfbabb_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/2b4cbf589b0cde25567aa820edcfbabb_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/2b4cbf589b0cde25567aa820edcfbabb_shuffle_idx.npy
    loaded indexed file in 0.008 seconds
    total number of samples: 9107582
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.015447
    using:
     number of documents:       415629
     number of epochs:          1
     sequence length:           1024
     total number of samples:   479225
 > elasped time to build and save sample-idx mapping (seconds): 0.007776
 > building shuffle index with split [0, 479225) and [479225, 479225) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.010830
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/be7da790c7b29a943732738597a79848_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/be7da790c7b29a943732738597a79848_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/be7da790c7b29a943732738597a79848_shuffle_idx.npy
    loaded indexed file in 0.006 seconds
    total number of samples: 479226
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001410
    using:
     number of documents:       8313
     number of epochs:          1
     sequence length:           1024
     total number of samples:   9006
 > elasped time to build and save sample-idx mapping (seconds): 0.000932
 > building shuffle index with split [0, 9006) and [9006, 9006) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001370
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/22b84fa19b45d8d9297aa75ecbb321fd_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/22b84fa19b45d8d9297aa75ecbb321fd_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/22b84fa19b45d8d9297aa75ecbb321fd_shuffle_idx.npy
    loaded indexed file in 0.003 seconds
    total number of samples: 9007
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.016333 seconds
    number of documents: 7762631
 > dataset split:
    train:
     document indices in [0, 7366736) total of 7366736 documents
    validation:
     document indices in [7366736, 7754868) total of 388132 documents
    test:
     document indices in [7754868, 7762631) total of 7763 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.279990
    using:
     number of documents:       7366736
     number of epochs:          1
     sequence length:           1024
     total number of samples:   8485224
 > elasped time to build and save sample-idx mapping (seconds): 0.151950
 > building shuffle index with split [0, 8485224) and [8485224, 8485224) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.219664
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/c4a27e3ba1412a4dff2c53b80a7197ce_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/c4a27e3ba1412a4dff2c53b80a7197ce_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/c4a27e3ba1412a4dff2c53b80a7197ce_shuffle_idx.npy
    loaded indexed file in 0.008 seconds
    total number of samples: 8485225
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.014929
    using:
     number of documents:       388132
     number of epochs:          1
     sequence length:           1024
     total number of samples:   443605
 > elasped time to build and save sample-idx mapping (seconds): 0.006997
 > building shuffle index with split [0, 443605) and [443605, 443605) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.010448
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e65f6427935414a4f31355c7a9009d19_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e65f6427935414a4f31355c7a9009d19_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e65f6427935414a4f31355c7a9009d19_shuffle_idx.npy
    loaded indexed file in 0.009 seconds
    total number of samples: 443606
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001416
    using:
     number of documents:       7763
     number of epochs:          1
     sequence length:           1024
     total number of samples:   9036
 > elasped time to build and save sample-idx mapping (seconds): 0.001003
 > building shuffle index with split [0, 9036) and [9036, 9036) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001085
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/6286698345740e5bca129db8b31a6794_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/6286698345740e5bca129db8b31a6794_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/6286698345740e5bca129db8b31a6794_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of samples: 9037
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.006017 seconds
    number of documents: 8348845
 > dataset split:
    train:
     document indices in [0, 7923054) total of 7923054 documents
    validation:
     document indices in [7923054, 8340496) total of 417442 documents
    test:
     document indices in [8340496, 8348845) total of 8349 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.291029
    using:
     number of documents:       7923054
     number of epochs:          1
     sequence length:           1024
     total number of samples:   9164606
 > elasped time to build and save sample-idx mapping (seconds): 0.166551
 > building shuffle index with split [0, 9164606) and [9164606, 9164606) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.260109
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/955c088f576ce828e6ed09609980c1f6_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/955c088f576ce828e6ed09609980c1f6_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/955c088f576ce828e6ed09609980c1f6_shuffle_idx.npy
    loaded indexed file in 0.008 seconds
    total number of samples: 9164607
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.016491
    using:
     number of documents:       417442
     number of epochs:          1
     sequence length:           1024
     total number of samples:   485214
 > elasped time to build and save sample-idx mapping (seconds): 0.008088
 > building shuffle index with split [0, 485214) and [485214, 485214) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.012049
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/3cc6396ca13d65a4f8965115d099312c_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/3cc6396ca13d65a4f8965115d099312c_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/3cc6396ca13d65a4f8965115d099312c_shuffle_idx.npy
    loaded indexed file in 0.006 seconds
    total number of samples: 485215
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001304
    using:
     number of documents:       8349
     number of epochs:          1
     sequence length:           1024
     total number of samples:   9592
 > elasped time to build and save sample-idx mapping (seconds): 0.001200
 > building shuffle index with split [0, 9592) and [9592, 9592) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001271
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/f42fdf5e3d9427222f048d3a7f77c4e7_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/f42fdf5e3d9427222f048d3a7f77c4e7_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/f42fdf5e3d9427222f048d3a7f77c4e7_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of samples: 9593
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.004426 seconds
    number of documents: 7901631
 > dataset split:
    train:
     document indices in [0, 7498647) total of 7498647 documents
    validation:
     document indices in [7498647, 7893729) total of 395082 documents
    test:
     document indices in [7893729, 7901631) total of 7902 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.275105
    using:
     number of documents:       7498647
     number of epochs:          1
     sequence length:           1024
     total number of samples:   8663742
 > elasped time to build and save sample-idx mapping (seconds): 0.153935
 > building shuffle index with split [0, 8663742) and [8663742, 8663742) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.232534
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/c6ed2b852e7bf66ea460a316ad12f783_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/c6ed2b852e7bf66ea460a316ad12f783_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/c6ed2b852e7bf66ea460a316ad12f783_shuffle_idx.npy
    loaded indexed file in 0.009 seconds
    total number of samples: 8663743
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.014692
    using:
     number of documents:       395082
     number of epochs:          1
     sequence length:           1024
     total number of samples:   450096
 > elasped time to build and save sample-idx mapping (seconds): 0.007054
 > building shuffle index with split [0, 450096) and [450096, 450096) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.010436
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/db37b21aae3835bfc490ac9803589e84_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/db37b21aae3835bfc490ac9803589e84_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/db37b21aae3835bfc490ac9803589e84_shuffle_idx.npy
    loaded indexed file in 0.005 seconds
    total number of samples: 450097
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001102
    using:
     number of documents:       7902
     number of epochs:          1
     sequence length:           1024
     total number of samples:   8855
 > elasped time to build and save sample-idx mapping (seconds): 0.000967
 > building shuffle index with split [0, 8855) and [8855, 8855) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001373
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e10ea69fb79634a832a779d905b1a45b_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e10ea69fb79634a832a779d905b1a45b_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e10ea69fb79634a832a779d905b1a45b_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of samples: 8856
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.005443 seconds
    number of documents: 8195153
 > dataset split:
    train:
     document indices in [0, 7777200) total of 7777200 documents
    validation:
     document indices in [7777200, 8186958) total of 409758 documents
    test:
     document indices in [8186958, 8195153) total of 8195 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.284632
    using:
     number of documents:       7777200
     number of epochs:          1
     sequence length:           1024
     total number of samples:   8985444
 > elasped time to build and save sample-idx mapping (seconds): 0.161799
 > building shuffle index with split [0, 8985444) and [8985444, 8985444) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.219669
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/03513ae8e2400e1e0739403792d85e44_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/03513ae8e2400e1e0739403792d85e44_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/03513ae8e2400e1e0739403792d85e44_shuffle_idx.npy
    loaded indexed file in 0.008 seconds
    total number of samples: 8985445
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.014838
    using:
     number of documents:       409758
     number of epochs:          1
     sequence length:           1024
     total number of samples:   480119
 > elasped time to build and save sample-idx mapping (seconds): 0.008690
 > building shuffle index with split [0, 480119) and [480119, 480119) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.011158
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/bd679bbd4b5632d57904c80193244a00_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/bd679bbd4b5632d57904c80193244a00_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/bd679bbd4b5632d57904c80193244a00_shuffle_idx.npy
    loaded indexed file in 0.005 seconds
    total number of samples: 480120
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001282
    using:
     number of documents:       8195
     number of epochs:          1
     sequence length:           1024
     total number of samples:   9100
 > elasped time to build and save sample-idx mapping (seconds): 0.000986
 > building shuffle index with split [0, 9100) and [9100, 9100) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001068
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/29f27e0a20119db5237995e85deaa3ce_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/29f27e0a20119db5237995e85deaa3ce_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/29f27e0a20119db5237995e85deaa3ce_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of samples: 9101
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.004363 seconds
    number of documents: 7699034
 > dataset split:
    train:
     document indices in [0, 7306383) total of 7306383 documents
    validation:
     document indices in [7306383, 7691335) total of 384952 documents
    test:
     document indices in [7691335, 7699034) total of 7699 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.268872
    using:
     number of documents:       7306383
     number of epochs:          1
     sequence length:           1024
     total number of samples:   8390602
 > elasped time to build and save sample-idx mapping (seconds): 0.152878
 > building shuffle index with split [0, 8390602) and [8390602, 8390602) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.195173
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/3ae53b3cf0cb827bf15c5bd908277f15_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/3ae53b3cf0cb827bf15c5bd908277f15_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/3ae53b3cf0cb827bf15c5bd908277f15_shuffle_idx.npy
    loaded indexed file in 0.008 seconds
    total number of samples: 8390603
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.014318
    using:
     number of documents:       384952
     number of epochs:          1
     sequence length:           1024
     total number of samples:   436028
 > elasped time to build and save sample-idx mapping (seconds): 0.006948
 > building shuffle index with split [0, 436028) and [436028, 436028) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.010630
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/eb15a707ff4e82a7eedef802ef781978_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/eb15a707ff4e82a7eedef802ef781978_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/eb15a707ff4e82a7eedef802ef781978_shuffle_idx.npy
    loaded indexed file in 0.006 seconds
    total number of samples: 436029
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001233
    using:
     number of documents:       7699
     number of epochs:          1
     sequence length:           1024
     total number of samples:   8941
 > elasped time to build and save sample-idx mapping (seconds): 0.001079
 > building shuffle index with split [0, 8941) and [8941, 8941) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001097
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/124fdeca682f511c6b3e37108aa77afd_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/124fdeca682f511c6b3e37108aa77afd_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/124fdeca682f511c6b3e37108aa77afd_shuffle_idx.npy
    loaded indexed file in 0.005 seconds
    total number of samples: 8942
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.001897 seconds
    number of documents: 7678458
 > dataset split:
    train:
     document indices in [0, 7286857) total of 7286857 documents
    validation:
     document indices in [7286857, 7670780) total of 383923 documents
    test:
     document indices in [7670780, 7678458) total of 7678 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.266349
    using:
     number of documents:       7286857
     number of epochs:          1
     sequence length:           1024
     total number of samples:   8398885
 > elasped time to build and save sample-idx mapping (seconds): 0.149108
 > building shuffle index with split [0, 8398885) and [8398885, 8398885) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.197144
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/5e2535c1e54daa9da92f31231c43fb98_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/5e2535c1e54daa9da92f31231c43fb98_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/5e2535c1e54daa9da92f31231c43fb98_shuffle_idx.npy
    loaded indexed file in 0.008 seconds
    total number of samples: 8398886
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.014570
    using:
     number of documents:       383923
     number of epochs:          1
     sequence length:           1024
     total number of samples:   439276
 > elasped time to build and save sample-idx mapping (seconds): 0.007039
 > building shuffle index with split [0, 439276) and [439276, 439276) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.010540
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/827be7e556c2a3d394c0e8a87d176b92_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/827be7e556c2a3d394c0e8a87d176b92_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/827be7e556c2a3d394c0e8a87d176b92_shuffle_idx.npy
    loaded indexed file in 0.005 seconds
    total number of samples: 439277
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001618
    using:
     number of documents:       7678
     number of epochs:          1
     sequence length:           1024
     total number of samples:   8810
 > elasped time to build and save sample-idx mapping (seconds): 0.000971
 > building shuffle index with split [0, 8810) and [8810, 8810) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001003
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/150ac8bb0d2086d3fc267a2d26c2b20a_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/150ac8bb0d2086d3fc267a2d26c2b20a_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/150ac8bb0d2086d3fc267a2d26c2b20a_shuffle_idx.npy
    loaded indexed file in 0.003 seconds
    total number of samples: 8811
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.004125 seconds
    number of documents: 7320971
 > dataset split:
    train:
     document indices in [0, 6947601) total of 6947601 documents
    validation:
     document indices in [6947601, 7313650) total of 366049 documents
    test:
     document indices in [7313650, 7320971) total of 7321 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.256522
    using:
     number of documents:       6947601
     number of epochs:          1
     sequence length:           1024
     total number of samples:   7985432
 > elasped time to build and save sample-idx mapping (seconds): 0.160589
 > building shuffle index with split [0, 7985432) and [7985432, 7985432) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.196357
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/7a41b2618d599a2f677ef40f485028dd_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/7a41b2618d599a2f677ef40f485028dd_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/7a41b2618d599a2f677ef40f485028dd_shuffle_idx.npy
    loaded indexed file in 0.009 seconds
    total number of samples: 7985433
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.011777
    using:
     number of documents:       366049
     number of epochs:          1
     sequence length:           1024
     total number of samples:   427903
 > elasped time to build and save sample-idx mapping (seconds): 0.006914
 > building shuffle index with split [0, 427903) and [427903, 427903) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.010499
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e6301d2e633d04532f59d23cbe1538c5_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e6301d2e633d04532f59d23cbe1538c5_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e6301d2e633d04532f59d23cbe1538c5_shuffle_idx.npy
    loaded indexed file in 0.005 seconds
    total number of samples: 427904
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001463
    using:
     number of documents:       7321
     number of epochs:          1
     sequence length:           1024
     total number of samples:   8133
 > elasped time to build and save sample-idx mapping (seconds): 0.001340
 > building shuffle index with split [0, 8133) and [8133, 8133) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001116
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/fe416b8bdc8d0a672cc6df4298037b97_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/fe416b8bdc8d0a672cc6df4298037b97_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/fe416b8bdc8d0a672cc6df4298037b97_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of samples: 8134
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.005231 seconds
    number of documents: 9568845
 > dataset split:
    train:
     document indices in [0, 9080834) total of 9080834 documents
    validation:
     document indices in [9080834, 9559276) total of 478442 documents
    test:
     document indices in [9559276, 9568845) total of 9569 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.341189
    using:
     number of documents:       9080834
     number of epochs:          1
     sequence length:           1024
     total number of samples:   10528759
 > elasped time to build and save sample-idx mapping (seconds): 0.217684
 > building shuffle index with split [0, 10528759) and [10528759, 10528759) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.281052
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/6534e8b46c62c4da7902e45442704377_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/6534e8b46c62c4da7902e45442704377_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/6534e8b46c62c4da7902e45442704377_shuffle_idx.npy
    loaded indexed file in 0.008 seconds
    total number of samples: 10528760
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.013610
    using:
     number of documents:       478442
     number of epochs:          1
     sequence length:           1024
     total number of samples:   554651
 > elasped time to build and save sample-idx mapping (seconds): 0.008553
 > building shuffle index with split [0, 554651) and [554651, 554651) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.012787
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/128d5d13664ed0a322ebc7e28ad07b69_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/128d5d13664ed0a322ebc7e28ad07b69_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/128d5d13664ed0a322ebc7e28ad07b69_shuffle_idx.npy
    loaded indexed file in 0.007 seconds
    total number of samples: 554652
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001073
    using:
     number of documents:       9569
     number of epochs:          1
     sequence length:           1024
     total number of samples:   10758
 > elasped time to build and save sample-idx mapping (seconds): 0.000862
 > building shuffle index with split [0, 10758) and [10758, 10758) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001189
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/936a40645b916768efcf49f1be2f4a5e_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/936a40645b916768efcf49f1be2f4a5e_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/936a40645b916768efcf49f1be2f4a5e_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of samples: 10759
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.005292 seconds
    number of documents: 8957584
 > dataset split:
    train:
     document indices in [0, 8500747) total of 8500747 documents
    validation:
     document indices in [8500747, 8948626) total of 447879 documents
    test:
     document indices in [8948626, 8957584) total of 8958 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.306591
    using:
     number of documents:       8500747
     number of epochs:          1
     sequence length:           1024
     total number of samples:   9788660
 > elasped time to build and save sample-idx mapping (seconds): 0.181530
 > building shuffle index with split [0, 9788660) and [9788660, 9788660) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.235691
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/0cd9d212b007a152ac43b348457515b7_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/0cd9d212b007a152ac43b348457515b7_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/0cd9d212b007a152ac43b348457515b7_shuffle_idx.npy
    loaded indexed file in 0.017 seconds
    total number of samples: 9788661
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.013158
    using:
     number of documents:       447879
     number of epochs:          1
     sequence length:           1024
     total number of samples:   516556
 > elasped time to build and save sample-idx mapping (seconds): 0.007984
 > building shuffle index with split [0, 516556) and [516556, 516556) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.012069
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/3be84cf40d621bff89a53dc2790aad1d_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/3be84cf40d621bff89a53dc2790aad1d_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/3be84cf40d621bff89a53dc2790aad1d_shuffle_idx.npy
    loaded indexed file in 0.005 seconds
    total number of samples: 516557
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001352
    using:
     number of documents:       8958
     number of epochs:          1
     sequence length:           1024
     total number of samples:   10045
 > elasped time to build and save sample-idx mapping (seconds): 0.001320
 > building shuffle index with split [0, 10045) and [10045, 10045) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001046
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/630e56716361f842f0529f9829497c41_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/630e56716361f842f0529f9829497c41_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/630e56716361f842f0529f9829497c41_shuffle_idx.npy
    loaded indexed file in 0.008 seconds
    total number of samples: 10046
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.005924 seconds
    number of documents: 7711950
 > dataset split:
    train:
     document indices in [0, 7318640) total of 7318640 documents
    validation:
     document indices in [7318640, 7704238) total of 385598 documents
    test:
     document indices in [7704238, 7711950) total of 7712 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.266966
    using:
     number of documents:       7318640
     number of epochs:          1
     sequence length:           1024
     total number of samples:   8442030
 > elasped time to build and save sample-idx mapping (seconds): 0.150306
 > building shuffle index with split [0, 8442030) and [8442030, 8442030) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.201246
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/28d662f60f214e6dc6d49d456c8619ab_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/28d662f60f214e6dc6d49d456c8619ab_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/28d662f60f214e6dc6d49d456c8619ab_shuffle_idx.npy
    loaded indexed file in 0.008 seconds
    total number of samples: 8442031
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.014404
    using:
     number of documents:       385598
     number of epochs:          1
     sequence length:           1024
     total number of samples:   451891
 > elasped time to build and save sample-idx mapping (seconds): 0.008450
 > building shuffle index with split [0, 451891) and [451891, 451891) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.010307
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/4e64cdefeed01acc25f4e3863c1cddb5_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/4e64cdefeed01acc25f4e3863c1cddb5_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/4e64cdefeed01acc25f4e3863c1cddb5_shuffle_idx.npy
    loaded indexed file in 0.005 seconds
    total number of samples: 451892
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001108
    using:
     number of documents:       7712
     number of epochs:          1
     sequence length:           1024
     total number of samples:   8513
 > elasped time to build and save sample-idx mapping (seconds): 0.001154
 > building shuffle index with split [0, 8513) and [8513, 8513) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001122
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/d2b463cec20447f46d82d011b19a28a3_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/d2b463cec20447f46d82d011b19a28a3_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/d2b463cec20447f46d82d011b19a28a3_shuffle_idx.npy
    loaded indexed file in 0.004 seconds
    total number of samples: 8514
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.003376 seconds
    number of documents: 8396375
 > dataset split:
    train:
     document indices in [0, 7968160) total of 7968160 documents
    validation:
     document indices in [7968160, 8387979) total of 419819 documents
    test:
     document indices in [8387979, 8396375) total of 8396 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.290817
    using:
     number of documents:       7968160
     number of epochs:          1
     sequence length:           1024
     total number of samples:   9200336
 > elasped time to build and save sample-idx mapping (seconds): 0.167429
 > building shuffle index with split [0, 9200336) and [9200336, 9200336) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.224102
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/2c24716ddfc6988ad870adb2638a4ca9_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/2c24716ddfc6988ad870adb2638a4ca9_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/2c24716ddfc6988ad870adb2638a4ca9_shuffle_idx.npy
    loaded indexed file in 0.009 seconds
    total number of samples: 9200337
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.015857
    using:
     number of documents:       419819
     number of epochs:          1
     sequence length:           1024
     total number of samples:   490242
 > elasped time to build and save sample-idx mapping (seconds): 0.008856
 > building shuffle index with split [0, 490242) and [490242, 490242) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.010856
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/781c89970c4c0ac6dbfa14658f50439a_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/781c89970c4c0ac6dbfa14658f50439a_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/781c89970c4c0ac6dbfa14658f50439a_shuffle_idx.npy
    loaded indexed file in 0.005 seconds
    total number of samples: 490243
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001153
    using:
     number of documents:       8396
     number of epochs:          1
     sequence length:           1024
     total number of samples:   9463
 > elasped time to build and save sample-idx mapping (seconds): 0.001131
 > building shuffle index with split [0, 9463) and [9463, 9463) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001225
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/1b7a3dc81e96b1480b0fc45fdc25e987_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/1b7a3dc81e96b1480b0fc45fdc25e987_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/1b7a3dc81e96b1480b0fc45fdc25e987_shuffle_idx.npy
    loaded indexed file in 0.079 seconds
    total number of samples: 9464
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.001149 seconds
    number of documents: 9381064
 > dataset split:
    train:
     document indices in [0, 8902630) total of 8902630 documents
    validation:
     document indices in [8902630, 9371683) total of 469053 documents
    test:
     document indices in [9371683, 9381064) total of 9381 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.325629
    using:
     number of documents:       8902630
     number of epochs:          1
     sequence length:           1024
     total number of samples:   10283055
 > elasped time to build and save sample-idx mapping (seconds): 0.200449
 > building shuffle index with split [0, 10283055) and [10283055, 10283055) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.245436
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/931cf11c6015148bc17cde82cf0e7c24_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/931cf11c6015148bc17cde82cf0e7c24_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/931cf11c6015148bc17cde82cf0e7c24_shuffle_idx.npy
    loaded indexed file in 0.008 seconds
    total number of samples: 10283056
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.013673
    using:
     number of documents:       469053
     number of epochs:          1
     sequence length:           1024
     total number of samples:   542943
 > elasped time to build and save sample-idx mapping (seconds): 0.008467
 > building shuffle index with split [0, 542943) and [542943, 542943) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.012491
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/8aaaa248481e1e85fa2ac48aa3ca2b85_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/8aaaa248481e1e85fa2ac48aa3ca2b85_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/8aaaa248481e1e85fa2ac48aa3ca2b85_shuffle_idx.npy
    loaded indexed file in 0.005 seconds
    total number of samples: 542944
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001498
    using:
     number of documents:       9381
     number of epochs:          1
     sequence length:           1024
     total number of samples:   11310
 > elasped time to build and save sample-idx mapping (seconds): 0.001084
 > building shuffle index with split [0, 11310) and [11310, 11310) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001141
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/35d5d0c5be1dd7f9874a0024f89ca9f9_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/35d5d0c5be1dd7f9874a0024f89ca9f9_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/35d5d0c5be1dd7f9874a0024f89ca9f9_shuffle_idx.npy
    loaded indexed file in 0.004 seconds
    total number of samples: 11311
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.005337 seconds
    number of documents: 7830881
 > dataset split:
    train:
     document indices in [0, 7431506) total of 7431506 documents
    validation:
     document indices in [7431506, 7823050) total of 391544 documents
    test:
     document indices in [7823050, 7830881) total of 7831 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.272244
    using:
     number of documents:       7431506
     number of epochs:          1
     sequence length:           1024
     total number of samples:   8552240
 > elasped time to build and save sample-idx mapping (seconds): 0.154428
 > building shuffle index with split [0, 8552240) and [8552240, 8552240) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.216355
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/3cfbe176553c0e3792118126aa85f66c_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/3cfbe176553c0e3792118126aa85f66c_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/3cfbe176553c0e3792118126aa85f66c_shuffle_idx.npy
    loaded indexed file in 0.008 seconds
    total number of samples: 8552241
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.015568
    using:
     number of documents:       391544
     number of epochs:          1
     sequence length:           1024
     total number of samples:   447678
 > elasped time to build and save sample-idx mapping (seconds): 0.007344
 > building shuffle index with split [0, 447678) and [447678, 447678) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.010437
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/6fd23d2da3d307dc707bca0ddc4a265f_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/6fd23d2da3d307dc707bca0ddc4a265f_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/6fd23d2da3d307dc707bca0ddc4a265f_shuffle_idx.npy
    loaded indexed file in 0.006 seconds
    total number of samples: 447679
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001552
    using:
     number of documents:       7831
     number of epochs:          1
     sequence length:           1024
     total number of samples:   8726
 > elasped time to build and save sample-idx mapping (seconds): 0.001222
 > building shuffle index with split [0, 8726) and [8726, 8726) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001009
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/450e88e22eaa7692481afc3c07fd8fdc_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/450e88e22eaa7692481afc3c07fd8fdc_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/450e88e22eaa7692481afc3c07fd8fdc_shuffle_idx.npy
    loaded indexed file in 0.006 seconds
    total number of samples: 8727
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.006597 seconds
    number of documents: 1363395
 > dataset split:
    train:
     document indices in [0, 1293862) total of 1293862 documents
    validation:
     document indices in [1293862, 1362032) total of 68170 documents
    test:
     document indices in [1362032, 1363395) total of 1363 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.040299
    using:
     number of documents:       1293862
     number of epochs:          1
     sequence length:           1024
     total number of samples:   1544223
 > elasped time to build and save sample-idx mapping (seconds): 0.024404
 > building shuffle index with split [0, 1544223) and [1544223, 1544223) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.035698
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/0d1cdeb27e3e6c97d6d2d66409b45ef9_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/0d1cdeb27e3e6c97d6d2d66409b45ef9_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/0d1cdeb27e3e6c97d6d2d66409b45ef9_shuffle_idx.npy
    loaded indexed file in 0.008 seconds
    total number of samples: 1544224
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.002766
    using:
     number of documents:       68170
     number of epochs:          1
     sequence length:           1024
     total number of samples:   87729
 > elasped time to build and save sample-idx mapping (seconds): 0.002266
 > building shuffle index with split [0, 87729) and [87729, 87729) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.003012
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/b7fabda883f8e94d4436373a63f618b5_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/b7fabda883f8e94d4436373a63f618b5_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/b7fabda883f8e94d4436373a63f618b5_shuffle_idx.npy
    loaded indexed file in 0.003 seconds
    total number of samples: 87730
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.000916
    using:
     number of documents:       1363
     number of epochs:          1
     sequence length:           1024
     total number of samples:   1390
 > elasped time to build and save sample-idx mapping (seconds): 0.000968
 > building shuffle index with split [0, 1390) and [1390, 1390) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.000774
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/356560f72fec76df51f6bc91fffa3dea_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/356560f72fec76df51f6bc91fffa3dea_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/356560f72fec76df51f6bc91fffa3dea_shuffle_idx.npy
    loaded indexed file in 0.003 seconds
    total number of samples: 1391
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.005864 seconds
    number of documents: 947630
 > dataset split:
    train:
     document indices in [0, 899300) total of 899300 documents
    validation:
     document indices in [899300, 946682) total of 47382 documents
    test:
     document indices in [946682, 947630) total of 948 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.025710
    using:
     number of documents:       899300
     number of epochs:          1
     sequence length:           1024
     total number of samples:   13262270
 > elasped time to build and save sample-idx mapping (seconds): 0.115067
 > building shuffle index with split [0, 13262270) and [13262270, 13262270) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.335596
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/8bf06b52a90ad89e8d28220798c8c539_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/8bf06b52a90ad89e8d28220798c8c539_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/8bf06b52a90ad89e8d28220798c8c539_shuffle_idx.npy
    loaded indexed file in 0.008 seconds
    total number of samples: 13262271
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.002471
    using:
     number of documents:       47382
     number of epochs:          1
     sequence length:           1024
     total number of samples:   703054
 > elasped time to build and save sample-idx mapping (seconds): 0.005698
 > building shuffle index with split [0, 703054) and [703054, 703054) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.017321
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/d2adb45b12ccb892cf7ea61f2273a2e5_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/d2adb45b12ccb892cf7ea61f2273a2e5_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/d2adb45b12ccb892cf7ea61f2273a2e5_shuffle_idx.npy
    loaded indexed file in 0.006 seconds
    total number of samples: 703055
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001004
    using:
     number of documents:       948
     number of epochs:          1
     sequence length:           1024
     total number of samples:   14395
 > elasped time to build and save sample-idx mapping (seconds): 0.001141
 > building shuffle index with split [0, 14395) and [14395, 14395) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001224
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/90560337cb5f4a6f54d592635753b7e6_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/90560337cb5f4a6f54d592635753b7e6_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/90560337cb5f4a6f54d592635753b7e6_shuffle_idx.npy
    loaded indexed file in 0.003 seconds
    total number of samples: 14396
    total number of epochs: 1
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.005581 seconds
    number of documents: 10106852
 > dataset split:
    train:
     document indices in [0, 9591402) total of 9591402 documents
    validation:
     document indices in [9591402, 10096745) total of 505343 documents
    test:
     document indices in [10096745, 10106852) total of 10107 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.355350
    using:
     number of documents:       9591402
     number of epochs:          1
     sequence length:           1024
     total number of samples:   10814771
 > elasped time to build and save sample-idx mapping (seconds): 0.212074
 > building shuffle index with split [0, 10814771) and [10814771, 10814771) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.282620
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/b9f3ff511e33317dc7cc64fadc40e402_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/b9f3ff511e33317dc7cc64fadc40e402_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/b9f3ff511e33317dc7cc64fadc40e402_shuffle_idx.npy
    loaded indexed file in 0.008 seconds
    total number of samples: 10814772
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.015543
    using:
     number of documents:       505343
     number of epochs:          1
     sequence length:           1024
     total number of samples:   577544
 > elasped time to build and save sample-idx mapping (seconds): 0.008941
 > building shuffle index with split [0, 577544) and [577544, 577544) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.013250
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e25376cea7c72531e1bff50ee7ebd808_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e25376cea7c72531e1bff50ee7ebd808_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/e25376cea7c72531e1bff50ee7ebd808_shuffle_idx.npy
    loaded indexed file in 0.005 seconds
    total number of samples: 577545
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001332
    using:
     number of documents:       10107
     number of epochs:          1
     sequence length:           1024
     total number of samples:   12357
 > elasped time to build and save sample-idx mapping (seconds): 0.001048
 > building shuffle index with split [0, 12357) and [12357, 12357) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001277
 > loading doc-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/aaadbedc485fc3b43a32a9235b37cf14_doc_idx.npy
 > loading sample-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/aaadbedc485fc3b43a32a9235b37cf14_sample_idx.npy
 > loading shuffle-idx mapping from /bb/llm/gaf51275/llama/datasets/okazaki_lab_cc_2100_okazaki_lab_cc_nfkc_16k_aligned_8/index-cache/aaadbedc485fc3b43a32a9235b37cf14_shuffle_idx.npy
    loaded indexed file in 0.003 seconds
    total number of samples: 12358
    total number of epochs: 1
> building indices for blendable datasets ...
 > sample ratios:
   dataset 0, input: 0.0407197, achieved: 0.0407197
   dataset 1, input: 0.0409047, achieved: 0.0409047
   dataset 2, input: 0.0462526, achieved: 0.0462526
   dataset 3, input: 0.0469503, achieved: 0.0469503
   dataset 4, input: 0.0458646, achieved: 0.0458646
   dataset 5, input: 0.0414041, achieved: 0.0414041
   dataset 6, input: 0.0382774, achieved: 0.0382774
   dataset 7, input: 0.0428163, achieved: 0.0428163
   dataset 8, input: 0.0398806, achieved: 0.0398806
   dataset 9, input: 0.0431001, achieved: 0.0431001
   dataset 10, input: 0.0407053, achieved: 0.0407053
   dataset 11, input: 0.0422758, achieved: 0.0422758
   dataset 12, input: 0.0394242, achieved: 0.0394242
   dataset 13, input: 0.039475, achieved: 0.039475
   dataset 14, input: 0.0375764, achieved: 0.0375764
   dataset 15, input: 0.049502, achieved: 0.049502
   dataset 16, input: 0.0460265, achieved: 0.0460265
   dataset 17, input: 0.0397225, achieved: 0.0397225
   dataset 18, input: 0.0432814, achieved: 0.0432814
   dataset 19, input: 0.0483559, achieved: 0.0483559
   dataset 20, input: 0.0401964, achieved: 0.0401964
   dataset 21, input: 0.00728795, achieved: 0.00728797
   dataset 22, input: 0.05, achieved: 0.05
   dataset 23, input: 0.05, achieved: 0.05
> elapsed time for building blendable dataset indices: 0.97 (sec)
> size of blendable dataset: 25728011 samples
> building indices for blendable datasets ...
 > sample ratios:
   dataset 0, input: 0.0407197, achieved: 0.0407196
   dataset 1, input: 0.0409047, achieved: 0.0409047
   dataset 2, input: 0.0462526, achieved: 0.0462525
   dataset 3, input: 0.0469503, achieved: 0.0469501
   dataset 4, input: 0.0458646, achieved: 0.0458646
   dataset 5, input: 0.0414041, achieved: 0.0414041
   dataset 6, input: 0.0382774, achieved: 0.0382776
   dataset 7, input: 0.0428163, achieved: 0.0428163
   dataset 8, input: 0.0398806, achieved: 0.0398807
   dataset 9, input: 0.0431001, achieved: 0.0431001
   dataset 10, input: 0.0407053, achieved: 0.0407053
   dataset 11, input: 0.0422758, achieved: 0.0422759
   dataset 12, input: 0.0394242, achieved: 0.0394243
   dataset 13, input: 0.039475, achieved: 0.039475
   dataset 14, input: 0.0375764, achieved: 0.0375765
   dataset 15, input: 0.049502, achieved: 0.0495021
   dataset 16, input: 0.0460265, achieved: 0.0460264
   dataset 17, input: 0.0397225, achieved: 0.0397224
   dataset 18, input: 0.0432814, achieved: 0.0432813
   dataset 19, input: 0.0483559, achieved: 0.0483558
   dataset 20, input: 0.0401964, achieved: 0.0401966
   dataset 21, input: 0.00728795, achieved: 0.00728813
   dataset 22, input: 0.05, achieved: 0.0499999
   dataset 23, input: 0.05, achieved: 0.0499999
> elapsed time for building blendable dataset indices: 0.10 (sec)
> size of blendable dataset: 2583103 samples
> building indices for blendable datasets ...
 > sample ratios:
   dataset 0, input: 0.0407197, achieved: 0.0407648
   dataset 1, input: 0.0409047, achieved: 0.0408619
   dataset 2, input: 0.0462526, achieved: 0.0462972
   dataset 3, input: 0.0469503, achieved: 0.0469766
   dataset 4, input: 0.0458646, achieved: 0.045909
   dataset 5, input: 0.0414041, achieved: 0.0414442
   dataset 6, input: 0.0382774, achieved: 0.0382413
   dataset 7, input: 0.0428163, achieved: 0.0428031
   dataset 8, input: 0.0398806, achieved: 0.0398913
   dataset 9, input: 0.0431001, achieved: 0.0430942
   dataset 10, input: 0.0407053, achieved: 0.0406678
   dataset 11, input: 0.0422758, achieved: 0.0423178
   dataset 12, input: 0.0394242, achieved: 0.039406
   dataset 13, input: 0.039475, achieved: 0.0395031
   dataset 14, input: 0.0375764, achieved: 0.0375619
   dataset 15, input: 0.049502, achieved: 0.0495001
   dataset 16, input: 0.0460265, achieved: 0.046006
   dataset 17, input: 0.0397225, achieved: 0.0396972
   dataset 18, input: 0.0432814, achieved: 0.0432884
   dataset 19, input: 0.0483559, achieved: 0.0483354
   dataset 20, input: 0.0401964, achieved: 0.0401825
   dataset 21, input: 0.00728795, achieved: 0.00727943
   dataset 22, input: 0.05, achieved: 0.0499854
   dataset 23, input: 0.05, achieved: 0.0499854
> elapsed time for building blendable dataset indices: 0.00 (sec)
> size of blendable dataset: 10303 samples
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2024-01-29 08:02:40 
done with setup ...
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (4034.53, 4056.46)
    train/valid/test-data-iterators-setup ..........: (27039.24, 27361.40)
training ...
[before the start of training step] datetime: 2024-01-29 08:02:40 
[after training is done] datetime: 2024-01-29 08:02:40 
saving checkpoint at iteration   25000 to /groups/gaf51275/llama/checkpoints/Llama-2-7b-seq-1024/okazaki_lab_cc/tp2-pp2
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
Traceback (most recent call last):
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    .data.copy_(tensors[key].detach().cpu())
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
KeyError: 'exp_avg'
    .data.copy_(tensors[key].detach().cpu())
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    pretrain(train_valid_test_datasets_provider,
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
KeyError: 'exp_avg'
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    optimizer.save_parameter_state(optim_checkpoint_name)
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    .data.copy_(tensors[key].detach().cpu())
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
KeyError: 'exp_avg'
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
Traceback (most recent call last):
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    .data.copy_(tensors[key].detach().cpu())
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
KeyError: 'exp_avg'
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
Traceback (most recent call last):
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/pretrain_gpt.py", line 132, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/training.py", line 165, in pretrain
    save_checkpoint(iteration, model, optimizer, opt_param_scheduler)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/checkpointing.py", line 264, in save_checkpoint
    optimizer.save_parameter_state(optim_checkpoint_name)
  File "/bb/1/llm/gaf51275/llama/Megatron-LM/megatron/optimizer/distrib_optimizer.py", line 651, in save_parameter_state
    .data.copy_(tensors[key].detach().cpu())
KeyError: 'exp_avg'
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[28714,1],23]
  Exit code:    1
--------------------------------------------------------------------------
